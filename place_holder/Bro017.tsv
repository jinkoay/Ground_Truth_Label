Is it starting now?	0
Yep.	0
So what - what - from - what - Whatever we say from now on,	0
Hello?	0
it can be held against us, right?	0
That's right.	0
and uh	0
It's your right to remain silent.	0
Yeah. So I - I - the - the problem is that I actually don't know how th- these held meetings are held, if they are very informal and sort of just people are say what's going on and  O_K.	0
Yeah.	0
Yeah, that's usually what we do. We just sorta go around and people say what's going on, what's the latest uh -	0
Yeah.	0
O_K. So I guess that what may be a - reasonable is if I uh first make a report on what's happening in [MASK] in general, at least what from  my  perspective.	0
Yeah.	0
That  would be  great.	0
Uh o-	0
And -	0
and uh so, I - I think that Carmen and Stephane reported on uh Amsterdam meeting, which was kind of interesting because it was for the first time we realized we are not friends really, but we are competitors.	0
Cuz until then it was sort of like everything was like wonderful and -	0
Yeah.	0
It seemed like there were still some issues, right? that they were trying to decide?	1
Yeah.	0
There is a plenty of - there're plenty of issues.	0
Like the voice activity detector, and -	0
Well and what happened was that they realized that if two leading proposals, which was [MASK] and us	0
both had uh voice activity detector.	0
Right.	0
And I said "well big surprise, I mean we could have told you that  n- n- n- four  months  ago, except we  didn't  because nobody  else  was bringing it up". Obviously [MASK] didn't volunteer this information  either,	0
Right.	0
cuz we were working on -	0
mainly  on voice activity detector for past uh several months because that's buying us the most uh thing. And everybody said "Well but this is not  fair.  We didn't  know  that."	0
And of course uh the - it's not working on  features  really. And be- I agreed. I said "well yeah, you are absolutely right, I mean if I  wish  that you provided better end point at speech because uh -	0
Right.	0
or at least that if we could modify the  recognizer,	0
uh to account for these long silences, because otherwise uh that - that - th- that wasn't a correct thing."	0
And so then ev- ev- everybody else says "well we should - we need to do a  new eval-  evaluation	0
without  voice activity detector, or we have to do something about it". And in principle I - uh I - we agreed.	0
Right.	0
Mm-hmm.	0
We said uh "yeah".	0
Because uh - but in that case,	1
uh we would like to change the uh - the algorithm because uh if we are working on different data, we probably will use a different set of tricks.	1
Right.	0
But  unfortunately nobody ever officially can somehow acknowledge that this can be done, because [MASK] was saying "no, no, no, now everybody has access to our code,	0
so everybody is going to copy what we did."	0
Yeah well our argument was everybody ha- has access to  our  code, and everybody always  had  access to our code. We never	0
uh - uh denied that. We thought that people are honest, that if you copy something and if it is protected - protected by patent	0
Yeah.	0
then you negotiate, or  something,  right? I mean, if you find our technique useful, we are very happy.	0
Right.	0
Right.	0
Mm-hmm.	0
But - And [MASK] was saying "no, no, no, there is a lot of little tricks which uh sort of uh cannot be protected and you guys will take them,"	0
which probably is also true. I mean,  you  know,	0
it might be that people will take uh	0
uh th- the algorithms apart and use the blocks from that. But I somehow think that it wouldn't be so bad, as long as people are happy abou- uh uh uh honest about it. And I think they have to be honest in the long run, because winning proposal again -	0
Yeah.	0
uh what will be available th- is - will be a  code.  So the uh - the people can go to code and say "well listen this is what you stole from me"  you know?	0
Mm-hmm.	0
Right.	0
Right.	0
"so let's  deal  with that". So I don't see the problem. The biggest problem of course is that f- that [MASK] cl- claims "well we fulfilled	0
the conditions. We are the best. Uh. We are the standard."	0
And e- and other people don't feel that, because they - so they now decided that - that - is - the whole thing will be done on well-endpointed data,	0
essentially that somebody will endpoint the data based on clean speech, because most of this the SpeechDat-Car has the also close speaking mike and endpoints will be provided.	0
Mm-hmm.	0
Ah.	0
And uh we will run again - still not clear if we are going to run the -	0
if we are allowed to run uh uh new  algorithms,  but I  assume  so. Because uh we would  fight  for that, really.	0
uh  but  - since uh  u- u- n- u-  - at least  our  experience is that only endpointing a - [MASK]	0
gets uh - gets you twenty-one percent improvement  overall  and twenty-seven improvement on [MASK]	0
Hmm.	0
then obvious the  database  - uh I mean the - the -	0
the -	0
uh the  baseline  will go up. And  nobody  can then achieve fifty percent  improvement.	0
Right.	0
So they  agreed  that uh there will be a twenty-five percent improvement required on -	0
on uh  h- u- m-  bad mis- badly mismatched -	0
But wait a minute, I thought the endpointing really only helped in the  noisy  cases.	0
It uh -	0
Oh, but you still have that with [MASK] O_K. Yeah.	0
Y- yeah. Yeah but you have the same prob- I mean [MASK] basically has an enormous number of uh insertions.	0
Right.	0
Yeah.	0
Yeah. Yeah.	0
And so, so  now  they want to say "we - we will require fifty percent improvement only for well matched condition, and only twenty-five percent for the serial cases."	0
Hmm.	0
And uh - and they almost agreed on that except that it wasn't a hundred percent agreed. And so last time uh during the meeting,	0
I just uh brought up the issue, I said "well you know uh quite frankly I'm surprised how lightly you are making these decisions because	0
this is a major decision. For two years we are fighting for fifty percent improvement	0
and suddenly you are saying "oh no we - we will do something less", but maybe we should  discuss  that. And everybody said "oh we discussed that and you were not a mee- there" and I said "well a lot of  other  people were not there because not everybody participates at these	0
teleconferencing c- things." Then they said "oh no no no because uh everybody is  invited. "	0
However, there is only ten or fifteen lines, so people  can't  even con- you know participate.	0
So eh they agreed, and so they said "O_K, we will discuss that."	0
Immediately Nokia uh raised the question and they said "oh yeah we agree this is not good to	0
to uh dissolve the uh uh - the uh - the criterion." So now officially, Nokia is uh uh complaining and said they - they are looking for support,	1
Mm-hmm.	0
uh I think QualComm is uh saying,  too  "we shouldn't abandon the fifty percent  yet.  We should at least  try  once again, one more round."	1
Mm-hmm.	0
Mm-hmm.	0
So this is where we  are.  I hope that - I hope that this is going to be a- adopted. Next Wednesday we are going to have uh	0
Hmm.	0
another uh teleconferencing call, so we'll see what uh - where it goes.	0
So what about the issue of um the weights on the - for the different systems, the	0
well-matched, and medium-mismatched and -	0
Yeah, that's what - that's a g-  very  good uh point, because David says "well you know we ca- we can manipulate this number by choosing the right weights anyways." So while you are right but - uh you know but	0
Mm-hmm.	0
Uh yeah, if- of course if you put a zero - uh weight zero on a mismatched condition, or highly mismatched then - then you are  done.	0
Mm-hmm.	0
But weights were also deter- already decided uh half a year ago.	0
And they're the - staying the same?	0
So -	0
Well, of course people will not like it.	0
Mm-hmm.	0
Now - What is happening now is that I th- I think that people try to match the criterion to solution.	0
They have solution. Now they	0
Right.	0
want to  make sure their criterion is -	0
And I think that this is not the right way.	0
Yeah.	0
Uh it may be that - that -   Eventually  it may ha- may ha- it may have to happen.	0
Mm-hmm.	0
But it's should happen at a point where everybody feels comfortable that we did all what we could.	0
Mm-hmm.	0
And I don't think we did. Basically, I think that - that this test was a little bit bogus because of the data	0
and uh essentially  there were these arbitrary	0
decisions made, and - and everything. So,	0
so - so this is - so this is where it is.	0
So what we are doing at [MASK] now is uh uh	1
uh	0
working basically on our parts which we I think a little bit neglected,	1
like noise separation.	1
Uh so we are looking in ways is - in uh which - uh with which we can provide better initial estimate of the mel spectrum basically,	1
which would be a l- uh, f-	1
more robust to noise, and so far not much uh success.	1
Hmm.	0
We tried uh	0
things which uh a long time ago Bill Byrne suggested, instead of using [MASK] fr	0
Their argument there was [MASK] fits the  peaks  of the spectrum, so it may be m- naturally more robust in  noise.	0
And I thought "well, that makes sense," but so far we can't get much - much  out  of it.	0
Hmm.	0
uh we may try some standard techniques like spectral subtraction and -	0
You haven't tried that yet?	0
not - not - not much. Or even I was thinking about uh looking back into these totally ad- hoc  techniques like for instance uh	0
Hmm.	0
Dennis Klatt was suggesting	0
uh the one way to uh deal with noisy speech is to add noise to  everything.	0
Hmm!	0
So.  I mean, uh uh add moderate amount of noise	0
Oh!	0
to all data.	0
I  see.	0
So that makes uh th- any additive noise less addi- less a- a-	0
effective, right? Because you already uh  had  the noise uh in a -	0
Right.	0
And it was  working  at the time. It was kind of like one of  these  things, you know, but if you  think  about it, it's actually pretty ingenious.	0
So well, you know, just take a - take a spectrum and - and - and add of the constant, C_, to every - every value.	0
Well you're - you're basically y-	0
Yeah. So you're making all your  training  data more  uniform.	0
Exactly.  And if - if then - if this data becomes noisy, it b- it becomes eff- effectively becomes less noisy basically.	0
Hmm.	0
But of course you cannot add  too  much noise because then you'll s- then you're  clean  recognition goes down, but I mean it's yet to be seen how much, it's a very simple technique.	0
Mm-hmm.	0
Yes indeed it's a very simple technique, you just take your spectrum and - and use whatever is coming from [MASK]  add  constant,	0
Hmm.	0
you know? on - onto power spectrum.	0
That - that -	0
Or the other thing is of course if you have a spectrum, what you can s- start doing, you can leave - start leaving out	0
the p- the parts which are uh uh low in energy	0
and then perhaps uh one could try to find a - [MASK] to such a spectrum. Because [MASK] will still try to -	0
to - to put the -	0
the continuation basically of the - of the model into these parts where  the issue  set to zero. That's what we want to try.	0
I have a visitor from Brno.  He's a - kind of like young faculty.	0
pretty hard-working so he - so he's - so he's looking into that.	0
Hmm.	0
And  then  most of the effort is uh now also aimed at this e- e- [MASK]	1
This uh - this is this recognition from temporal patterns.	1
Hmm! What is that?	0
Ah, you don't know about TRAPS!	0
Hmm.	0
[MASK] sound familiar, I - but I don't -	0
Yeah I mean tha- This is familiar like sort of because we gave you the  name,  but, what it is, is that	1
Mm-hmm.	0
normally what you do is that you recognize uh speech based on a shortened spectrum.	1
Mm-hmm.	0
Essentially L_P_- [MASK]	1
Uh so if you s- So, given the  spectrogram  you essentially are sliding - sliding the spectrogram along the uh f- frequency axis and you keep shifting this thing,	1
Mm-hmm.	0
Mm-hmm.	0
and you have a  spectrogram.  So you can say "well you can also take the  time  trajectory	1
of the energy at a given  frequency ",	1
Mm-hmm.	0
and what you  get  is then, that you get a p-   vector.	1
And this  vector  can be a - a - s- assigned to s- some  [MASK]  Namely you can say	1
i- it - I will - I will say that this  vector  will eh - will - will describe [MASK] which is in the center of the vector.	1
And you can try to classify based on that.	0
Hmm.	0
And you - so you classi- so it's a very different vector, very different properties, we don't know much about it,	1
Hmm.	0
but the truth is -	0
But you have  many  of those vectors per [MASK] right?	0
Well, so you get many  decisions.	1
Uh-huh.	0
And then you can start dec- thinking about how to combine these decisions. Exactly, that's what - yeah, that's what it is.	0
Hmm.	0
Hmm.	0
Because if you run this uh recognition, you get - you still get about twenty percent error - uh twenty percent  correct.  You know, on - on like for the frame by frame basis, so  uh -	0
Hmm.	0
uh so it's much better than  chance.	0
How wide are the uh frequency bands?	0
That's another thing. Well c- currently we start - I mean we start always with critical band spectrum.	0
For various reasons. But uh the latest uh observation	0
uh is that you - you - you are - you can get quite a big advantage of using two critical bands at the same time.	0
Are they adjacent, or are they s- O_K.	0
Adjacent, adjacent. And the reasons - there are some reasons for that. Because there are some reasons I can - I could talk about, will have to tell you about things like masking experiments	0
which uh uh uh uh yield critical bands, and also experiments with release of masking,	0
which actually tell you that something is happening  across  critical bands,  across  bands.	0
And -	0
Well how do you - how do you uh convert	0
this uh energy over time in a particular frequency band into a vector of numbers?	0
It's uh uh uh I mean time T_zero is  one  number,	0
time t-	0
Yeah but what's the number? Is it just the -	0
It's a spectral energy, logarithmic spectral energy, yeah.	0
it's just the amount of energy in that band from f- in that time interval.	0
Yes, yes.	0
Yes, yes.	0
O_K.	0
And that's what - that's what I'm saying then, so this is a - this is a starting vector.	0
It's just like shortened f-  spectrum, or something. But now we are trying to understand what this vector actually represents, for instance a question is like "how correlated are the  elements  of this vector?"	0
Mm-hmm.	1
Turns out they  are  quite correlated, because I mean, especially the  neighboring  ones, right? They - they represent the same - almost the same configuration of the vocal tract.	0
Yeah. Yeah.	0
Mm-hmm.	0
So there's a  very  high correlation. So the	0
classifiers which use the diagonal covariance matrix don't  like  it. So we're thinking about  de-correlating  them.	0
Hmm.	0
Then the question is uh "can you describe elements of this vector by Gaussian distributions", or to what extent?	0
Because uh -	0
And - and - and so on and so on. So we are learning quite a lot about that. And then another issue is how many vectors we should be using, I mean the - so the  minimum  is  one.	0
Hmm.	0
Mm-hmm.	0
But I mean is the - is [MASK] the right uh uh dimension?	0
So we somehow made arbitrary decision, "yes".	0
Then - but then now we are thinking a lot how to -	0
uh	0
how to use at least the  neighboring  band because that seems to be happening - This I somehow start to believe that's what's happening in recognition.	0
Cuz a lot of experiments point to the fact that people  can  split the signal into [MASK]	0
but	0
then oh uh uh so you can - you are quite  capable  of processing a signal	0
in- uh uh independently in individual [MASK] That's what masking experiments tell you. But at the  same  time	0
you most likely pay attention to at least neighboring bands when you are making any decisions, you compare what's happening in - in this band to what's happening to the band -	0
to - to - to the - to [MASK]	0
And that's how you make uh decisions.	0
That's why the articulatory events, which uh F- F-  Fletcher  talks about, they are about two critical bands. You need at least  two,  basically. You need some relative,  relative  relation.	0
Hmm.	0
Hmm.	0
Absolute  number doesn't tell you the right thing. You need to - you need to compare it to something else, what's happening	0
but it's what's happening in the - in the  close  neighborhood. So if you are making decision what's happening at one kilohertz, you want to know what's happening at nine hundred hertz and it -	0
and maybe at eleven hundred hertz, but you don't much care what's happening at three kilohertz.	1
So it's really w- It's sort of like saying that what's happening at one kilohertz  depends  on what's happening  around  it. It's sort of  relative  to it.	0
To some extent, it - that is  also  true. Yeah. But it's - but for - but for instance,	0
Mm-hmm.	0
th- uh	0
uh what - what uh humans are very much capable of doing is that if th- if they are exactly the same thing happening in two [MASK]	0
recognition can  discard  it.	0
Hmm.	1
Is what's happening -	0
Hey!	0
Hey!	0
O_K, we need us another - another  voice  here.	0
Hey Stephane.	0
Yeah, I  think  so.	0
Yep. Sure. Go ahead.	0
Yeah?	0
And	1
so	0
so - so for instance if you d- if you a- if you  add  the  noise  that normally masks - masks the uh - the - the signal	1
Mm-hmm.	0
right? and you can show that in - that if the - if you add the noise outside [MASK] that doesn't affect	0
the - the decisions you're making about a signal within [MASK]  Unless  this noise is modulated. If the noise is modulated, with the  same  modulation frequency	0
Hmm.	0
as the noise in [MASK]	0
the amount of masking is less.	0
Mmm.	0
The moment you -	0
moment you provide the noise in n- [MASK]	0
So the s- m- masking curve, normally it looks like sort of - I start from - from here, so you -	1
you have uh  no  noise then you - you - you are expanding [MASK] so the amount of  maching  is increasing. And when you e- hit a certain point,	0
which is [MASK] then the amount of masking is the same.	0
Mmm.	0
So that's the famous experiment of Fletcher, a long time ago.	0
Like that's where people started thinking "wow this is interesting!"	0
Yeah.	0
So.  But,  if you - if you - if you  modulate  the noise, the masking goes up and the moment you start hitting the - another critical band, the masking goes down.	0
So essentially - essentially that's a very clear indication that - that -	0
that  cognition can take uh	0
uh into consideration what's happening in the neighboring bands.	0
But if you go too  far  in a - in a - if you - if the noise is very  broad,  you are not increasing much more, so - so if you - if you are far away from the signal -	1
Mm-hmm.	0
uh from the signal f- uh the  frequency  at which the  signal  is,	0
Yeah.	0
then the m-  even  the - when the noise is co-modulated it - it's not helping you much.	0
Mm-hmm.	0
So.	0
Hmm.	0
So things like this we are kind of playing with - with - with the  hope	0
that perhaps we could eventually u- use this in a - in a real recognizer.	0
Mm-hmm.	0
Like uh  partially  of course we promised to do this under the - the - [MASK]	1
uh program.	1
But you probably won't have anything before the next	0
time we have to evaluate, right? Yeah.	0
Probably not. Well, maybe, most likely we will not have anything which c- would comply with the rules.	0
Ah.	0
like	0
because uh uh	0
Latency and things.	0
latency currently chops the require uh significant uh latency	0
Mm-hmm.	0
amount of processing, because uh we don't know any better, yet, than to use the neural net classifiers,	1
Yeah.	0
uh and uh - and uh [MASK] Though the - the work which uh everybody is looking at now aims	1
Mm-hmm.	0
Hmm.	0
at	0
s- trying to find out what to do with these vectors, so that a g- simple Gaussian classifier would be  happier  with it.	0
Mm-hmm.	0
or to what extent a Gaussian classifier should be unhappy uh that, and how to Gaussian-ize the	0
vectors, and -	0
Hmm.	0
So this is uh what's happening. Then Sunil is uh uh uh	1
asked me f- for one month's vacation	0
and since he did not take any vacation for two years, I had no - I didn't have heart to tell him no.	0
So he's in India.	0
Wow.	0
And uh -	0
Is he getting married or something?	0
Uh well, he may be looking for a girl, for - for I don't - I don't - I don't ask.	0
I know that Naran- when last time  Narayanan  did that he came back engaged.	0
Right. Well, I mean, I've known other friends who - they - they go to Ind- they go back home to India for a month, they come back  married,	0
Yeah.	0
I know.	0
I know, I know, and then of course then what happened with  Narayanan  was that he start pushing me that he needs to get a P_H_D because they wouldn't give him his wife.	0
you know, huh.	0
And she's very pretty and he loves her  and so - so we had to really -	0
So he finally had some incentive to finish, huh?	0
Oh yeah. We had - well  I  had a incentive because he - he always had this plan except he never told me.	1
Oh.	0
Sort of figured that -	0
That was a uh that he uh	0
he told me the day when	0
we did very well at our [MASK] evaluations of speaker recognition, the technology, and he was involved there.	0
We were - after presentation we were driving home and he told me.	0
When he knew you were happy, huh?	0
Yeah. So I - I said "well, yeah, O_K" so he took another - another three quarter of the year but uh he was out. So I - wouldn't surprise me if he has a plan like that,  though  -	1
though uh Pratibha still needs to get out first.	1
Hmm.	0
Cuz Pratibha is there a - a year earlier.	0
Hmm.	0
And S- and  Satya  needs to get out very first because he's - he already has uh  four  years served,	0
though one year he was getting masters. So.	0
Hmm.	0
So.	0
So have the um - when is the next uh evaluation?  June  or something?	0
Which? Speaker recognition?	0
No, for uh [MASK]	0
Uh there, we don't know about evaluation, next  meeting  is in June. And uh uh but like getting - get together.	0
Hmm.	0
Oh, O_K.	0
Are people supposed to rerun their systems, or - ?	0
Nobody said that yet. I  assume  so.	0
Hmm.	0
Uh yes, uh, but nobody even set up yet the	0
date for uh delivering uh endpointed data.	0
Wow.	0
And this uh - that - that sort of stuff.	0
But I uh, yeah, what I think would be of course  extremely  useful, if we can come to our next meeting and say "well you know	0
we  did  get	0
fifty percent improvement.	0
If - if you are interested we eventually can tell you how",	0
Mm-hmm.	0
but uh we  can  get fifty percent improvement. Because people will s- will be saying it's impossible.	0
Hmm.	0
Do you know what the new baseline is? Oh, I guess if you don't have -	0
Twenty-two - t- twenty - twenty-two percent better than the old baseline.	0
Using  your  uh voice activity detector?	0
u- Yes. Yes. But I assume that it will be similar, I don't - I - I don't see the reason why it shouldn't be. I d- I don't see reason why it should be  worse.	0
Similar, yeah.	0
Mm-hmm.	0
Yeah.	0
Cuz if it is  worse,  then we will raise the objection, we say "well you know how come?"	0
Because eh if we just use our voice activity detector, which we don't claim even that it's  wonderful,  it's just like  one  of them.	0
Mm-hmm.	0
Yeah.	0
We get this sort of improvement, how come that we don't see it on - on - on - on  your  endpointed data?	0
Yeah.	0
I guess it could be even  better,  because the voice activity detector that I choosed is something that cheating, it's using the alignment	0
I think so.	0
Yeah.	0
of the speech recognition system,	0
C- yeah uh and on clean speech data.	0
and  only  the alignment on the clean channel, and then	0
Oh, O_K.	0
Yeah. Well David told me - David told me yesterday or Harry actually he told Harry from QualComm and Harry uh brought up the suggestion we should still go for fifty percent	0
mapped this alignment to the noisy channel.	0
he says are you aware that your system does only thirty percent	0
uh comparing to - to	0
endpointed baselines? So they must have run already  something.	0
Yeah.	0
Hmm.	0
So.	0
And Harry said " Yeah.  But I mean we think that we - we didn't say the last  word  yet, that we have other - other things which we can try."	0
Hmm.	0
So. So there's a lot of discussion now about this uh new criterion.	0
Mm-hmm.	0
Because Nokia was objecting, with uh QualComm's - we basically supported that, we said "yes".	0
Mm-hmm.	0
Now everybody else is saying "well you guys might - must be out of your mind."	0
uh The - Guenter Hirsch who d- doesn't speak for Ericsson anymore because he is not with Ericsson and Ericsson may not - may withdraw from the whole [MASK] activity because they have so many troubles now.	1
Wow.	0
Ericsson's laying off twenty percent of people.	0
Wow.	0
Where's uh  Guenter  going?	0
Well Guenter is already - he got the job uh already was working on it for past two years or three years -	0
Mm-hmm.	0
he got a job uh at some - some Fachschule, the technical college not too far from Aachen.	0
Hmm!	0
So it's like professor - u- university professor	0
Mm-hmm.	0
you know, not quite a university, not quite a sort of - it's not	1
Aachen University, but it's a good school and he -  he's  happy.	1
Mm-hmm.	0
Hmm!	0
And	0
he - well, he was hoping to work uh with Ericsson like on t-	0
uh like consulting basis, but right now he says - says it doesn't look like that anybody is even thinking about speech recognition.	0
Mm-hmm.	0
Wow!	0
They think about survival.	0
Yeah.	0
Hmm.	0
So.	1
So. But this is being now discussed right now, and it's possible that uh - that -	0
that	0
it may get through, that we will still stick to fifty percent. But that means that nobody will probably get this im- this improvement.	0
Mm-hmm.	0
yet, wi- with the current system. Which event- es- essentially I think that we should be  happy  with because that - that would mean that at least people may be forced to look into alternative solutions and -	1
Mm-hmm.	0
Mm-hmm.	0
But maybe -	0
I - I mean we are not too  far  from - from fifty percent,	0
from the new  baseline.	0
Uh, but not -	0
Which would mean like sixty percent	0
Yeah.	0
over the current baseline, which is -	0
Yes. Yes.	0
Well.	0
We - we getting - we getting there, right.	0
We are around fifty, fifty-five. So.	0
Yeah.	0
Yeah.	0
Mm-hmm.	0
Is it like sort of - is - How did you come up with this number? If you improve twenty - by twenty percent the c- the f- the all baselines, it's just a quick c- comp- co-	0
Yeah. I don't know exactly if it's -	0
computation?	0
Uh-huh.	0
I think it's about right.	0
Yeah, because it de- it depends on the  weightings  and -	0
Yeah, yeah.	0
Yeah. But.	0
Mm-hmm.	0
Hmm.	0
How's your	1
documentation or whatever it w- what was it you guys were working on last week?	1
Yeah, finally we - we've not  finished  with this. We stopped.	1
More or less it's finished.	1
Yeah.	0
Ma- nec- to need a little more time to improve the English,	1
and maybe s- to fill in something - some small detail, something like that, but it's more or less ready.	1
Mm-hmm.	0
Hmm.	0
Yeah.	0
Well, we have a document that explain	1
Necessary to - to include the bi- the bibliography. Mm-hmm.	0
a big part of the experiments, but it's not, yeah, finished yet.	1
Mm-hmm.	0
So have you been running some  new  experiments? I - I thought I saw some jobs of yours running on some of the machine -	1
Yeah. Right. We've fff  done some strange things like	1
removing [MASK] or [MASK] from the -	0
the vector of parameters, and we noticed that	1
[MASK] is almost not useful at all.	1
Really?!	0
You can remove it from the vector, it doesn't hurt.	1
That has no effect?	0
Um.	0
Eh - Is this in the baseline? or in uh -	0
In the - No, in the proposal.	0
in - uh-huh, uh-huh.	0
So we were just discussing, since you mentioned that, in - it w- driving in the car with Morgan this morning,	1
Mm-hmm.	0
Mm-hmm.	0
we were discussing a good experiment for b- for beginning graduate student	0
who wants to run a lot of - who wants to get a lot of numbers on something	1
which is, like, "imagine that you will -	1
you will	0
start putting every co-  any  coefficient, which you are using in your vector, in some general power.	1
In some  what?	0
General pow- power. Like sort of you take a s-	1
power of two, or take a square root,	0
Mm-hmm.	0
Mm-hmm.	0
or something.	1
Mm-hmm.	0
So suppose that you are working with a s- C_zer- [MASK]	0
So if you put it in a s- square root,	0
that effectively makes your model half as efficient.	0
Because uh your uh Gaussian mixture model, right? computes the mean.	1
Mm-hmm.	0
And - and uh i- i- i- but it's - the mean is an exponent	1
of the	0
whatever, the - the - this	0
You're  compressing  the  range,  right? of that -	0
Gaussian function.	1
So you're compressing the range of this coefficient, so it's becoming less efficient.	1
Right?	0
Mm-hmm.	0
So. So.	0
Morgan was  @@  and he was - he was saying well this might be the alternative way how to play with a - with a fudge factor, you know,	1
uh in the - you know, just compress the whole vector.	0
Oh.	0
Yeah.	0
And I said "well in that case why don't we just start compressing individual elements, like when - when - because in old days we were doing - when - when people still were doing template matching and Euclidean distances,	1
we were doing this liftering of parameters, right?	0
Uh-huh.	0
because we observed that uh higher parameters were more important than  lower  for recognition. And basically the - the C_ze- [MASK] contributes mainly slope, and it's highly affected by	1
Right.	0
Mm-hmm.	0
uh frequency response of the - of the recording equipment and that sort of thing, so - so we were coming with all these f- various lifters.	0
Mm-hmm.	0
Mm-hmm.	0
uh Bell Labs had he - this uh	0
uh r- raised cosine lifter which still I think is built into H_ - [MASK] for reasons n- unknown to anybody, but -	1
but uh	0
we had exponential lifter, or triangle lifter,  basic  number of lifters.	1
Hmm.	0
And. But so they may be a way to - to fiddle with the f- with the f-	0
Insertions.	0
Insertions, deletions,  or  the - the -	0
giving a relative - uh basically modifying relative importance of the various parameters.	0
Mm-hmm.	0
The only of course problem is that there's an infinite number of combinations and if the - if you s- if y-	0
Oh. Uh-huh.	0
You need like a - some kind of a -	0
Yeah, you need a lot of graduate students, and a lot of computing power.	1
You need to have a genetic algorithm, that basically tries random	0
I know. Exactly. Oh.	0
permutations of these things.	0
If you were at Bell Labs or - I d- d-	0
I shouldn't be saying this in - on - on a mike, right?	0
Or I - uh - I_B_M,	0
that's what - maybe that's what somebody would be doing.	1
Yeah.	0
Hmm.	0
Oh, I mean, I mean the places which have a lot of computing power, so because it is really it's a p-	0
Mm-hmm.	0
it's a - it's - it will be reasonable search	0
Yeah.	0
uh	0
but I  wonder  if there isn't some way of doing this	0
uh	0
search like when we are searching say for best discriminants.	0
You know  actually,  I don't know that this wouldn't be all that  bad.  I mean you - you compute the features  once,  right?	0
Yeah.	0
Yeah.	0
And then	0
Absolutely.	0
these  exponents  are just applied to that - So.	0
And hev- everything is  fixed.   Everything  is  fixed.  Each - each -	0
And is this something that you would adjust for training? or only  recognition?	0
For  both,  you would have to do.	0
You would do it on  both.  So you'd actually -	0
Yeah. You have to do bo- both.	0
Because essentially you are saying "uh this feature is not important".	0
Mm-hmm.	0
Or  less  important, so that's - th- that's a - that's a painful one, yeah.	0
So for each -	0
uh set of exponents that you would try, it would require a training and a recognition?	0
Yeah.	0
But - but wait a minute. You may not need to re- uh uh retrain the m- model.	0
You just may n- may need to c- uh give uh less  weight	0
to -	0
to	0
uh a mod- uh a component of the model which represents this particular feature.	0
You don't have to  retrain  it.	0
Oh. So if you - Instead of altering the  feature  vectors  themselves,	0
You just multiply.	0
you - you modify the - the - the  Gaussians  in the  models.	0
Yeah.	0
Yep.	0
You modify the Gaussian in the  model,  but in the - in the  test  data you would have to put it in the power, but in a  training  what you c- in a  training  uh - in trained model,	0
Uh-huh.	0
all you would have to do is to multiply a  model  by appropriate  constant.	0
But why - if you're - if you're multi- if you're altering the  model,  why w- in the  test  data, why would you have to muck with the uh [MASK]	0
Because in uh test - in uh test data you ca- don't have a model. You have uh only data. But in a - in a tr-	0
No. But you're  running  your data through that same  model.	0
That is true, but w- I mean, so what you want to do -	0
You want to say if uh obs- you -	0
if you observe something like Stephane observes, that [MASK] is not important, you can do two things.	0
Mm-hmm.	0
Mm-hmm.	0
If you have a trained - trained recognizer, in the model, you  know	0
the - the - the - the  component  which - I - I mean di-  dimension	0
Mm-hmm.	0
All of the - all of the mean and variances that correspond to [MASK] you put them to  zero.  Yeah.	0
wh-	0
To the s- you - you  know  it. But what I'm proposing now, if it is important but not  as  important, you multiply it by point one	0
in a model.	0
But - but - but -	0
But what are you  multiplying?  Cuz those are  means,  right? I mean you're -	0
You're multiplying the standard deviation? So it's -	0
I think that you multiply the - I would - I would have to look in the - in the math, I mean how - how does the model uh - Yeah.	1
I  think  you - Yeah, I think you'd have to modify the standard  deviation  or something, so that you make it	0
Cuz -	0
Yeah.	0
Yeah.	0
Yeah.	0
Effectively, that's - that - that's - I -  Exactly.  That's what you do. That's what you do, you - you - you modify the standard deviation as it was trained.	1
wider or narrower.	0
Yeah.	0
Effectively you, you know y- in f- in front of the - of the model,	0
you put a constant.	0
S- yeah effectively what you're doing is you - is you are modifying the - the - the deviation.	0
Right?	0
The spread, right.	0
Oop.  Sorry.	0
Yeah, the spread.	0
So.	0
It's the same - same  mean,  right?	0
And - and - and -	0
So by making th- the standard deviation  narrower,	0
Yeah.	0
uh your scores get  worse  for - unless it's  exactly  right  on  the mean.	0
Your als- No. By making it narrower,	0
Right?	0
I mean there's - you're - you're allowing for less variance.	0
uh y- your -	0
Mm-hmm.	0
Yes, so you making this particular dimension less important.	0
Because see what you are fitting is the multidimensional Gaussian, right?	0
Mm-hmm.	0
It's a - it has - it has uh thirty-nine dimensions, or thirteen dimensions if you g- ignore deltas and double-deltas.	0
Mm-hmm.	0
Mm-hmm.	0
So in order - if you - in order to make dimension which - which  Stephane  sees uh less important,	0
uh uh I mean not - not useful, less important, what you do is that this particular component in the model	0
you can multiply by w- you can - you can basically de- weight  it in the model. But you can't do it in a - in a test data because you don't have a model for th- I mean	0
uh when the  test  comes, but what you can  do  is that you put this particular	0
component  in - and - and you  compress  it.	0
That becomes uh th- gets less variance, subsequently becomes less important.	1
Couldn't you just do that to the test data and  not	0
do  anything  with your  training  data?	0
That would be very bad, because uh your t- your model was trained uh expecting	0
uh, that wouldn't work. Because your model was trained expecting a certain var- variance on [MASK]	0
Uh-huh.	0
And because the model thinks [MASK] is important.	0
After  you train the model,	0
you sort of -	0
y- you could do - you could do still what I was proposing initially,	0
that during the training you - you compress [MASK]	0
Mm-hmm.	0
that becomes - then it becomes less important	0
in a training.	0
But if you have - if you want to run e- ex- extensive experiment without retraining the model, you don't have to retrain the model. You train it on the original vector.	0
But after, you - wh- when you are doing this parametric study of importance of [MASK]	0
you will	0
de-weight	0
[MASK] component in the model,	0
and you will put in the - you will compress	0
the - this component in a - in the test data.	0
Could you also if you wanted to -	0
s- by the same amount.	0
if you wanted to try an experiment uh by  leaving out	0
say, [MASK] couldn't you, in your  test  data,	0
uh	0
modify the - all of the C_one values	0
to be um way outside of	0
the normal range of the  Gaussian	0
for C_one that was trained in the model?	0
So that effectively,	0
Mm-hmm.	0
the C_one	0
never really contributes to the score?	0
Do you know what I'm say-	0
No, that would be a severe mismatch, right? what you are proposing? N- no you don't want that. Because that would - then your model would be unlikely.	1
Yeah, someth-	0
Your likelihood would be low, right?	0
Mm-hmm.	0
Because you would be providing severe mismatch.	0
But what if you set if to the  mean  of the model, then?	0
And it was a cons- you set  all   C_ones  coming	0
in through your  test  data, you - you change whatever value that was  there  to the  mean	0
that your  model  had.	0
No that would be very  good  match, right?	0
Yeah.	0
That you would -	0
Which - Well, yeah, but we have  several  means. So.	0
I see what you are sa-   saying,  but	0
Right?	0
Saying.	0
uh,	0
no, no I don't think that it would be the same. I mean, no, the -	0
If you set it to a mean, that would -	0
No, you can't do that. Y- you ca- you ca- Ch- Chuck, you can't do that. Because that would be a really f- fiddling with the data, you can't do that.	0
Oh, that's true, right, yeah, because you - you have -	0
Wait. Which -	0
Yeah.	0
Mm-hmm. Mm-hmm.	0
But what you can do, I'm confident you ca- well, I'm  reasonably  confident and I putting it on the record, right? I mean y- people will listen to it for - for  centuries  now,  is   what you can do, is you train the model uh with the - with the original data.	0
Mm-hmm.	0
Then you decide that you want to see how important C_ - [MASK] is.	0
So what you will do is that a component in the model for [MASK] you will divide it by -	0
by two.	0
And you will compress your test data	0
by square root.	0
Mm-hmm.	0
Then you will still have a perfect m- match. Except that this component of [MASK] will be half as important in a - in a overall score.	0
Mm-hmm.	0
Mm-hmm.	0
Then you divide it by four and you take a square, f-  fourth  root.	0
Mm-hmm.	0
Then if you think that some component is more - is  more  important then th- th- th- it then - then uh uh i- it is,	0
based on training, then you uh multiply this particular component in the model by -	0
You're talking about the standard deviation?	0
by - by - yeah. Yeah, multiply this component uh i- it by number b- larger than one,	1
Yeah.	0
Mm-hmm.	0
and you put your data in power higher than one.	0
Then it becomes more important.	0
In the overall score, I believe.	0
Yeah, but, at the -	0
But   don't  you have to do something to the  mean,  also?	0
No.	0
No.	0
No.	1
Yeah.	0
But I think it's -	0
uh the - The variance is on - on the  denominator  in the - in the Gaussian equation. So. I think it's	0
maybe it's the  contrary.	0
If you want to decrease the importance of a c- parameter,	0
Yes.	0
you have to	0
Right.	0
increase it's variance.	0
Multiply.	0
Yes.	0
Exactly.  Yeah. So you - so you may want to do it other way around, yeah.	0
Hmm.	0
That's  right.  O_K.	0
Mm-hmm.	0
Right.	0
But if your -	0
If your um original data	0
for [MASK] had a mean of  two.	0
Uh-huh.	1
And now you're -	0
you're - you're changing that	0
by  squaring  it.	0
Now your mean	0
of your [MASK] original data has -	0
is  four.	0
But your  model  still has a mean of  two.	0
So even though you've	0
expended the  range,  your  mean  doesn't match anymore.	0
Mm-hmm.	0
Let's see.	0
Do you see what I mean?	0
I think - What I see - What could be done is you don't change your features, which are computed once for all,	0
Uh-huh.	1
but you just tune the model.	0
So. You have your features. You train your - your model on these features.	0
Mm-hmm.	0
And then if you want to decrease the importance of C_one	0
you just take the variance of the [MASK] component in the - in the model	0
Yeah.	0
and increase it if you want to decrease the importance of [MASK] or decrease it -	0
Yeah.	0
Right.	0
Yeah.	0
You would  have  to modify the mean in the model. I - you - I agree with you. Yeah.	0
Yeah, but I mean, but it's - it's i- it's do-able, right? I mean, it's predictable.	0
Yeah, so y-	0
Well.	0
It's predictable, yeah.	0
Uh. Yeah. Yeah. Yeah, it's predictable.	0
Mmm.	0
Yeah.	0
But as a  simple  thing, you  could  just -	0
just muck with the  variance.	0
Just adjust the model, yeah.	0
to get uh this - uh this - the effect I think that  you're  talking about, right?	0
Mm-hmm.	0
It might be.	0
Could increase the variance to decrease the  importance.	0
Mm-hmm.	0
Mm-hmm.	0
Yeah, because if you had a  huge  variance,	0
Yeah, it becomes more flat and -	0
Doesn't matter -	0
you're  dividing  by a large number,  you get a very small	0
Right.	0
Yeah.	0
contribution.  Yeah.	0
Yeah.	0
Hmm.	0
Yeah, the sharper the variance, the more -	0
more important to get that one right.	0
Mm-hmm.	0
Yeah, you know  actually,  this reminds me of something that happened uh when I was at [MASK] We were playing with putting um  pitch	0
Mm-hmm.	0
into the  Mandarin  recognizer.	0
And this particular  pitch  algorithm	0
um	0
when it didn't think there was any  voicing,  was spitting out  zeros.	0
So we were getting -	0
uh when we did  clustering,  we were getting	0
groups uh of  features	0
p- Pretty new outliers, interesting outliers, right?	0
yeah, with -	0
with a mean of zero and basically zero  variance.	0
Variance.	0
So,	0
when ener-  when anytime any one of those  vectors  came in that had a  zero  in it, we got a  great  score.	0
I mean it was just,  you know,	0
Mm-hmm.	0
incredibly  high score, and so that was throwing everything off.	0
So	0
if you have  very  small variance you get  really  good scores when you get something that  matches.  So.	0
Yeah.	1
Mm-hmm.	0
So that's a way, yeah, yeah - That's a way to increase the - yeah, n- That's  interesting.	0
So in  fact,  that would be -	0
That  doesn't  require any retraining.	0
Yeah. No.	0
No, that's right.	0
No.	0
So  that  means	0
So it's just	0
Yeah.	0
it's just  recognitions.	0
tuning the models and testing, actually.	0
Yeah.	0
Yeah.	0
You - you have a step where you	0
It would be quick.	0
you  modify  the models, make a d-  copy  of your models with whatever variance modifications you make, and  rerun  recognition. And then do a  whole  bunch of those.	0
Mm-hmm.	0
Yeah.	0
Yeah.	0
Yeah.	1
Yeah.	0
Yeah.	0
Mm-hmm.	0
That  could be set up fairly  easily  I think, and you have a whole bunch of	0
you know -	0
Chuck is getting himself in trouble.	0
That's an interesting  idea,  actually.	0
For  testing  the -	0
Yeah.	0
Huh!	0
Didn't you say you got these uh [MASK] set up on the new Linux boxes? Yeah.	0
That's  right.  In fact, and - and they're just t- right  now  they're installing uh - increasing the  memory  on that uh -	0
Hey!	0
And Chuck is sort of really fishing for how to keep his computer busy, right?	1
the  Linux  box.	0
Right.	0
Yeah.    Absinthe.   Absinthe.  We've got five  processors  on that.	0
Well, you know, that's -	0
that's - yeah, that's a good thing because then y- you just write the "do"-loops and then you pretend that you are working while you are sort of - you c- you can go fishing.	1
Oh yeah. That's right.	0
And two gigs of  memory.	0
Yeah.	0
Yeah.	0
Exactly.	0
Pretend, yeah.	0
Yeah.   See how many cycles we used?	0
Go fishing.	0
Yeah.	0
Then you are sort of in this mode like all of those [MASK] people are, right?	1
Yeah.	0
Uh, since it is on the record, I can't say uh which company it was,	0
but it was reported to me	0
that uh somebody visited a company	0
and during a - d- during a discussion,	0
there was this guy who was always hitting the carriage returns	0
uh on a computer.	0
Uh-huh.	0
So after two hours uh the visitor said "wh- why are you hitting this carriage return?"	0
And he said "well you know, we are being paid by a computer	0
ty- I mean we are - we have a government contract. And they pay us by - by amount of computer time we use."	0
It was in old days when there were uh - of [MASK] -eights and that sort of thing.	1
Oh, my gosh!	0
So he had to make it look like -	0
Because so they had a - they literally had to c- monitor at the time - at the time on a computer how much	0
Yeah.	0
How -	0
time is being spent	0
Idle  time. Yeah.	0
I - i- i- or on - on this particular project.	0
Yeah.	0
Nobody was looking even at what was coming out.	1
Have you ever seen those little um -	0
It's - it's this thing that's the shape of a bird and it has a red ball and its beak dips into the water?	0
Yeah, I know, right.	0
So  if you could hook that up so it hit the  keyboard  -	0
Yeah.	0
Yeah.	0
Yeah.	0
Yeah.	0
That's  an interesting  experiment.	0
It would be similar - similar to - I knew some people who were	1
uh that was in old Communist uh Czechoslovakia, right?	0
so we were watching for American airplanes, coming	0
Mm-hmm.	0
to spy on - on uh - on us at the time,	0
Mm-hmm.	0
so there were three guys uh uh	0
stationed in the middle of the woods on one l- lonely	0
uh watching tower, pretty much spending a year and a half there because there was this service right?	0
Ugh!	0
And so they - very quickly they made friends with local girls and local people in the village and -	0
Yeah.	0
and so but they - there was one plane flying over s- always uh uh above,	0
and so that was the only work which they had.	0
They - like four in the afternoon they had to report there was a plane from Prague to Brno	0
Basically f- flying there,	0
Yeah.	0
so they f- very q- f- first thing was that they would always run back and - and at four o'clock and -	0
and quickly make a call, "this plane is uh uh passing"	0
then a second thing was that they - they took the line from this u- u- post to uh uh a local pub.	0
And they were calling from the pub.	0
And they -	0
but  third  thing which they made, and when they screwed up, they - finally they had to p- the - the p- the  pub  owner to make these phone calls because they didn't even bother to be  there  anymore.	0
And one day there was - there was no  plane.  At least  they  were sort of smart enough that they looked if the plane is flying there, right?	0
Yeah.	0
And the pub owner says "oh my - four o'clock, O_K, quickly p- pick up the phone, call that there's a plane flying." There was no plane for some reason, it was downed, or -	0
And there  wasn't?	0
and -	0
so they got in trouble.	0
But.	0
But uh.	0
Huh!	0
Well that's - that's a really i-	0
So. So. Yeah. Yeah. Yeah.	0
That  wouldn't be too difficult to  try.  Maybe I could set that  up.	0
And we'll just -	0
Well, at least go test the s- test the uh assumption about C_- [MASK] I mean to begin with.	0
Mm-hmm.	0
But then of course one can then think about some predictable result to change  all  of them. It's just like we used to do these uh - these uh - um the - the	0
uh distance measures. It might be that uh -  Yeah.	0
Yeah, so the  first  set of uh variance weighting vectors would be just you know  one  - modifying one and leaving the others the  same.	0
Yeah.	0
Yeah.	0
Yeah.	0
Yeah.	0
Yeah. Maybe.	0
And - and do that for each one. That would be  one  set of experiment -	0
Because you see, I mean, what is happening here in a - in a -	0
in a - in such a model is that it's - tells you yeah what has a low variance uh is uh - is uh - is more reliable, right?	0
How do we - Yeah. Yeah. Yeah. Yeah. Yeah.	0
Wh- yeah, when the data  matches  that,	0
then you get really -	0
Yeah. Right.	0
How do we know, especially when it comes to noise?	0
But there  could  just  naturally  be low variance.	0
Yeah?	0
Because I - Like,  I've  noticed in the higher  cepstral  coefficients, the numbers seem to get  smaller,  right?	0
So d-	0
They - t- Yeah. They have smaller  means,  also.	0
I mean, just  naturally.	0
Yeah, th- that's -	0
Uh.	0
Yeah.   Exactly.	0
And so it seems like they're  already  sort of  compressed.	0
Uh-huh.	0
The  range   of values.	0
Yeah that's why uh people used these lifters were inverse variance weighting lifters basically that makes uh uh	0
Mm-hmm.	0
Euclidean distance more like uh Mahalanobis distance	0
Mm-hmm.	0
with a diagonal covariance when you  knew  what all the variances were over the old data.	0
Hmm.	0
What they would do is that they would weight each coefficient by inverse of the variance. Turns out that uh the variance decreases	0
at least at fast, I believe, as the index of the cepstral coefficients. I think you can show that uh uh analytically.	0
Mm-hmm.	0
Hmm.	0
So typically what happens is that you - you need to weight the - uh weight the higher coefficients more than uh the lower coefficients.	0
Mm-hmm.	0
Hmm.	0
Mmm.	0
So.	0
Any -	0
When - Yeah. When we talked about Aurora still I wanted to m- make a plea - uh encourage for	1
uh more communication between - between uh  uh different uh parts of the distributed uh  uh center.	1
Uh even when there is absolutely nothing to - to s- to say but the weather is good in Ore- in - in Berkeley. I'm sure that it's being appreciated in Oregon and maybe it will generate similar responses down here,	1
like, uh -	0
We can set up a webcam maybe.	0
Yeah.	0
Yeah.	0
What - you know, nowadays,  yeah.  It's actually do-able, almost.	0
Is the um - if we mail to "Aurora- inhouse ",	1
does that go up to you guys also?	1
I don't think so. No.	1
No.	0
O_K. So i- What is it -	0
So  we  should  do  that.	0
Yeah.	0
We should definitely set up -	0
Yeah we sh- Do we  have  a mailing list that includes uh [MASK] people?	1
Yeah.	0
Uh no. We  don't  have.	1
Oh!	0
Uh-huh.	0
Maybe we should set that  up.  That would make it much  easier.	1
Yeah. Yeah. Yeah, that would make it easier.	1
So maybe just call it "Aurora" or something that would -	0
Yeah. Yeah. And then we also can send the -  the dis-  to the same address right, and it goes to everybody	1
Mm-hmm.	0
Mm-hmm.	0
Yeah.	0
O_K.	0
Maybe we can set that up.	0
Because what's happening naturally in  research,  I  know,  is that people	0
essentially start working on something and they don't want to be much bothered, right? but what the - the - then the  danger  is in a group like this,	0
is that two people are working on the same thing	0
Mm-hmm.	0
and i- c- of course both of them come with the s- very good solution, but it could have been done somehow in half of the effort or something.	0
Oh, there's another thing which I wanted to uh uh report. Lucash, I think,	0
uh wrote the software for this [MASK] system.	0
reasonably	0
uh good one, because he's doing it for Intel, but I trust that we have uh rights to uh use it uh or distribute it and everything. Cuz Intel's intentions originally was to distribute it free of charge anyways.	1
Hmm!	0
u- s-	0
And so - so uh we - we will make sure that at least you can see the software and if - if - if - if it is of any use.	0
Just uh -	0
Mm-hmm.	0
It might be a reasonable point for p- perhaps uh start converging.	0
Mm-hmm.	0
Because Morgan's point is that - He is an experienced guy. He says "well you know it's very difficult to collaborate if you	0
are working with supposedly the same thing, in quotes,	0
except which is not s- is not the same.	0
Mm-hmm.	0
Which - which uh uh	0
one is using that set of hurdles, another one set - is using another set of hurdles. So.	1
And -	0
And then it's difficult to c- compare.	1
What about Harry? Uh.	0
We received a mail last week and you are starting	0
He got the - he got the software. Yeah. They sent the release. Yeah. Yeah. Yeah.	0
to - to do some experiments.	0
And use this Intel	0
version.	0
Yeah.	0
Hmm.	0
Yeah because Intel paid us uh should I say on a microphone? uh some amount of money, not much. Not much I can say on a microphone. Much less then we should have gotten  for this amount of work.	0
And they wanted uh to - to have software so that they can also play with it, which means that it has to be in a certain environment - they use actu- actually some Intel	0
Hmm.	0
libraries, but in the process, Lucash just rewrote the whole thing because he figured rather than trying to f- make sense uh of uh - including ICSI software	0
Hmm.	0
uh not for  training  on the nets but I think he rewrote the - the - the - or so- maybe somehow reused over the parts of the thing so that - so that - the  whole  thing, including [MASK]	0
Oh.	0
Mm-hmm.	0
one piece of	0
uh software.	0
Wow!	0
Is it useful?	0
Yeah?	0
Ye-	0
Yeah.  I mean, I remember when we were trying to put together all the ICSI software for the submission.	0
Or -	0
That's what he was saying, right. He said that it was like - it was like just  so  many libraries and nobody knew what was used  when,  and -	1
and so that's where he started and that's where he realized that it needs to be - needs to be uh uh at least cleaned up, and so I think it - this is available.	0
Yeah.	0
Mm-hmm.	0
Hmm.	0
Yeah.	0
So -	1
Well, the - the only thing I would check is	0
if he - does he use Intel math libraries, because if it's the case,	0
uh e- ev-	0
n- not maybe - Maybe not in a first - maybe not in a first ap- approximation because I think he started first just with a plain C_ - C_ or C_-plus-plus or something	0
it's maybe not so easy to use it on another architecture.	0
Ah yeah.	0
Mm-hmm.	0
before - I - I can check on that. Yeah.	0
Yeah. O_K.	0
Hmm.	0
And uh in -  otherwise  the Intel libraries, I think they are available free of f- freely.	0
But they may be running only on - on uh - on uh Windows.	0
Yeah.	0
Or on - on the -	0
On Intel architecture maybe. I'm -	0
Yeah, on Intel architecture, may not run in SUN.	1
Yeah.	0
Yeah.	0
Yeah.	0
That is p- that is - that is possible.	0
Hmm.	0
That's why Intel of course is distributing it, right?	0
Well.	0
Or -	0
Yeah.	0
Well there are - at least there are optimized version for their architecture.	0
That's -	0
Yeah.	1
I don't know. I never	0
checked carefully these sorts of -	0
I know there was some issues that initially of course we d- do all the development on Linux	0
but we use - we don't have - we have only three uh uh uh uh s- SUNs	0
and we have them only because they have a SPERT board in.	0
Otherwise - otherwise we t- almost exclusively are working with uh P_C's now, with Intel.	0
In that way Intel succeeded with us, because they gave us too many good machines	0
Yeah.	0
for very little money or nothing. So.	1
Wow!	0
So.	0
So we run everything on Intel.	1
Hmm.	0
And -	0
Does anybody have anything  else?  to -	0
Shall we read some digits?	0
Yeah.	0
Yes.	0
I have to take my glasses -	1
So. Hynek, I don't know if	0
you've ever  done  this. The way that it works is each person goes around in  turn,	0
No.	0
Mm-hmm.	0
and uh	0
you say the  transcript  number and then you read the digits,	0
the - the  strings  of  numbers  as individual digits.	0
So you don't say "eight hundred and fifty", you say "eight five oh", and so forth.	0
O_K.	0
O_K.	0
Um.	0
So can - maybe - can I t- maybe start then?	1
Sure.	0
