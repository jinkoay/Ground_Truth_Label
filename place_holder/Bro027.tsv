Eight, eight?  Three.	0
O_K, we're  going.	0
This is three.	0
Yep. Yep.	0
Test.	0
Hmm.  Let's see.	0
Move it bit.  Test?	0
Test? O_K, I guess it's alright.	0
So, let's see.	0
Yeah, Barry's not here and Dave's not here. Um, I can say about -	0
just q- just quickly to get through it, that Dave and I submitted this A_S_R_U.	0
This is for A_S_R_U.	0
Yeah. So.	0
Um.	0
Yeah, it's -	0
it's interesting. I mean, basically we're dealing with rever- [MASK]	1
and, um, when we deal with [MASK] the technique he's using works really, really well.	1
Uh, and when they had the reverberation  here,  uh, we'll measure the [MASK] and it's, uh, abou	0
So, um,	0
Hmm.	0
a fair amount of -	0
You mean, from the actual, uh, recordings?	0
k-	0
Yeah.	0
It's nine D_B?	0
Yeah. Um -	0
And actually it brought up a question which may be relevant to the [MASK] stuff too.	0
Um, I know that when you figured out the  filters  that we're using for [MASK]	0
there was some experimentation that went on at - at, uh - at [MASK]	0
Um,	0
but one of the differences that we found between the two systems that we were using,  the - [MASK]	0
and the system that we were - the - the uh,  other  system we were using, the uh, [MASK]	0
was that [MASK] had maybe a, um, hundred hertz high-pass.	0
Yep.	0
And the, uh, [MASK] it was like twenty.	0
S- sixty-four.	0
Uh.	0
S- sixty-four.	0
Sixty-four? Uh.	0
Yeah, if you're using the baseline.	0
Is that the ba- band center?	0
No, [MASK]	0
The edge is really, uh, sixty-four? For some reason, uh,	0
Yeah .   @@	0
So the, uh, center would be somewhere around	0
Dave thought it was  twenty,  but.	0
like hundred and -	0
hundred and - hundred - hundred and - maybe - it's like - fi- hundred hertz.	0
But do you know, for instance, h- how far down it would be at twenty hertz?	0
What the - how much [MASK] would there be at twenty hertz, let's say?	0
At twenty hertz.	0
Yeah, any idea what the curve looks like?	0
Twenty hertz frequency -	0
Oh, it's - it's  zero  at twenty hertz, right? The  filter?	0
Yea- actually, the left edge of the first filter is at sixty-four. So -	0
Sixt- s- sixty-four. So anything less than sixty-four is zero.	0
Mmm.	0
It's actually set to zero?	0
Yeah.	0
What kind of filter is  that?	0
Yeah.	0
Is this - oh, from the - from -	0
It -	0
This is the filter bank	0
in the frequency domain that starts at sixty-four. Yeah.	0
Oh, so you, uh - so you really set it to zero, [MASK]	0
Yeah, yeah. So it's - it's a weight on the  ball  spectrum.	0
[MASK]	0
Right. O_K.	0
Um -	0
O_K. So that's - that's a little different than Dave thought, I think. But - but, um,	0
still,  it's possible that we're getting in some more  noise.	0
So I wonder, is it -  @@  Was there - their experimentation	0
with, uh, say, throwing away that filter or something? And, uh -	0
Uh, throwing away the first?	0
Yeah.	0
Um, yeah, we - we've tried including the full -  full  bank.	0
Right? From zero to four K_.	0
And   that's always worse than using sixty-four hertz.	0
Mm-hmm.	0
Right, but the question is, whether sixty-four hertz is - is, uh, too,	0
uh,  low.	0
Yeah, I mean, make it a  hundred  or so?	0
Yeah.	0
I t- I think I've tried a hundred and it was	0
more or less the same, or slightly worse.	0
On what test set?	0
On the same, uh, [MASK] Aurora.	0
Um, it  was  on [MASK]	0
Yeah.	0
So I tried a hundred to four K_. Yeah.	0
Um,	0
So it was -	0
and on - and on the, um, um,	0
T_I-digits  also?	0
No, no, no. I think I just tried it on [MASK]	0
Mmm. That'd be something to look at sometime because what,	0
um,	0
eh, he was looking at was performance in  this  room.	0
Mm-hmm.	0
Would that be more like -	0
Well, you'd  think  that'd be more like [MASK] I guess, in terms of the noise.	0
[MASK] is more, uh, sort of	0
roughly stationary, a lot of it. And -	0
Yeah.	0
and T_I-digits maybe is not so much as - Yeah.	0
Mm-hmm.	0
Yeah.	0
Mm-hmm.	0
O_K. Well, maybe it's not a big deal.	0
But, um -	0
Anyway, that was just something we wondered about.	0
But, um, uh, certainly a lot of the noise,	0
uh, is, uh, below a hundred hertz. Uh, the	0
Yeah.	0
signal-to-noise ratio, you know, looks a fair amount better if you - if you [MASK] from this room.	0
But, um - but it's  still  pretty noisy. Even - even for a hundred hertz up, it's - it's still fairly noisy. The signal-to-noise ratio is - is -	0
Mm-hmm.	0
is actually still pretty bad.	0
So, um, I mean, the main - the - the -	0
Hmm.	0
So that's on th- that's on the f- [MASK] ones though, right? Yeah.	0
Yeah, that's on [MASK] Yeah, the near field's pretty good.	0
So wha- what is, uh - what's  causing  that?	0
Well, we got a - a video projector in here,	0
uh, and, uh - which we keep on during every - every session we record, which, you know, I - I -	0
Yeah.	0
w- we were aware of but - but we thought it wasn't a bad thing. I mean, that's a	0
Uh-huh.	0
Yeah.	0
nice noise source. Uh, and there's also the, uh - uh, air conditioning.	0
Hmm.	0
Which, uh, you know,	0
is a pretty low frequency kind of thing. But - but, uh -	0
Mm-hmm.	0
So, those are - those are major components, I think,	0
I see.	0
uh, for the stationary kind of stuff.	0
Mmm.	0
Um,	0
but, um, it, uh - I guess, I - maybe I said this last week too but it - it - it really became apparent to us that we need to - to take account of noise.	0
And, uh, so I think when - when he gets done with his prelim study I think  one of the next things we'd want to do is to	0
take this, uh - uh, noise, uh, processing stuff and - and,	0
uh - uh, synthesize some speech from it. And then -	0
When are his prelims?	0
Um, I think in about, um,	0
a little less than two weeks.	0
Oh.	0
Wow.	0
Yeah.	0
Yeah. So.	0
Uh, it might even be sooner. Uh, let's see, this is the sixteenth, seventeenth?	0
Yeah, I don't know if he's before - It might even be in a week.	0
A week, week and a half.	0
So, I- Huh. I - I guessed that they were gonna do it some time during the semester but they'll do it any time, huh?	0
They seem to be - Well, the semester actually is starting up.	0
Is it already?	0
Yeah, the semester's late - late August they start here.	0
Yikes.	0
So they do it right at the beginning of the semester.	0
Yeah.	0
Yeah.	0
So, uh - Yep. I mean, that - that was sort of one - I mean,	0
the overall results seemed to be first place in - in - in the case of either,	0
um, [MASK]	0
or a modest sized training set.	0
Uh, either way,	0
uh, i- uh, it helped a lot.	0
And - But if you had a - a really big training set,	0
a recognizer, uh, system that was capable of taking  advantage  of a really large training set -	0
I thought that -  One thing with the [MASK] is that is has the - as we're  using  - the configuration we're  using	0
is w- s- is - being bound by the terms of [MASK] we have	0
all those parameters just set as they  are.  So even if we had a hundred times as much data, we wouldn't	0
go out to, you know,	0
ten or t- or a hundred times as many Gaussians or anything. So,	0
um, it's kind of hard to take advantage of - of - of big chunks of data.	0
Mmm, yeah.	0
Mm-hmm.	0
Uh, whereas the other one does sort of expand as you have more training data. It does it automatically, actually.	0
And so, um,	0
uh,	0
that one really benefited from the larger set. And it was also a diverse set with different noises and so forth. Uh,	0
so, um,	0
that, uh - that seemed to be - So, if you have that -	0
that  better  recognizer that can - that	0
can build up more parameters,	0
and if you, um, have the natural room, which in this case has a p- a pretty bad signal-to-noise ratio,	0
then in that case, um, the right thing to do is just do - u- use speaker adaptation.	0
And - and not bother with - with	0
this acoustic, uh, processing. But I think that that would	0
not be true if we did some explicit noise-processing as well as, uh,	0
Mm-hmm.	0
the convolutional kind of things we were doing.	0
So.	0
That's sort of what we found.	0
Hmm.	0
I, um -	1
uh, started working on the	0
uh -	0
Mississippi State	0
recognizer.	1
Oh, O_K.	0
So, I got in touch with Joe and -	1
and, uh, from your email and things like that. And,	1
uh, they added me to the list -	1
uh, the mailing list. And he gave me all of the	1
O_K, great.	0
pointers and everything that I needed. And so I downloaded the, um -	1
There were two things,	1
uh, that they had to download.	1
One was the, uh, I guess the software.	0
And another  wad   - was a, um,	0
sort of like a sample - a sample run.	0
So I downloaded the software and compiled all of that. And it	1
Eight.	0
compiled fine. No problems.	1
Oh, eh, great.	0
And, um, I grabbed the sample stuff but I haven't, uh,	0
That sample was released only yesterday or the day before, right?	0
compiled it.	0
No - Well, I haven't grabbed  that  one yet. So there's  two.	0
Oh, there is  another  short sample set - o- o- sample. O_K.	0
There was  another   short  one, yeah. And so I haven't grabbed the  latest  one that he just, uh, put out yet.	0
Oh, O_K. F- Yeah, O_K.	0
So.	0
Um, but, the software seemed to compile fine and everything, so.	0
And, um,	0
So.	0
Is there any word yet about the issues about, um, adjustments for different feature sets or anything?	1
No, I - I d-	0
You asked me to write to him and I think I forgot to ask him about that.	1
Yeah.	0
Or if I  did  ask him, he didn't reply. I - I don't remember  yet .	0
Uh, I'll - I'll d- I'll double check that and ask him again.	1
Yeah.	0
Yeah, it's like that - that could r- turn out to be an important issue for us. Yeah.	1
Hmm. Mmm.	0
Yeah. Yeah.	0
Cuz  they have it -	0
Maybe I'll send it to the list.	0
Yeah.	0
Cuz they have, uh, already frozen those in i- [MASK]and all  those  stuff is what - I feel. Because they have this document	1
Uh-huh.	0
explaining the recognizer.	0
And they have these tables with,	1
uh, various language model weights, insertion penalties.	1
u-	0
O_K, I haven't seen that one yet.	0
Uh, it's th- it's there on that web. And, uh, on  that,  I mean, they have run some experiments using various	0
So.	0
O_K.	0
insertion penalties and all those -	0
And so they've picked -	0
Yeah, I think they pi- p- yeah, they picked the values from -	0
the values. Oh, O_K. O_K.	0
For r- w- what test set?	0
Uh, p- the one that they have reported is a NIST evaluation, Wall Street Journal.	0
But that has nothing to do with what we're  testing  on, right?	0
You know. No. So they're, like - um -	0
Mm-hmm.	0
So they are actually trying to, uh,	0
fix that - those values using the clean,	0
uh, training part of the Wall Street Journal. Which is - I mean, the  Aurora.	0
[MASK] has a clean subset. I mean, they want to train it and then this - they're going to run some evaluations.	0
Right.	0
So they're set- they're setting it based on that?	0
Yeah.	0
O_K. So  now,  we may come back to the situation where	1
we may be looking for a modification of the features to account for the fact	0
that we can't modify these parameters. But, um,	1
Yeah.	0
Yeah.	0
uh - but it's still worth, I think, just - since - you know, just chatting with Joe about the issue.	1
Yeah, O_K. Do you think that's something I should just send to  him  or do you think I should send it to this - there's an - a m- a mailing  list.	0
Um -	0
Well, it's not a  secret.  I mean, we're, you know, certainly willing to talk about it with everybody, but I think - I think that, um -	0
um, it's probably best to start talking with him just to -	0
O_K.	0
Uh  @@   you know, it's a dialogue between two of you about what - you know, what does he  think  about this and what - what - you know - what could be  done  about it. Um,	0
Yeah.	0
O_K.	0
if you get ten people in - involved in it there'll be a lot of perspectives based on, you know, how -	0
Yeah.	0
you know. Uh - But, I mean, I think it all  should  come up eventually, but if - if -	0
Right.	0
O_K.	0
if there is any, uh, uh, way to move in - a way that would - that would, you know, be more open to different kinds of	0
features.  But if - if, uh - if there  isn't,  and it's just kind of shut down and - and then  also  there's probably not	0
worthwhile bringing it into a larger forum where - where political issues will come in.	0
Yeah.	0
O_K.	0
Oh.  @@  So this is now - it's - it's compiled under Solaris?	0
Yeah.	0
Yeah, O_K. Because he - there was some mail r- saying that it's - may not be stable for Linux and all those.	0
Yep.	0
Yeah.	0
Yeah, i- that was a particular  version.	0
[MASK]  yeah.	0
Yeah,  [MASK]  or whatever it was but we don't have that. So.	0
Yeah, yeah.	0
Yeah, O_K.	0
O_K, that's fine. Yeah.	0
Should  be O_K. Yeah, it compiled  fine  actually. No - no errors. Nothing. So.	0
That's  good.	0
Uh, this is slightly off topic but, uh,	0
I noticed, just glancing at the, uh, Hopkins	0
workshop, uh, web site that, uh,	0
um -	0
one of the thing- I don't know - Well, we'll see how much they  accomplish,  but one of the things that they were  trying  to do in the	0
graphical models thing was to put together a - a, uh, tool kit	0
for doing, uh r- um, arbitrary	0
graphical models for, uh, speech recognition.	0
Hmm.	0
So - And Jeff, uh - the two Jeffs were	0
Who's the second Jeff?	0
Uh - Oh, uh, do you know Geoff Zweig?	0
No.	0
Oh. Uh, he - he, uh - he was here for a couple years and he, uh - got his P_H_D. He -	1
Oh, O_K.	0
And he's, uh, been at I_B_M for the last couple years.	0
Oh, O_K.	0
So. Uh, so he did - he did his P_H_D on [MASK] uh,	1
Wow.   That  would be neat.	0
for - for speech recognition. He had some continuity built into the model,	1
presumably to handle some, um,	0
inertia in the - in the production system, and,	0
um -	0
Hmm.	0
So.	0
Hmm.	0
Um, I've been playing with, first, the, um, [MASK]	1
Um,  so it's exactly the same approach, but	1
the features that [MASK] use are, uh,	1
Oh, I think I have the results.	0
What was it using before?	1
Before it was just	1
@@	0
[MASK] So.	1
Yeah, it  was  actually - No. Not - I mean, it was just the noisy features I guess. Yeah, yeah, yeah,  not   compensated .	1
Yeah, noisy - noisy features.	0
Um -	0
This is what we get after -	0
This - So, actually, we,  yeah,   here  the features are noise compensated and there is also the [MASK]	0
Um, and then it's a pretty small neural network which use,	0
um,  nine frames of -	0
of six features from [MASK] plus the first derivatives.	0
And it has one hundred hidden units.	0
Is that nine frames u- s- uh, centered around the current frame? Or -	0
Yeah. Mm-hmm.	0
S- so, I'm - I'm sorry, there's - there's - there's how many - how many inputs?	0
So it's twelve times nine.	0
Twelve times nine inputs, and a hundred, uh, hidden.	0
Hidden and	0
Two outputs.	0
two outputs.	0
Two outputs.	0
O_K. So I guess about eleven thousand	0
parameters,	0
Mm-hmm.	0
which - actually shouldn't be a problem, even in - in small phones. Yeah.	0
So, I'm - I'm - s- so what is different between this and -	0
It should be O_K.	0
So the previous syst-  It's based on the system that has a fifty-three point sixty-six percent improvement.	1
and what you -	0
It's the same system. The only	1
thing that changed is the n-	0
a p- eh - a es- the estimation of [MASK]	1
Ah. O_K.	0
Which now is based on, uh, cleaned features.	1
And,  it's a l- it's a lot  better.	1
Wow.	0
Yeah. Um -	0
That's  great.	0
So it's - it's not  bad,  but the problem is still that the latency is too large.	1
What's the latency?	0
Because -	0
um -	0
the - the latency of [MASK] is two hundred and twenty milliseconds.	1
And, uh, the V_A_D is used	0
uh, i- for on-line normalization,	0
and it's used before [MASK]	0
So if you add	0
these components it goes t- to a hundred and seventy, right?	0
I - I'm confused. You started off with two-twenty and you ended up with one-seventy?	0
With two an- two hundred and seventy.	0
If - Yeah, if you add the c- delta comp- [MASK] which is done afterwards.	0
Two-seventy.	0
Oh.	0
Um -	0
So it's two-twenty. I- the- is this - are these twenty-millisecond	0
frames? Is that why? Is it after downsampling? or -	0
The two-twenty is one hundred milliseconds for the um - No, it's forty milliseconds for t- for the, uh,	0
uh,	0
cleaning of the speech.	0
Um - then there is, um, [MASK] which use	0
nine frames. So it adds forty milliseconds.	0
a-	0
O_K.	0
Um, after that, um, you have the um, filtering of the silence probabilities.	0
Which is a million  filter it ,	0
and it creates a one hundred milliseconds delay.	0
So, um -	0
@@	0
Plus there is a delta at the input.	0
Yeah, and there is the delta at the input which is,	0
One hundred	0
um -	0
milliseconds for smoothing.	0
So it's -	0
Uh, median.	0
@@  -	0
It's like forty plus - forty - plus -	0
Mmm. Forty -	0
And then forty -	0
This forty plus twenty, plus one hundred.	0
forty p-	0
@@	0
So it's two hundred actually.	0
Uh -	0
Yeah, there are twenty that comes from -	0
There is ten that comes from [MASK] also. Right?	0
Oh, O_K.	0
Uh, so it's	0
two hundred and ten, yeah.	0
If you are using -	0
Uh -	0
Plus the frame, so it's two-twenty.	0
t- If you are using  three  frames - If you are phrasing f-  using  three  frames, it is thirty here for delta.	0
Yeah, I think it's - it's  five  frames, but.	0
So  five  frames, that's twenty.	0
O_K, so it's who un-  two hundred and ten.	0
Uh, p- Wait a minute. It's forty -  forty for the - for the cleaning of the speech, forty for the I_N_ - A_N_N, a hundred for the smoothing.	0
So. Forty cleaning.	0
Yeah.	0
Well, but at ten - ,	0
Twenty for the delta.	0
At th-  At the  input.  I mean, that's at the input to the net.	0
Twenty for delta.	0
Yeah.	0
And there i-	0
Delta at input to net?	0
Yeah.	0
Yeah.	0
So it's like s- five, six cepstrum plus delta at nine - nine frames of -	0
And then ten milliseconds for -	0
Fi- There's [MASK]	0
ten milliseconds for [MASK]	0
and t- and ten -  another  ten  milliseconds   you said for the frame?	0
For the frame I guess. I computed two-twenty - Yeah, well, it's -	0
I guess it's for the fr - the -	0
O_K. And then there's delta  besides  that?	0
So this is the features that are used by our network and	0
then	0
afterwards,	0
you have to compute the delta on the, uh, main feature stream, which is	0
O_K.	0
um, delta and double-deltas, which is fifty milliseconds.	0
Yeah. No, I mean, the - after the noise part, the forty - the - the other hundred and eighty -	0
Well, I mean,	0
hhh,	0
Wait a minute.  Some  of this is, uh - is, uh - is in  parallel,  isn't it? I mean, [MASK] -	0
Oh, you have [MASK] as part of the V_D_- uh, V_A_D? Or -	0
[MASK] use, uh,	0
Oh, it  does?	0
Mm-hmm.	0
Ah.	0
So in that case there isn't too much  in  parallel.	0
Uh -	0
No. There is,	0
um,	0
just downsampling, upsampling,	0
and [MASK]	0
Um, so [MASK] at the  end  is how much?	0
It's -	0
It's fifty.	0
Fifty.	0
Alright. So -	0
But well, we could probably put the delta, um,	1
before [MASK] It should not that make a big difference, because -	1
What if you used a smaller window for the delta?	1
Could  that  help a little bit?	0
I mean, I guess there's a  lot  of things you could do to -	1
Yeah.	0
Yeah.	0
Yeah, but, nnn -	0
So- Yeah. So if you - if you put the delta before the, uh, ana- on-line - If - Yeah - uh - then - then it could go in parallel. And then y- then you don't have that additive -	1
Mm-hmm. Cuz i-	0
Yep.	0
Yeah, cuz the time constant of the on-line normalization is	1
pretty long compared to the	0
O_K.	0
delta window, so.	1
It should not make -	0
O_K.	0
And you ought to be able to shove  tw- , uh - sh- uh - pull off twenty milliseconds from  somewhere  else to get it under two hundred, right? I mean -	0
Mm-hmm.	0
Is two hundred the d-	0
The hundred milla-	0
mill- a hundred milliseconds for smoothing is sort of an arbitrary amount. It could be eighty and - and probably do  @@  -	0
Yeah, yeah.	0
i- a hun- uh - Wh- what's the baseline you need to be under?	1
Well, we don't know. They're still arguing about it.  I mean, if it's two - if - if it's, uh -	1
Two hundred?	0
@@	0
Oh.	0
if it's two-fifty,	0
then we could keep the delta where it is if we shaved off twenty. If it's two hundred,	1
if we shaved off twenty, we could - we could, uh, meet it by moving the delta back.	1
So, how do you know that what you have is too  much  if they're still deciding?	1
Uh, we don't, but it's just - I mean, the  main  thing is that since that we got burned last time,	1
and - you know, by not worrying about it very much, we're just staying  conscious  of it.	1
Uh-huh.	0
Oh, O_K, I see.	0
And so, th- I mean, if - if - if a week before we have to be done someone says, "Well, you have to have fifty milliseconds less than you have now", it  would  be pretty frantic around here. So -	1
Ah, O_K.	0
Uh -	0
But  still,  that's - that's a pretty big, uh,  win.  And it doesn't seem like you're - in terms of your	0
delay,  you're, uh, that -	0
He added a bit on, I  guess,  because  before  we were - we were - had - were able to have the noise,	0
Hmm.	0
uh, stuff, uh, and [MASK] be in  parallel.  And now he's - he's requiring it to be done first.	0
Well, but- I think the main thing, maybe, is the cleaning of the speech, which takes forty milliseconds or so.	0
And -	0
Right. Well, so you say - let's say ten  milliseconds   - seconds for the L_D_A.	0
and - but - [MASK] is, well, pretty  short  right now. Yeah.	0
Well, ten.	0
And then forty for the other.	0
Yeah, [MASK] - L_D_A - we don't know, is, like - is it very crucial for the features, right?	0
No. I just -	0
This is the first try. I mean, I - maybe the L_D_A's not very useful then.	0
Yeah.	0
S- s- h-	0
Right, so you could start pulling back, but -	0
Yeah, l-	0
But I think you have - I mean, you have twenty for [MASK] which y- now you're sort of doing twice, right? But yo- w- were you doing that before?	0
Mmm.	0
On the - in the - Mm-hmm.	0
Well, in the proposal, um, the input of [MASK] were	0
Just -	0
just three frames, I think.	0
Yeah, just the static, no delta.	0
Uh, static features.	0
Right.	0
So, what you have now is fort- uh, forty for the - the noise, twenty for the delta, and ten for the L_D_A. That's seventy milliseconds	0
@@	0
of stuff which was formerly in parallel, right?	0
So I think,	0
Mm-hmm.	0
you know, that's - that's the difference as far as the  timing,  right?	0
Yeah.	0
Um, and you could experiment with cutting various pieces of these back a bit, but -	1
I mean, we're s- we're not -	1
we're not in terrible shape.	1
Yeah, that's what it seems like to me. It's pretty good.	0
Mm-hmm.	0
Yeah. It's - it's not like it's adding up to four hundred milliseconds or something.	0
Where - where is this -	1
where is this fifty-seven point O_ two in - in comparison to the last evaluation?	1
Well, it's - I think it's better than anything, uh, anybody got.	1
Yeah.	0
Oh, is that right?	0
The best was fifty-four	1
Yeah.	0
point five.	1
Point s-	0
Oh.	0
Yeah. Uh-	0
And our system was	0
forty-nine, but with the neural network.	0
Wow. So  this  is almost ten  percent.	0
With the f-  with  [MASK] Yeah, and r- and -	0
Yeah, so this is - this is like the  first  proposal. The proposal- one.  It was forty-four, actually.	0
It would-	0
Yeah. Yeah. And we still don't have [MASK] in. So - so it's - You know. So it's -	0
Wow.	0
We're - we're doing  better.  I mean, we're getting	0
This is - this is really good.	0
better  recognition.  I mean, I'm sure other people working on this are not sitting still  either,  but -	0
but -	0
Yeah.	0
but, uh -	0
Uh, I mean, the  important  thing is that we	0
learn how to do this better, and,	0
you know. So.	0
Um,	0
Yeah.  So, our,	0
um -	0
Yeah, you can see the kind of - kind of numbers that we're having, say, on [MASK] which is a hard task, cuz	0
it's really, um - I  think  it's just sort of -	0
sort of reasonable numbers,	0
starting   to be.	0
Mm-hmm.	0
I mean, it's still  terri-	0
Yeah, even for a well-matched case it's	0
sixty percent error rate reduction, which is -	0
Yeah.	0
Yeah.  Probably half.	0
Good!	0
Um,	0
Yeah.	0
So actually, this is in between	0
what we had with the previous [MASK] and	0
what Sunil did with [MASK]	0
Which gave sixty-two percent improvement, right?	0
Yeah, it's almost that. It's almost an average somewhere around - Yeah.	0
So -	0
Yeah.	0
What was that? Say that last part again?	0
So, if you use, like, [MASK]	0
o- o-	0
uh, for dropping the frames,	0
Or the best we can get.	0
the best that we can get - i- That means that we estimate the silence probability on the clean version of the utterances.	0
Then you can go up to sixty-two percent error rate reduction, globally.	0
Mmm.	0
Mmm -	0
Yeah.	0
So that would be even - That wouldn't change this number down here to  sixty-two?	0
Yeah.	0
Yeah. So you - you were get-	0
If you add a g- good v-  very  [MASK]	0
Yeah.	0
that works as well as [MASK] working on clean speech,	0
Yeah.	0
then you wou- you would go -	0
So that's sort of the best you could hope for.	0
Mm-hmm.	0
I see.	0
Probably.  Yeah.	0
So fi- si- fifty-three is what you were getting with the old V_A_D.	0
Yeah.	0
And, uh -	0
and sixty-two with the - the, you know, quote, unquote, cheating V_A_D. And fifty-seven is what you got with the real V_A_D.	0
Mm-hmm.	0
That's great.	0
Uh, yeah, the next thing is, I started to play -	0
Well, I don't want to worry too much about the delay, no. Maybe it's better to wait	0
O_K.	0
for the decision	0
Yeah.	0
from the committee.	0
Uh, but I started to play with the, um,	1
uh, [MASK]	1
Mmm	0
I just did the configuration that's very similar to	1
what we did for the February proposal.	1
And -	0
Um. So. There is a f- a first feature stream that use uh straight	1
[MASK]	1
Mm-hmm.	0
Well, these features actually.	0
And the other stream is the output of [MASK] using as input, also, these,	1
um,	0
cleaned	0
M_F_C_C.	1
Um -	0
I don't have the comp- Mmm?	0
Those are th- those are th- what is going into [MASK]	0
Those two?	0
So there is just  this  feature stream,  the fifteen M_F_C_C plus delta and double-delta.	0
No.	0
Yeah?	0
Um, so it's - makes forty-five features  that are used as input to [MASK]	0
And then, there is - there are more inputs that comes from [MASK]	0
Oh, oh. O_K. I see.	0
Yeah, h- he likes to use them  both,  cuz then it has one part that's discriminative, one part that's not.	0
Uh- huh.	0
Yeah. Um -	0
Right. O_K.	0
So, um,	0
uh, yeah. Right now it seems	0
that - i- I just tested on SpeechDat-Car while the experiment are running on  your  - on [MASK]	0
Well, it improves on the well-matched and the mismatched conditions, but it	1
get worse on the highly mismatched.	1
Um,	0
Compared to  these  numbers?	0
Compared to these numbers, yeah.	0
Um, like, on the well-match and medium mismatch, the gain is around five percent relative,	0
y-	0
but it goes down	0
a lot more, like	0
fifteen percent on [MASK]	0
You're just using the full ninety features?	1
@@	0
The -	0
Y- you have ninety features?	0
i-	0
I have, um -	0
From the networks, it's twenty-eight. So -	1
And from the other side it's forty-five. So it's - you have seventy-three features,	1
So, d- i- It's forty-five. Yeah.	0
Yeah.	0
and you're just feeding them like that.	0
Mm-hmm.	0
There isn't any [MASK] or anything?	0
There's a K_L_T after the neural network, as - as before.	1
That's how you get down to twenty-eight?	1
Yeah.	0
Why twenty-eight?	0
I don't know.  Uh.   It's -	0
Oh.	0
i- i- i- It's because it's what we did for the first proposal. We tested,	0
Ah.	0
uh, trying to go down  and   Yeah.	0
It's a multiple of seven.	0
Yeah.   Yeah. Yeah.	0
So -	0
Um.	0
I wanted to do something very similar to the proposal as a first -	1
I see.	0
Yeah.	0
Yeah. That makes sense.	0
first try.	1
But we have to - for sure, we have to go down, because the limit is now sixty features. So,	1
Yeah.	0
uh, we have to find a way to decrease	1
the number of features.	1
Um -	0
So, it seems funny that -	0
I don't know, maybe I don't u- quite understand everything,  but that adding features -	0
I guess - I guess if you're keeping the  back-end   fixed.	0
Maybe  that's  it. Because it seems like just adding information shouldn't give worse results. But I guess if you're	0
keeping the number of Gaussians fixed in the  recognizer,  then -	0
Well, yeah. But, I mean, just in general, adding information -	0
Mmm.	0
Suppose the information you added,  well , was a really terrible feature and all it brought in was  noise.	0
Yeah.	0
Right? So - so, um -	0
Or - or suppose it wasn't	0
completely  terrible,  but it was  completely   equivalent  to  another  one feature that you had,	0
except it was  noisier.	0
Uh-huh.	0
Right? In that case you wouldn't necessarily expect it to be better at  all.	0
Oh, yeah, I wasn't necessarily saying it should be  better.	0
I'm just surprised that you're getting fifteen percent relative  worse	0
Uh-huh.	0
But it's worse.	0
on the wel- On the highly mismatch. Yeah.	0
On the highly mismatched condition.	0
Yeah, I -	0
So, "highly mismatched condition" means that in fact your training is a bad estimate of your test.	0
Uh-huh.	0
So having - having, uh, a g- a l- a greater number of  features,  if they aren't maybe the right features that you use, certainly can e- can  easily,	0
uh, make things worse.	0
I mean, you're  right.  If you have - if you have, uh, lots and lots of data,	0
and you have - and your - your - your training is representative of your test,	0
then getting more sources of information should just help. But - but it's -	0
It doesn't necessarily work that way.	0
Huh.	0
Mm-hmm.	0
So I wonder, um,	0
Well, what's your - what's your thought about what to do next with it?	1
Um, I don't know. I'm surprised, because	1
I expected [MASK] to help more	0
when there is more mismatch, as	0
it was the case for the -	1
So, was the training set same as the p- the February proposal?	0
Mm-hmm.	1
@@	0
Yeah, it's the same training set, so it's [MASK] with	0
O_K.	0
[MASK] uh, noises,	0
Mm-hmm.	0
uh, added.	0
Um -	0
Well, we might - uh, we might have to experiment with, uh	1
better training sets. Again. But, I - The  other  thing is, I mean,  before  you found that was the best configuration, but you might have to retest those things now that we have different -	1
Mm-hmm.	0
The rest of it is different, right? So, um,	1
uh,	0
For instance, what's the effect of just putting	1
[MASK] on without the o- other - other path?	1
Mm-hmm.	0
Yeah.	0
I mean, you know what the  straight  features do. That gives you  this.	1
Mm-hmm.	0
You know what it does in  combination.	0
You don't necessarily know what -	0
What if you did the -  Would it make sense to do the [MASK]	0
on the full set of  combined  features?	0
Instead of just on the -	0
Yeah. I g- I  guess.  Um. The reason I did it this ways is that	0
in February, it - we - we tested different things like that,	0
so, having two [MASK] having ju	0
or having [MASK]	0
Oh, I see.	0
And -	0
So you  tried  [MASK] before and it didn't really -	0
Well -	0
Yeah. And, uh, th- Yeah. The differences between these configurations were not huge, but -	0
I see.	0
it was	0
marginally better with	0
this configuration.	0
Uh-huh. Uh-huh.	0
But, yeah, that's obviously another thing to try,	0
Um.	0
since things are - things are different. And I guess if the -	0
Mm-hmm. Mm-hmm.	0
These are all - so all of these seventy-three features are going into,	0
um,	0
the, uh - the H_M_M.	0
Yeah.	0
And is - are - i- i- are - are any deltas being computed of tha- of  them?	0
Of the straight features, yeah.	0
So.	0
n- Not of the -	0
But n- th- the, um,	0
tandem features are	0
u-	0
Are not.	0
used as they are. So,	0
yeah, maybe we can add some context from these features also as -	0
Could.  i-	0
Dan did in - in his last work.	0
Yeah, but the other thing I was thinking was, um -	0
Uh, now I lost track of what I was thinking. But.	0
What is the -	0
You said there was a limit of sixty features or something?	0
Mm-hmm.	0
What's the relation between that limit and the, um, forty-eight -	0
Oh, I know what I was gonna say.	0
uh, forty eight hundred	0
bits per second?	0
Um, not - no relation. The f- the forty-eight	0
No relation.	0
So I - I - I don't understand, because i-	0
hundred bits is for transmission of some features.	0
I mean, if you're only using h-	0
And generally, i- it - s- allows you to transmit like, fifteen,	0
uh, [MASK]	0
The  issue  was that, um, this is supposed to be a standard that's then gonna be fed to somebody's recognizer somewhere	0
which might be, you know, it - it might be a concern how many parameters are use - u- used and so forth. And so,	0
uh, they felt they wanted to set a limit.	1
So they chose sixty.	1
Some people wanted to use hundreds of parameters and - and that bothered some other people. u- And so	0
Uh-huh.	0
they just chose that. I - I - I think it's kind of r- arbitrary too. But -	1
but that's - that's kind of what was chosen. I - I remembered what I was going to say. What I was going to say is that, um,	0
maybe  -  maybe with the noise removal, uh, these things are now more correlated.	0
So you have two sets of things that are kind of uncorrelated, uh,	0
within  themselves,	0
but they're pretty correlated with one  another.	0
Mm-hmm.	0
And, um,	0
they're being fed into these, uh, variants, only Gaussians and so forth, and - and, uh,	0
Mm-hmm.	0
so maybe it would be	0
a better idea now than it was before to, uh, have, uh, one [MASK] over everything,	0
Mm-hmm.	0
to de-correlate it.	0
Yeah, I see.	0
Maybe.   You know .	0
What are [MASK] in the training set,	0
It's, uh, ranging from	0
zero to clean?	0
Mm-hmm.	0
Yeah. From zero to clean.	0
Yeah.	0
So we found this - this, uh - this Macrophone data,	0
and so forth, that we were using for these other experiments, to be pretty good. So that's - i- after you explore these other alternatives, that might be another way to start looking, is - is just improving the training set.	0
Mm-hmm.	0
Mm-hmm.	0
I mean, we were getting,	0
uh,  lots	0
better	0
recognition	0
using that, than -	0
Of course, you  do  have the problem that,	0
um,	0
u- i-	0
we are not able to increase [MASK] uh, or anything to, uh,	0
uh,	0
to  match  anything. So we're only improving the training of our feature set, but that's still probably something.	0
So you're saying, add [MASK]	0
Yeah, that's the only place that we can  train.  We can't train the other stuff with anything other than the standard amount, so.	0
Yeah.	0
Right.	0
Um,	0
um -	0
What -  what  was it trained on again? The one that you used?	0
It's [MASK] with noise.	0
Uh-huh.	0
So, yeah, it's rather a small -	0
Yeah.	0
How  big  is the  net,  by the way?	0
Um,	0
Uh, it's, uh, five hundred hidden units. And -	0
And again, you did experiments back then where you made it bigger and it - and that was - that was sort of the	0
threshold point. Much less than that, it was worse, and	0
Yeah.	0
Yeah.	0
much more than that, it wasn't much better.	0
Hmm.	0
So is it - is it  though  the performance,	0
Yeah.  @@ ?	0
big relation in the high ma- high mismatch has something to do with the,	0
uh, cleaning up	0
that you - that is done on [MASK] after adding noise? So -	0
it's - i- All the noises are from [MASK] right?	0
Yeah.	0
So you - i-	0
Um -	0
Well, it- it's like the high mismatch of [MASK]	0
They - k- uh -	0
after cleaning up, maybe having more noise than the -	0
the training set of [MASK] after clean - s- after you do the noise clean-up.	0
Mmm.	0
I mean,  earlier  you never  had  any compensation, you just trained it straight away.	0
Mm-hmm.	0
So it had like all these different conditions of [MASK]	0
Mm-hmm.	0
actually in their training set of [MASK]	0
Mm-hmm.	0
But after cleaning up you have now a different set of [MASK] right?	0
For the training of [MASK]	0
Yeah.	0
Mm-hmm.	0
And -	0
is it something to do with the mismatch that - that's created  after  the cleaning up, like the high mismatch -	0
You mean the - the most noisy	0
occurrences on [MASK] might be	0
Mm-hmm.	0
a lot more noisy than -	0
Of - that - I mean, [MASK] after the noise compensation of the SpeechDat-Car.	0
Oh, so - Right. So the training - the - the  neural net   is being trained with noise compensated	0
Maybe.	0
@@	0
Yeah.	0
Yeah, yeah.	0
stuff. Which makes  sense,	0
Yeah.	0
but, uh, you're saying - Yeah, the noisier	0
ones are  still  going to be,	0
Yeah.	0
even after our noise compensation, are still gonna be pretty noisy.	0
Mm-hmm.	0
Yeah, so now [MASK] is seeing a different set	0
Of [MASK] Because in [MASK] it was zero to some clean.	0
Right.	0
So the net saw all [MASK]  @@	0
Yes.	0
conditions. Now after cleaning up it's a different set of [MASK]	0
Right.	0
Right.	0
And [MASK] may not be, like, com- covering the whole set	0
Right, but [MASK] that you're seeing is  also  reduced in noise by the  noise  compensation.	0
Yeah, yeah, yeah, yeah, it  is.  But, I'm saying, there could be some -	0
Yeah.	0
So.	0
Mm-hmm.	0
some issues of -	0
Yeah.	0
Well, if the initial range of [MASK] is different, we - the problem was already there before. And -	0
Yeah.	0
Because -	0
Mmm -	0
Yeah, I mean, it depends on whether you believe that the noise compensation is equally reducing the noise on the test set and the training set.  Uh -	0
Hmm.	0
On the test set, yeah.  @@	0
Right? I mean, you're saying there's a mismatch in  noise	0
Hmm.	0
Mm-hmm.	0
that wasn't there  before,  but if they were both the  same  before, then if they were both reduic- reduced  equally,	0
Mm-hmm.	0
then,	0
there would  not  be a mismatch.	0
So, I mean, this may be -	0
Heaven forbid, this	0
noise compensation process may be  imperfect,	0
but.  Uh, so maybe it's treating some things differently.	0
Well, I -	0
Yeah, uh -	0
I don't know. I - I just - that could be seen from [MASK] uh, testing condition because, um, the noises are from [MASK] right? Noise -	0
Yeah. So -	0
So cleaning up [MASK] and if the performance	0
goes  down   in  [MASK] mismatch -  high  mismatch like this -	0
Clean training, yeah.	0
on a clean training, or [MASK]	0
Yeah, we'll - so we'll see. Uh. Maybe.	0
Yeah. Then it's something to do.	0
Mm-hmm.  Yeah.	0
I mean, one of the things about - I mean, the [MASK]	0
um, I think, you know, it was recorded over many different telephones.	0
Mm-hmm.	0
And, um, so, there's lots of different kinds of acoustic conditions.	0
I mean, it's not artificially added noise or anything.	0
So it's not the same. I don't think there's  anybody  recording over a car	0
from a car, but - I think it's - it's varied enough that if - if doing this adjustments, uh, and playing around with it	0
doesn't, uh, make it better, the most - uh, it seems like the most obvious thing to do is to improve the training set.	0
Um - I mean, what we were -	0
uh - the condition - It - it gave us an  enormous  amount of improvement in what we were doing with  [MASK] even though	0
there,  again,  these m- Macrophone	0
digits were very, very  different  from, uh,	0
what we were going on  here.  I mean, we weren't talking over a telephone here.	0
But it was just - I think just having a - a nice variation in	0
acoustic conditions was just a good thing.	0
Mm-hmm.	0
Mmm.	0
Yep.	0
Yeah, actually  to s- eh, what I observed in [MASK]	1
case is that	0
the number of deletion	0
dramatically increases. It -	1
it doubles.	1
Number of   deletions.	0
When I added the num- [MASK] it doubles the number of deletions.	1
Yeah, so I don't you know	1
how to interpret that, but, mmm -	1
Yeah. Me  either.	1
t-	0
And - and did - an- other numbers stay the same? Insertion substitutions stay the same?	1
They p- stayed the same, they - maybe they are a little bit	1
Roughly?	0
uh, lower.	0
Uh-huh.	0
They are a little bit better. Yeah. But -	0
Mm-hmm.	0
Did they increase the number of deletions even for the	1
cases that got  better?  Say, for the - I mean, it - So it's only the highly mismatched?	1
No, it doesn't. No.	1
And it - Remind me again, the "highly mismatched" means that the -	0
Clean training and -	0
Uh, sorry?	0
It's clean training - Well, close microphone training and	0
Close mike training -	0
distant microphone, um, high speed, I think. Well -	0
The most noisy cases are the distant microphone for testing.	0
Right.	0
So -	0
Well, maybe the noise subtraction is	0
subtracting off  speech.   Wh-	0
Separating .	0
Yeah.	0
But - Yeah.	0
I mean, but without [MASK] it's - well, it's better. It's just when w	0
Yeah, right. Uh, that's right, that's right. Um -	0
Well that - that says that, you know, the, um -	0
the models in - in, uh,	0
the recognizer are really paying attention to [MASK] features.	0
Yeah.  Mm-hmm.	0
Uh.	0
But, yeah,  actually  -	0
[MASK] noises  are sort of a range of noises and they're not so much the stationary	0
driving kind of noises, right? It's - it's pretty different. Isn't it?	0
Uh, there  is  a car noise. So there are f- just four noises. Um,	0
uh, "Car", I think,	0
"Babble."	0
"Babble", "Subway", right? and -	0
"Street" or "Airport" or something.	0
and - "Street" isn't - " Train  station", yeah.	0
Or "Train station".	0
Yeah.	0
So - it's mostly - Well, "Car" is stationary,	0
Mm-hmm.	0
"Babble",	0
it's a stationary background plus some voices,	0
Mm-hmm.	0
some speech	0
over it. And	0
the other two are rather stationary also.	0
Well, I - I think that	0
if you run it -	0
Actually, you - maybe you remember this. When you - in - in the  old  experiments when you ran	1
with [MASK] only, and didn't have this side path,	0
um, uh, with the - the pure features as well,	0
Mm-hmm.	0
did it	0
make things  better  to have [MASK] Was it about the  same?	1
Uh, w- i-	0
It was -	1
b- a little bit worse.	1
Than - ?	0
Than just the features, yeah.	0
So,	0
until you put the second path in with the pure features, the neural net wasn't helping at all.	1
Mm-hmm.	0
Well,  that's  interesting.	0
It was helping,	1
uh, if the features are b-	0
were bad, I mean.	1
Yeah.	0
Just plain [MASK]	0
Yeah.	0
But	1
as soon as we added [MASK] and	0
all these things, then -	1
They were doing similar enough things.	1
Well, I  still  think it would be k- sort of interesting to see	0
what	0
would happen if you just had [MASK] without the side thing. And - and the thing I - I have in mind is,	1
Yeah, mm-hmm.	0
uh,  maybe  you'll see that the results are  not  just a little bit worse. Maybe that	1
they're a  lot  worse.	1
You know? And, um -	0
But if on the ha- other hand,	1
uh, it's, say, somewhere in  between  what you're seeing now and - and - and, uh, what you'd have with just the pure features,	0
then maybe there  is  some problem of a -	0
of a, uh, combination	0
of these things, or correlation between them somehow.	1
Mm-hmm.	0
If it really is that the  net  is  hurting  you at the moment, then	1
I think the issue is to	0
focus on - on, uh, improving the - the  net.	1
Yeah, mm-hmm.	0
Um.	0
So what's the overall effe- I mean, you haven't done all the experiments but you said	0
it was i-	0
somewhat better, say, five percent better, for the first two conditions, and fifteen percent worse for the  other  one?	0
But it's - but of course that one's  weighted  lower, so I wonder what the net  effect  is.	0
Y- yeah, oh. Yeah.	0
I d- I -	0
I think it's - it was one or two percent.	0
That's not that bad, but it was l- like two percent	0
relative worse on SpeechDat-Car.	0
I have to - to check that. Well, I have - I will.	0
Well, it will -  overall  it will be still better even if it is fifteen percent worse,	0
because the fifteen percent	0
worse is given like f- w- twenty-five -	0
Right.	0
point two five eight.	0
Mm-hmm.	0
Hmm.	0
Right. So the - so the worst it could be, if the others were  exactly  the same, is  four,	0
Is it like -	0
Yeah, so it's four.	0
and - and, uh, in fact since the others are somewhat better -	0
Is i-	0
So either it'll get cancelled out, or you'll get, like, almost the same.	0
Yeah, it was - it was slightly worse. Um,	0
Uh.	0
Slightly bad.	0
Yeah.	0
Yeah, it should be pretty close to cancelled out.	0
Yeah.	0
Mm-hmm.	0
You know,  I've  been wondering about something. In the, um - a lot of the, um -	1
the Hub-five systems, um, recently have been using [MASK]	1
and - and they, um -	1
They run [MASK] on the features right before they train the models.	1
So there's the - [MASK] is - is right there b	0
Yeah.	0
So, you guys are using [MASK] but it seems like it's pretty far back in the process.	0
Uh, [MASK] is different from [MASK] that you are talking abou	1
saying is, like, you take a block of features, like nine frames or something,	0
Yeah.	0
Uh-huh.	0
and then do [MASK] on it, and then redu	1
And then feed it to H_M_M.	0
Yeah, you c- you c- you  can.  I mean, it's - you know, you're just basically i-	0
Yeah, so this is like a two d-	0
two dimensional  tile .	0
You're shifting the feature space.  Yeah.	0
So this is a two dimensional  tile .	1
And [MASK] that we are f- applying is only in time, not in frequency -	1
high  cost frequency. So it's like - more like a filtering in time, rather than	1
Ah. O_K.	0
doing a r-	0
So what i- what about, um - i- u-	0
what i- w- I mean, I don't know if this is a good idea or  not,  but what if you put - ran the  other  kind of L_D_A,	1
uh, on your features right before they go into	0
Uh, it -	0
the H_M_M?	1
m-	0
Mm-hmm. No, actually, I think - i- Well. What do we do with [MASK] is -	0
is something like that except that it's not linear. But	1
Yeah.	0
it's - it's like [MASK] But.	1
Right, it's the - It's - Right. The - So - Yeah, so it's sort of like - The  tandem  stuff is kind of like i-	1
Yeah. It's -	0
[MASK]  I g-  Yeah.	1
Yeah.	0
Yeah.	0
Uh.	0
But I mean, w- but the  other  features that you have, um,	1
th- [MASK]	1
Mm-hmm.	0
Yeah, I know. That - that - Yeah. Well, in the proposal, they were	1
transformed u- using [MASK] but -	1
Uh-huh.	0
Yeah, it might be [MASK]	1
The a- the  argument  i- is kind of i- in - and it's not like we really  know,   but the  argument  anyway is that, um,	0
could be better.	1
uh, we  always  have the prob- I mean, discriminative things are good. [MASK]	0
Yeah.	0
Uh, they're good because you - you - you learn to distinguish between these categories that you want to be good at distinguishing between.	0
And [MASK] doesn't do that. It - P_A_C- P_C_A -	0
[MASK] throws away pieces that are	0
uh, maybe not - not gonna be helpful just because they're  small,  basically.	0
Right.	0
But, uh, the problem is, training sets aren't perfect and testing sets are different.	0
So you f- you - you face the  potential  problem with discriminative stuff, be it [MASK] that you are training	0
to discriminate between categories in  one  space but what you're  really  gonna be g- getting is - is something  else.	0
Uh-huh.	0
And so, uh,  Stephane's  idea was,	0
uh, let's feed, uh,  both  this discriminatively trained thing	0
and something that's  not.	0
So you have a  good  set of features that everybody's worked really hard to make,	0
Yeah.	0
and then, uh, you - you discriminately train it, but you  also	0
take the path that - that  doesn't  have that, and putting those in  together.	0
Uh-huh.	0
And that - that seem- So it's kind of like a combination of the -	0
uh, what, uh, Dan has been calling, you know, a feature - uh, you know, a feature combination versus posterior combination or something. It's -	0
it's, you know, you  have  the posterior combination but then you get the features from  that  and use them as a feature combination with these - these other things.	0
And that seemed, at least in the  last  one, as he was just saying, he -	0
he - when he  only  did discriminative stuff,	0
Yeah.	0
i- it actually was - was - it didn't help at  all  in this particular case. There was enough of a difference, I guess, between the	0
testing and training.	0
But by having  them    both  there - The fact is  some  of the time,	0
the discriminative stuff is gonna  help  you.	0
Mm-hmm.	0
And  some  of the time it's going to  hurt  you, and by combining two information sources if, you know - if - if -	0
Right.	0
So you wouldn't necessarily then want to do [MASK] on	0
[MASK] because	0
That i- i-	0
now you're doing something to them that -	0
I think that's counter to that idea. Now, again, it's - we're just trying these different things. We don't really know what's gonna work best. But	0
Yeah, right.	0
if that's the hypothesis, at least it would be counter to that hypothesis to do that.	0
Right.	0
Um, and in  principle  you would  think  that the neural net would do	0
better	0
at the  discriminant  part than [MASK]	0
Right.	0
Yeah. Well - y-	0
Though, maybe  not.	0
Yeah.   Exactly.  I mean, we, uh - we were getting ready to do the tandem, uh, stuff for [MASK]	0
and, um, Andreas and I  talked  about it, and	0
the idea w- the  thought  was, "Well,	0
uh, yeah, that i- you know - th- [MASK] should be  better,  but we should at least have	0
uh, a  number,  you know, to show that we did try [MASK]	0
in  place  of [MASK] so that we can	0
Right.	0
you know, show a clear  path.  You know, that you have it  without  it, then you have [MASK] then you have [MASK] and you can see,	0
theoretically. So.	0
I was just wondering - I - I -	0
Well, I think that's a good idea.	0
Yeah.	0
Did - did you  do  that or - tha- that's a -	0
Um. No. That's what - that's what we're gonna do  next  as soon as I finish this other thing. So.	0
Yeah.  Yeah. No, well, that's a good idea.	0
I - I -	0
i- Yeah.	0
We just want to  show.  I mean, it - everybody  believes  it, but you know, we just -	0
Oh, no it's a g-	0
No, no, but it might not - not even be  true.  I mean, it's - it's - it's - it's - it's a great idea. I mean,	0
Yeah.	0
one of the things that always disturbed me, uh, in the - [MASK] that happened in the eighties was that, um,	0
a lot of people - Because [MASK] were pretty easy to - to use -	0
Yeah.	0
a lot of people were just using them for all sorts of things without,	0
uh, looking at  all  into the linear, uh - uh,  versions  of them. And,	0
Mm-hmm.	0
Yeah.	0
uh, people were doing [MASK] but not looking	0
Yeah, and everybody's putting that on their   systems  now, and so, I- that's what made me wonder about	0
Well, they've been putting them in their systems off and on for ten years, but - but - but, uh,	0
this, but.	0
Yeah, what I mean is it's - it's like in [MASK] you know, and you read the system descriptions and	0
And now they all have that.	0
everybody's  got,  you know, [MASK] on their features. And so. Uh.	0
I see.	0
Yeah.	0
It's the transformation they're estimating on -	0
Well, they are trained on the same	0
data	0
as the final  H_M_M  are.	0
Yeah, so it's  different.  Yeah, exactly. Cuz  they  don't have these, you know, mismatches that - that  you  guys have. So that's why I was wondering if maybe it's not even a good idea. I don't know.	0
Mm-hmm.	0
Mm-hmm.	0
I - I don't know enough about it, but - Um.	0
Mm-hmm.	0
I mean,  part  of why - I - I think part of why you were getting into [MASK] - Y- you were	0
describing to me at one point that you wanted to	0
see if,	0
uh, you know, getting good orthogonal features was - and  combining  the - the different	0
temporal	0
ranges  - was the key thing that was happening or whether it was this discriminant thing, right? So you were just trying -	0
I think you r- I mean, this is - it doesn't have the	0
[MASK] aspect but th- as far as the	0
orthogonalizing transformation, you were trying  that  at one point, right?	0
Mm-hmm.	0
Mm-hmm.	0
I think you were.	0
Yeah.	0
Does  something.  It doesn't work as well.	0
Yeah.	0
Yeah.	0
So, yeah, I've been exploring [MASK] withou	0
less latency using [MASK]	0
um, after the cleaning up.	0
So what I'd been trying was, um,	0
uh -	0
After the b- after the noise compensation,	0
n- I was trying t- to f- find a f- feature based on the ratio of the energies, that is, cl- after clean and before clean.	0
So that if - if they are, like, pretty c- close to one, which means it's speech. And if it is n- if it is close to zero, which is -	0
So it's like a scale  @@  probability value.	0
So I was trying, uh, with full band and multiple bands,	0
m- ps- uh - separating them to different frequency bands and deriving separate decisions on each bands, and trying to combine them.	0
Uh,	0
the advantage being like it doesn't have the latency of the neural net if it - if it can g- And  it gave me like, uh, one point -	0
Mm-hmm.	0
One - more than one percent  relative   improvement. So, from fifty-three point six it went to fifty f- four point eight. So it's, like,	0
only  slightly more than a percent improvement, just like -	0
Mm-hmm.	0
Which means that it's - it's doing a slightly better job than the previous [MASK]	0
Mm-hmm.	0
uh, at a l- lower delay.	0
Mm-hmm.	0
Um, so, um - so - u-	0
But - i- d- I'm sorry, does it still have [MASK] stuff?	0
It  still  has the median filter. So -	0
So it still has  most  of the delay, it just doesn't -	0
Yeah, so d- with the delay, that's gone is the  input,  which is the sixty millisecond.	0
The forty plus  twenty.	0
At the input of [MASK] you have this, uh, f- nine frames of context plus the delta.	0
Well, w- i-	0
Mm-hmm.	0
Oh, plus the delta, right. O_K.	0
Yeah. So that delay, plus the L_D_A.	0
Mm-hmm.	0
Uh, so the delay is only the forty millisecond of the noise cleaning, plus the hundred millisecond smoothing at the output.	0
Mm-hmm. Mm-hmm.	0
Um.	0
So. Yeah. So the - the - di- the biggest -	0
The problem f- for me was to find a consistent threshold that works  well across the different databases, because I t-	0
I try to make it work on tr- [MASK] and it fails	0
Mm-hmm.	0
Mm-hmm.	0
So, um.	0
So there are - there was, like, some problem in balancing the deletions and insertions when I try different thresholds. So -	0
Mm-hmm.	0
The -	0
I'm still trying to make it	0
better by using some other features from the -	0
after the p- clean up - maybe, some,	0
uh, correlation - auto-correlation or some s- additional features  of  - to mainly	0
the improvement of the  VAD .	0
I've been trying.	0
Now this - this - this, uh,	0
"before and  after  clean", it sounds like you think that's a good  feature.	0
That - that, it - you th- think that the, uh - the -	0
i- it appears to be a good feature, right?	0
Mm-hmm. Yeah.	0
What about using it in [MASK]	0
Yeah, eventually we could - could just	0
Yeah, so - Yeah, so that's the - Yeah. So we've been thinking about putting it into [MASK] also.	0
Yeah.	0
Because they did - that itself -	0
Then you don't have to worry about the thresholds and -	0
There's a threshold and - Yeah. Yeah. So that - that's, uh -	0
Yeah.	0
but just -	0
Yeah. So if we - if we can  live  with the latency or cut the latencies  elsewhere,  then - then that would be a,	0
Yeah. Yeah.	0
uh,	0
good thing. Um, anybody - has anybody - you guys or - or  Naren , uh, somebody, tried the, uh,	0
um,	0
second th- second stream thing?	0
Uh.	0
Oh, I just - I just h- put the second stream in place and, uh	0
ran one experiment, but just like - just to know that everything is fine.	0
Uh-huh.	0
So it was like, uh, forty-five [MASK] plus twenty-three mel -  log  mel.	0
Yeah.	0
And - and , just, like, it gave me the baseline performance of [MASK] which is like	0
zero improvement.	0
Yeah.	0
Yeah.	0
So I just tried it on Italian just to know that everything is -  But I -  I  didn't export  anything  out of it because it was, like, a weird feature set.	0
Yeah.	0
So.	0
Yeah. Well, what I think, you know, would be  more  what you'd want to do is - is - is, uh, put it into another  neural  net.	0
Yeah, yeah, yeah, yeah.	0
Mm-hmm.	0
Right? And then -	0
But, yeah, we're - we're not quite there yet. So we have to  figure out the neural nets, I guess.	0
Yeah.	0
The uh,  other  thing I was wondering was, um,	1
if [MASK] um, has any - because of the different noise con- unseen noise conditions for the	0
neural net, where, like, you train it on those four noise conditions,	1
Mm-hmm.	0
while you are feeding it with, like,	0
a- additional - some four plus some - f- few more conditions which it hasn't seen, actually,	1
from the - f- f- while testing. Um -	0
Yeah, yeah. Right.	0
instead of just h- having c-	1
uh, those cleaned up t- cepstrum, sh- should we feed some additional information, like -	0
The - the - We  have  [MASK]  I mean, should we f- feed the V_A_D flag, also, at the input so that it - it has some additional discriminating information at the input?	1
Hmm- hmm!  Um -	0
Wh- uh, the - the V_A_D what?	0
We have [MASK] information also available at the  back-end.	1
Uh-huh.	0
So if it is something [MASK] is not able to discriminate the classes -	1
Yeah.	0
I mean -	0
Because most of it is sil-	0
I mean, we have dropped some silence f-	0
We have dropped so- silence frames?   No,  we  haven't  dropped silence frames  still.	0
Mm-hmm.	0
Uh, still  not.  Yeah.	0
Yeah. So - the b- b- biggest classification would be the speech and silence.	0
Th-	0
So, by having an additional, uh, feature which says "this is speech and this is  nonspeech ", I mean, it certainly helps in some unseen noise conditions for the  neural  net.	1
What -	0
Do y- do you have that feature available for the  test  data?	0
Well, I mean, we have - we are transferring [MASK] to the  back-end  -  feature  to the  back-end.  Because we are dropping it at the  back-end  after everything - all the features are computed. So -	0
Oh, oh,  I  see.  I  see.	0
so the neural - so that is coming from a separate neural net or  some  [MASK]	0
O_K. O_K.	0
Which is - which is certainly	0
giving a	0
So you're saying, feed that,  also,  into	1
@@  to -  Yeah.  So it- it's an - additional discriminating information.	1
the neural net. Yeah.  Yeah.	1
Right.	0
You could feed it into the neural net. The  other  thing  you could do	1
So that -	0
is just, um, p-	0
modify the, uh,  output  probabilities of the - of the, uh,	0
uh,	0
um, [MASK]	1
Mm-hmm.	0
Right?	0
Mm-hmm.	0
So you have [MASK] of what the silence probability is,	0
and you could  multiply  the two things, and renormalize.	0
Uh, I mean, you'd have to do the	0
Yeah.	0
nonlinearity part and deal with  that.  Uh, I mean, go backwards from what the nonlinearity would, you know - would be. But - but, uh -	0
Through  - t- to the soft  max .	0
Yeah, so - maybe, yeah, when -	0
But in principle wouldn't it be  better  to feed it in? And let the  net  do that?	0
Well, u- Not  sure.	0
I mean, let's put it  this  way. I mean, y- you - you have this complicated system with thousands and thousand parameters	0
Hmm.	0
Yeah.	0
and you can tell it, uh, " Learn  this  thing. "	0
Or you can say, "It's  silence!  Go  away! "	0
I mean,  I mean, i- Doesn't - ? I think - I think the  second  one sounds a lot more  direct.   Uh.	0
What -	0
what if you -  Right.	0
So, what if you then, uh - since you  know  this, what if you only	0
use [MASK] on the  speech  portions?	0
Well, uh,	0
That's what -	0
Well, I guess that's the  same.  Uh, that's  similar.	0
Yeah, I mean, y- you'd have to actually  run  it  continuously,  but it's -  @@  -	0
But I mean - I mean,  train  the net only on -	0
Well, no, you want to train on - on the nonspeech  also,  because that's part of what you're  learning  in it,	0
to - to - to generate, that it's - it has to distinguish  between.	0
Speech.	0
But I mean, if you're gonna - if you're going to multiply the output of the net by this other  decision,	0
uh,	0
would - then you don't  care  about whether [MASK] makes that distinction, right?	0
Well, yeah. But this  other  thing isn't  perfect.	0
Ah.	0
So that you bring in  some  information from [MASK]  itself.	0
Right, O_K. That's a good point.	0
Yeah. Now the only thing that - that bothers me about all this is that I - I - I -	1
The - the fact -	1
i- i- It's  sort  of bothersome that you're getting more  deletions.	1
Yeah.	0
But -	1
So I might maybe look at,	1
is it due to the fact that	1
um, the probability of the silence at the output of the network, is,	1
uh,	0
Is too  high.	0
too - too high or -	1
If it's the case, then multiplying it again by -	0
Yeah. So maybe - So -	0
It may not be - it -	0
i- by something? Mm-hmm.	0
Yeah, it - it may be too - it's too high in a sense, like, everything is more like a, um,	0
Yeah.	0
flat probability.	0
Yeah.	0
So, like, it's not  really  doing any distinction between speech and nonspeech - or, I mean, different -  among  classes.	0
Oh-eee-hhh.	0
Uh, yeah.	0
Yeah.	0
Mm-hmm.	0
Be interesting to look at the - Yeah, for the -	0
I wonder if you could  do  this.	0
But if you look at the, um, highly mism- high mismat- the output of the net on the  high  mismatch case and just look at, you know, the  distribution	0
versus the - the  other  ones, do you - do you see more  peaks  or something?	0
Yeah.	0
Yeah, like the entropy of the - the output, or -	0
Yeah.	0
Yeah, for instance.	0
It - it seems that [MASK] doesn't - Well,	0
But I - bu-	0
it doesn't drop,	0
uh, too many frames because the dele- the number of deletion is reasonable.	0
But it's just when we add [MASK]	0
[MASK] and then -	0
Yeah. Now the  only  problem is you  don't  want to ta- I guess  wait  for the  output  of the V_A_D	0
u-	0
before you can put something  into  the other  system,  cuz that'll shoot up the latency a  lot,  right? Am I  missing  something here?	0
Mm-hmm.	0
But -	0
Yeah.	0
Right.	0
Yeah. So that's maybe a  problem  with what I was just saying. But -	0
but - I- I guess -	0
But if you were gonna  put  it in as a  feature  it means you already  have  it by the time you get to the  tandem  net, right?	0
Um, well. We - w- we don't have it, actually, because it's - it has a high  rate energy  - [MASK] has a -	0
No.	0
Ah.	0
Yeah.	0
O_K.	0
It's  kind  of done in - I mean,  some  of the things are,	0
not in parallel,  but	0
certainly,	0
it would be in parallel with the - with [MASK]	0
Right.	0
In time.	0
So maybe, if that doesn't work, um -	0
But it would be interesting to see if that was the problem, anyway.	0
And - and - and then I guess another alternative  would  be to take the feature that you're feeding into [MASK]	0
Mm-hmm.	0
and feeding it into the other one as well.	0
Mm-hmm.	0
And then maybe it would just learn -  learn  it better.	0
Um -	1
But that's - Yeah, that's an interesting thing to try to  see,  if what's going on	0
is that in the highly mismatched condition,	0
it's, um, causing deletions by having this silence probability up - up too high,	0
Mm-hmm.	0
at some point where [MASK] is saying it's actually speech.	0
Yeah. So, m-	0
Which is probably  true.	0
Cuz - Well, the V_A_- if [MASK] said - since [MASK] is - is - is  right  a lot,  uh -	0
Yeah.	0
Hmm. Anyway.	0
Might  be.	0
Mm-hmm.	0
Yeah. Well, we just started working with it. But these are - these are some good ideas I think.	0
Mm-hmm.	0
Yeah, and the other thing - Well, there are other issues maybe for the tandem, like,	0
uh, well, do we want to, w- uh n-	0
Do we want to work on the targets? Or,	0
like, instead of using phonemes, using more [MASK]	0
units?	0
For [MASK] you mean? Hmm.	0
Well, I'm -	0
Yeah. I'm thinking, also, a w- about	0
Dan's work	0
where he -	0
he trained  [MASK] not on phoneme targets but on the H_M_M state targets.	0
And -	0
it was giving s- slightly better	0
results.	0
Problem  is, if you are	0
going to run this on different	0
m-	0
Yeah.	0
test sets, including large vocabulary,	0
Yeah.	0
um,	0
Uh -	0
Mmm. I was just thinking maybe about,	0
I think -	1
like, [MASK] and -	0
come up with a -	0
a reasonable,	0
not too large, [MASK] and -	0
and -	0
Yeah.	0
And then anyway we would have to reduce this with [MASK] So. But -  I don't know.	0
Yeah.	0
Yeah.  Well, maybe.	0
Mm-hmm.	0
But I d- I d- it - it - i-	0
it's all worth looking at, but it sounds to me like, uh, looking at the relationship between this and the - speech noise stuff is - is -	0
Mm-hmm.	0
is probably a key thing.	0
That and the correlation  between  stuff.	0
So if, uh -	0
if the, uh, high mismatch case had been more like the,	0
uh,	0
the other two cases  in terms of giving you just a better performance,	0
Mm-hmm.	0
how would this number have changed?	0
Oh, it would be -	0
Yeah.	0
Around five percent  better,  I guess. If -	0
y- Like sixty?	0
if - i-	0
Well, we don't know what's it's gonna be [MASK] yet. He hasn't got the results back yet.	0
Yeah.	0
If you extrapolate [MASK] well-matched and medium-mismatch,	0
Uh-huh.	0
Yeah.	0
it's around, yeah, maybe five.	0
So this would be	0
sixty-two?	0
Sixty- two.	0
Sixty-two, yeah.	0
Yeah.	0
Somewhere around sixty, must be.	0
Which is -	0
Right? Yeah.	0
Well, it's around five  percent,  because it's - s- Right? If everything is five  percent.	0
Yeah. Yeah.	0
Mm-hmm.	0
All the  other  ones were five percent, the -	0
I d- I d- I just have [MASK] right now, so -	0
Yeah.	0
Yeah.	0
It's running - it shou- we should have the results today during the afternoon, but -	0
Hmm.	0
Well.	0
Hmm.	0
Well -	0
Um -	0
So I won't be here	1
for -	1
When -	0
When do you leave?	0
Uh, I'm leaving next Wednesday.	1
May or may not be in in the morning. I leave in the afternoon.	0
Um, so I -	0
But you're - are you - you're not gonna be around this afternoon?	0
Yeah.	0
Oh, well. I'm talking about  next  week. I'm leaving - leaving  next  Wednesday.	1
Oh.	0
This afternoon - uh - Oh, right, for the Meeting meeting? Yeah, that's just cuz of something on campus.	0
Uh-huh.	0
Ah, O_K, O_K.	0
Yeah.	0
But, um,	0
yeah, so next week I won't,	1
and the week after I won't, cuz I'll be in Finland.	1
And the week after that I won't.	0
By that time  you'll  be -	1
Uh, you'll  both  be gone  from  here.	1
So there'll be no - definitely no meeting on - on September sixth.	0
Uh, and -	0
What's September sixth?	0
Uh, that's during  [MASK]	0
Oh, oh, right. O_K.	0
So, uh, Sunil will be in Oregon. Uh, Stephane and I will be in Denmark.	0
Uh -	0
Right?	0
So it'll be a few weeks, really, before we have a meeting of the same	1
cast of characters.	1
Um, but, uh -	0
I guess,	0
just -	0
I mean, you guys should probably meet. And maybe Barry - Barry will be around. And -	0
and then uh,	1
uh, we'll start up again with	0
Dave and - Dave and Barry and Stephane	0
and us on the, uh,	0
twentieth.	1
No.	0
Thirteenth?	0
About a month?	0
So, uh, you're gonna be gone for the next	0
three  weeks or something?	0
I'm gone for two and a half weeks starting - starting next Wed- late next Wednesday.	0
So that's - you won't be at the next three of these meetings.	0
Is that right?	0
Uh, I won't -	0
it's probably  four  because of -	0
is it  three?  Let's see,	0
twenty-third, thirtieth,	0
sixth. That's right, next three.	0
And the - the  third  one	0
won't - probably won't  be  a meeting, cuz - cuz, uh, Su- Sunil, Stephane, and I will all not be here.	0
Oh, right. Right.	0
Um -	0
Mmm.  So it's just, uh, the next  two	0
where there will be - there, you know, may as well be meetings, but I just won't be at them.	0
O_K.	0
And then starting up on the  thirteenth,	0
uh, we'll have meetings again but we'll have to do without  Sunil  here somehow.  So.	0
When do you go back?	0
Thirty-first, August.	0
Yeah.	0
Yeah.	0
So.	0
Cool .	0
When is the evaluation? November, or something?	0
Yeah, it was supposed to be November fifteenth. Has anybody heard anything different?	0
I don't know. The meeting in - is the five and six of December.	0
p- s- It's like - Yeah, it's tentatively  all full . Yeah.	0
So -	0
Mm-hmm.	0
Uh, that's a proposed date, I guess.	0
Yeah, um -	0
so the evaluation should be  on	0
a week before or -	0
Yeah.	0
Yep.	0
But, no, this is good progress.	0
So.	0
Uh -	0
O_K.	0
Guess we're done.	0
Should we do digits?	0
Digits? Yep.	0
O_K.	0
It's a wrap.	0
