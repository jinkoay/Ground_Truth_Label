Uh, is it the twenty-fourth? Yeah.	0
Uh Chuck, is the mike type wireless -	0
Yeah. We uh we abandoned the lapel because they sort of were not too not too hot, not too cold, they were you know, they were	0
uh, far enough away that you got more background noise, uh, and uh and so forth but they weren't so close that they got quite the you know, the really good No, th-	0
they I mean they didn't -	0
Wait a minute. I'm saying that wrong.	0
They were not so far away that they were really good representative distant mikes,	0
but on the other hand they were not so close that they got rid of all the interference. So it was no didn't seem to be a good point to them.	0
On the other hand if you only had to have one mike in some ways you could argue the lapel was a good choice, precisely  because  it's in the middle.	0
There's uh, some kinds of junk that you get with these things that you  don't  get with the lapel uh, little mouth clicks and	0
breaths and so forth are worse with these than with the lapel, but	0
given the choice we there seemed to be very strong opinions for uh, getting rid of lapels.	0
The mike number is -	0
Uh, your mike number's written on the back of that unit there.	0
And then the channel number's usually one less than that.	0
It- it's one less than what's written on the back of your yeah.	0
So you should be  zero,  actually.	0
For your uh, channel number.	0
And you should do a lot of talking so we get a lot more of your pronunciations.	0
no, they don't don't have a have any Indian pronunciations.	0
So what we usually do is um, we typically will have our meetings and then at the end of the meetings we'll read the digits. Everybody goes around and reads the digits on the the bottom of their forms.	0
Yeah. We're This is session R_nineteen.	0
O_ K.  Do we have anything like an agenda? What's going on? Um.	0
Sunil's here for the summer?	0
Sunil's here for the summer, right.	1
Um, so, one thing is to talk about a kick off meeting	0
uh, and then just uh, I guess	1
uh, progress reports individually, and then uh, plans	1
for where we go between now and then, pretty much.	1
I could say a few words about um, some of the uh, compute stuff that's happening around here, so that people in the group know.	1
Why don't you  start  with that? That's sort of Yeah?	0
We um So we just put in an order for	1
about twelve new machines, uh, to use as sort of a compute farm.	1
uh, we ordered uh, SUN-Blade-one-hundreds, and um,	0
I'm not sure exactly how long it'll take for those to come in, but, uh, in addition, we're running -	0
So the plan for  using  these is, uh, we're running P_make and Customs here and Andreas has sort of gotten that all	1
uh, fixed up and up to speed.	1
And he's got a number of little utilities that make it very easy to um,	1
run things using P_make and Customs. You don't actually have to	1
write P_make scripts and things like that.	0
The  simplest  thing And I can send an email around or, maybe I should do an F_A_Q on the web site about it or something.	1
How about an email that  points  to the F_A_Q,  you know what I'm saying? so that you can   Yeah.	1
Uh, there's a command, uh, that you can use called " run  command". "Run dash command", "run hyphen command".	0
And, if you say that and then some job that you want to execute,	1
uh, it will find the fastest currently available machine,	1
and export your job to that machine,	1
and uh and run it there and it'll duplicate your environment. So	0
you can try this as a simple test with uh, the L_ S  command. So you can say "run dash command L_ S ",	0
and, um, it'll actually export that  L_S command to some machine in the institute, and um, do an L_S on your current directory.	0
So, substitute L_S for whatever command you want to run, and um And that's a simple way to get started using using this.	0
And, so,  soon,  when we get all the new machines up,	1
um, e- then we'll have lots more compute to use. Now th- one of the nice things is that uh, each machine that's part of the	1
P_make and Customs network has attributes associated with it. Uh, attributes like how much memory the machine has, what its speed is, what its operating system,	0
and when you use something like " run  command", you can specify those attributes for your program. For example if you only want your	0
thing to run under Linux, you can give it the Linux attribute, and then it will find the fastest available Linux machine and run it on that. So.	0
You can control  where  your jobs go, to a certain extent,	0
all the way down to an individual machine. Each machine has an attribute which is the name of  itself.	0
So you can give  that  as an attribute and it'll  only  run on  that.	0
If there's already a  job  running, on some machine that you're trying to select, your job will get queued up,	0
and then when that resource, that machine becomes available, your job will get exported there. So,	0
there's a lot of nice features to it and it kinda helps to balance the load of the machines and uh,	1
right now Andreas and I have been the main ones using it and we're Uh. The S_R_I recognizer has all this P_make customs stuff built into it. So.	0
So as I understand,  you know ,  he's  using all the machines and  you're  using all the machines,	0
is the rough division of -	0
Yeah, you know, I I sort of got started  using the recognizer just recently and uh,	0
uh I fired off a training job, and then I fired off a recognition job and I get this email about midnight from Andreas saying,	0
"uh, are you running two  trainings simultaneously s- my m- my jobs are not getting run."	0
So I had to back off a little bit. But,	0
soon as we get some more machines then uh -	0
then we'll have more compute available. So, um,	0
that's just a quick update about what we've got. So.	0
Um, I have I have a question about the uh,	0
So, um, let's say I have like, a thousand little little jobs to do?	0
Um, how do I do it with "run command"? I mean do -	0
You could write a script uh, which called run command on each sub-job right? But you probably	0
wanna be careful with that because um, you don't wanna saturate the network.	0
you know, you should you should probably not run more than, say ten	0
jobs yourself at any one time, uh, just because then it would	0
Oh, too much file transfer and stuff.	0
Well it's  not   that  so much as that, you know, e- with if  everybody  ran fifty jobs at once then it would just bring everything to a halt and, you know, people's jobs would get delayed, so it's sort of a sharing thing.	0
Um, so you should try to limit it to somet- sometim- some number around ten jobs at a time. Um. So if you had a script for example that had a thousand things it needed to run, um,	0
you'd somehow need to put some logic in there if you were gonna use "run command", uh, to  only  have ten of those going at a time.	0
And uh, then, when one of those finished you'd fire off another one.	0
I remember I I forget whether it was when the Rutgers or or Hopkins workshop, I remember  one  of the workshops I was at there were -	0
everybody was real excited cuz they got twenty-five machines and there was some kind of P_make like thing that sit- sent things out.	0
So all twenty-five people were sending things to all twenty-five machines  and  and things were a lot less efficient than if you'd just use your own machine.  as I recall, but.  Yeah.	0
Yeah, exactly. Yeah, you have to be a little bit careful.	0
but uh, you can also If you have  that  level of parallelization um, and you don't wanna have to worry about writing the logic in in a Perl script to take care of that, you can use um, P_make	0
and and you basically write a Make file that uh, you know your final job depends on these one thousand things, and when you run	0
P_make, uh, on your Make file, you can give it the dash capital J_	0
and and then a number, and that number represents how many uh, machines to use at once.	0
And then it'll make sure that it never goes above that.	0
I can get some documentation.	0
So it it's it's not systematically queued. I mean	0
all the jobs are  running.	0
If you launch twenty jobs, they are all  running.	0
It depends. If you "Run command", that I mentioned before, is doesn't know about other things that you might be running.	0
So, it would be  possible  to run a hundred run jobs at once,	0
and they wouldn't know about each other.	0
But if you use  P_make , then, it knows about all the jobs that it has to run	0
and  it  can control, uh, how many it runs simultaneously.	0
So "run command" doesn't use P_make, or ?	0
i- It's meant to be run one job at a time?	0
So you could fire off a  thousand  of those, and it doesn't know any  one  of those doesn't know about the other ones that are running.	0
So why would one use that rather than  P_make ?	0
Well, if you have, um Like, for example, uh if you didn't wanna write a P_make script and you just had a, uh an H_T_K training job that you know is gonna take uh, six hours to run,	0
and somebody's using, uh, the machine you typically use, you can say "run command" and your H_T_K thing and it'll find another machine, the fastest currently available machine and and run your job there.	0
Now, does it have the same sort of behavior as P_make, which is that, you know, if you run something on somebody's machine and they come in and hit a key then it -	0
Yes. Yeah, there are um Right. So some of the machines at the institute, um,	0
have this attribute called " no  evict".	0
And if you  specify  that,	0
in in one of your  attribute  lines, then it'll go to a machine which your job  won't  be evicted from.	0
But, the machines that  don't  have that  attribute,	0
if a job gets fired up on that,	0
which could be somebody's desktop machine, and -	0
and they were at lunch, they come  back  from lunch and they start typing on the console,	0
then your machine will get evicted your  job   will get evicted from their machine and be restarted on another machine. Automatically.	0
So which can cause you to lose time, right? If you	0
had a  two  hour job, and it got halfway through and then somebody came back to their machine and it got  evicted.  So.	0
If you  don't  want your job to  run  on a machine where it could be  evicted,  then you give it the minus the attribute, you know, "no evict",	0
and it'll pick a machine that it can't be evicted from. So.	0
Um, what what about I remember always  used  to be an issue, maybe it's not anymore, that	0
if you if something required if your machine required somebody hitting a key in order to evict things that are on it so you could work,	0
but if you were logged into it from home?	0
and you weren't hitting any keys? cuz you were, home?	0
Yeah, I I'm not sure how that works. Uh, it seems like Andreas did something for that. Um.	0
O_K. We can ask him sometime.	0
Yeah. I don't know whether it monitors the keyboard or actually looks at the console T_T_Y, so maybe if you	0
you know, dev dev console or something.  Hmm?	0
You probably wouldn't ordinarily, though.  Yeah.  Right?	0
You probably wouldn't  ordinarily.   I mean you sort of -	0
you're at home and you're trying to log in, and it takes forever to even log you in, and you probably go, "screw this", and  You know. Yeah.	0
Yeah, so, um, yeah. I I can I'm not sure about that one. But uh.	0
Uh, I need a little orientation about this environment and uh scr- s- how to run some jobs here because I never d- did anything so far	0
with this X_  emissions   So,	0
I think maybe I'll ask you after the meeting.	0
Yeah. Yeah, and and also uh,  Stephane's  a a really good resource for that if you can't	0
Yeah, yeah, yeah. Yep. O_K, sure  @@	0
Especially with regard to the  Aurora  stuff. He he knows that stuff better than I do.	0
O_K.  Well, why don't we uh,	0
Sunil since you're  haven't haven't  been   at  one of these yet, why don't yo- you tell us what's what's up with  you?  Wh- what you've been up to, hopefully.	1
uh, shall I start from -	0
Well I don't know how may I how -	0
O_K. Uh, I think I'll start from the post uh Aurora submission maybe.	0
Uh, yeah, after the submission the -	0
what I've been working on mainly was to take take other s- submissions and then	0
over  their  system, what they submitted,	0
because we didn't have any speech enhancement system in in ours. So -	0
First I tried just L_D_A. And then I found that uh,	0
I mean, if if I combine it with L_D_A, it gives  @@  improvement over  theirs.	0
Are y- are you saying L_D_A?	0
just just the L_D_A filters. I just plug in -	0
I just take the cepstral coefficients coming from their system and then plug in L_D_A on top of that.	0
But the L_D_A filter that I used was different from what we submitted in the proposal. What I did was	0
I took the L_D_A filter's design using  clean  speech,	0
uh, mainly because the speech is already cleaned up after the enhancement so, instead of using this,	0
uh, narrow narrow band L_D_A filter that we submitted uh,	0
I got  new  filters. So	0
that seems to be giving -	0
uh, improving over their uh, system.	0
Slightly.  But, not very significantly.	0
uh, showing any improvement over final by plugging in an L_D_A.	0
so then after after that I I added uh, on-line normalization  also  on top of that. And that -	0
there there also I n-	0
I found that I have to make some changes to their	0
time constant that I used	0
because th- it has a a mean and variance update time constant and -	0
suitable for the enhanced speech,	0
and whatever we try it on with proposal-one.	0
I didn't I didn't play with that time constant a lot, I just t- g-	0
I just found that I have to reduce the value I mean, I have to increase the time constant, or reduce the value of the update value.	0
That's all I found  So I have to .	0
uh, the other other thing what I tried was, I just um,	1
uh, took the baseline and then	1
ran it with the endpoint inf- uh th- information,	1
to see that how much the baseline itself improves	1
by just supplying the information of the I mean the w-	1
I found that the baseline itself improves by twenty-two percent by just giving the  wuh- .	1
Uh, can you back up a second, I I I missed something, uh,	0
I guess my mind wandered. Ad- ad- When you added the on-line normalization and so forth, uh, uh things got better again? or is it? Did it not?	0
No. No. No, things didn't get better with the same time constant that we used.	0
No, no. With a  different  time constant.	0
With the different time constant I found that -	0
I mean, I didn't get an improvement over	0
Oh. No you didn't, O_K.	0
because I I found that I would have change the value of the update factor.	0
But I didn't play it with	0
play play quite a bit to make it better than.	0
So, it's still not -	0
I mean, the on-line normalization didn't give me any improvement.	0
oh yeah  So I just stopped there with the uh, speech enhancement. The the other thing what I tried was the adding the	0
uh, endpoint information to the baseline and that itself gives like twenty-two percent because the -	1
the second the new phase is going to be with the endpointed speech. And just to get a feel of how much the baseline itself is going to change by adding this endpoint information, I just,	1
So people won't even have to  worry  about, uh, doing speech-nonspeech then.	1
Yeah that's, that's what the feeling is like.	1
They're going to give the endpoint information.	0
G- I guess the issue is that people do that anyway, everybody does that, and they wanted to see,	0
given that you're doing that, what what are the best features that you should use.	0
I mean  clearly  they're interact. So I don't know that I entirely agree with it. But but it might be uh In some ways it might be better t- to -	0
rather than giving the endpoints, to have a standard	0
that everybody uses and then interacts with. But, you know. It's it's still someth- reasonable.	0
So, are people supposed to assume that there is uh -	0
Are are people not supposed to use any speech  outside  of those endpoints?	0
No.   No.  That i- I -	0
use speech outside of it for estimating background noise and things?	0
I guess that is that is where the consensus is.	0
Like y- you will you will -	0
You'll be given the information about the beginning	0
and the end of speech	0
but the whole speech is available to you.	0
So it  should  make the spectral subtraction style things work even  better,	0
because you don't have the mistakes in it.	0
So that that The baseline itself I mean, it improves by twenty-two percent. I found that in s- one of the SpeechDat-Car cases,	0
that  like, the  Spanish  one improves by just fifty percent by just putting the endpoint.	0
I mean you don't  need  any further speech enhancement with fifty.	0
So the baseline itself improves by fifty percent.	0
Yeah. So it's g- it's gonna be harder to   beat  that actually. But but -	0
so that is when uh, the the qualification criteria was	0
reduced from fifty percent to something like	0
twenty-five percent for well-matched. And I think they have -	0
they have actually changed their qualification c- criteria now.	0
Yeah, I guess after that,	0
I just went home f- I just had a vacation fo- for four weeks.	0
O_K. No, that's that's that's a good good update.	0
Yeah, and I I came back and I started working on	0
uh, some other speech enhancement algorithm. I mean, so -	0
I from the submission what I found that people have tried spectral subtraction and Wiener filtering. These are the main	0
uh, approaches where people have tried, so just to -	0
just to fill the space with some f- few more speech enhancement algorithms to see whether it	0
improves  a lot , I I've been working on this uh, signal subspace approach for speech enhancement where you	0
take the noisy signal and  then  decomposing the signal s-	0
and the noise subspace and then try to estimate the clean speech from the signal plus noise subspace.	0
So, I've been actually running some s-	0
So far I've been trying it only on  Matlab.  I have to -	0
to to  test  whether it works first or not and then I'll	0
p- port it to C_ and I'll update it with the repository once I find  it- it  giving any some positive result.	0
So you said one thing I want to jump on for a second.	0
you're you're getting tuned into the repository thing that he has here and -	0
so we- we'll have a  single place where the stuff is.	0
so maybe uh, just briefly, you could	1
remind us about the related experiments. Cuz you did some stuff that you talked about last week, I guess?	1
Um, where you were also combining something  both  of you I guess were both combining something from the uh,	1
French Telecom system with  the u- uh I I don't know whether it was system one or system two, or ?	1
Mm-hmm. It was system one. So we -	0
The main thing that we did is just to take the spectral subtraction from the France Telecom, which provide us  some  speech samples that are uh, with noise removed.	1
So I let me let me just stop you there. So then, one distinction is that	0
uh, you were taking the actual France Telecom features and then applying something to -	0
Uh, no there is a slight different. Uh	0
I mean, which are extracted at the handset	0
because they had another back-end blind equalization -	0
But that's what I mean.	0
But u- u- Sorry, yeah, I'm not being I'm not being clear.	0
What I meant was you had something like cepstra or something, right?	0
And so one difference is that, I guess you were taking  spectra.	0
Yeah. But I guess it's the s- exactly the same thing because	0
on the heads- uh, handset they just applied this Wiener filter and then compute cepstral features, right? or ?	0
Yeah, the cepstral f- The difference is like There may be a slight difference in the way because they use exactly the baseline system for converting the cepstrum once you have the speech.	0
I mean, if we are using our own code for th-	0
I mean that that could be the only difference. I mean, there is no other difference. Yeah.	0
But you got some sort of different  result.  So I'm trying to  understand  it. But uh,	0
Yeah, well I think we should uh, have a table with all the result because I don't know I uh, I don't exactly know what are your results? But,	0
Yeah, but so we did  this,  and another difference I guess is that we just applied	0
uh, proposal-one system after this without -	0
well, with our modification to reduce the delay of the the L_D_A filters, and	0
Well there are slight modifications, but it was the full proposal-one. In your case, if you tried	0
just putting L_D_A,  then  maybe on-line normalization ?	0
Af- I after that I added on-line normalization, yeah.	0
Mm-hmm. So we just tried directly to to	1
just, keep the system as it  was   and,	1
um, when we plug the spectral subtraction it improves uh, signif- significantly.	1
Um, but, what seems clear also is that we have to retune the time constants of the on-line normalization. Because if we keep the value that was submitted	0
uh, it doesn't help at all. You can remove on-line normalization, or put it, it doesn't change  anything.	1
Uh, uh, as long as you have the spectral subtraction.	1
But, you can still find	0
some kind of optimum somewhere, and we don't know where exactly but,	0
So it sounds like you should look at some tables of results or something and see where i-	0
where they were different and what we can learn from it.	0
with changes, because we change it the system to have -	0
Oh yeah, I mean the the new L_D_A filters. I mean O_K.	0
There are other things that we finally were shown to improve also like,	0
Uh, it doesn't seem to hurt on T_I-digits, finally.	0
Maybe because of other changes.	0
Um, well there are some  minor changes, yeah.	0
And, right now if we look at the results, it's,	1
um, always better than -	1
it seems always better than France Telecom for mismatch and high-mismatch.	1
And it's still slightly worse for well-matched.	1
but this is not significant.	1
the problem is that it's not significant, but if you	0
put this in the, mmm,	0
uh, spreadsheet, it's still worse.	0
Even with very minor -	0
even if it's only slightly worse for well-matched.	0
And significantly better for H_M.	0
I don't think it's importa- important because when they will change their metric,	0
uh, mainly because of uh, when you p- you plug the um,	0
frame dropping in the baseline system,	0
it will improve a lot H_M, and M_M,  so,	0
um, I guess what will happen -	0
I don't know what will happen. But, the different contribution, I think, for the different test set will be more even.	0
Because the your improvement on H_M and M_M will also go down significantly in the spreadsheet so. But the  the well-matched may still I mean the well-matched may be the one which is least affected by adding the	0
Yeah. So the the M_M -	0
M_M and H_M are going to be v- hugely affected by it. Yeah.	0
But they d- the  everything  I mean is like, but there	0
that's how they reduce why they reduce the qualification to twenty-five percent or some something on.	0
But are they changing the weighting?	0
Uh, no, I guess they are going ahead with the same weighting.	0
So there's nothing on -	0
I don't understand that. I guess I I haven't been part of the	0
it seems to me that the well-matched condition is gonna be unusual,	0
Because, um, you don't actually have	0
good matches ordinarily for what any  @@  particular person's car	0
It seems like something like the  middle  one is is more	0
So I don't know why the  well-matched is	0
Yeah, but actually the well well	0
I mean the the well-matched condition is not like, uh, the one in T_I-digits where	0
uh, you have all the training, uh, conditions exactly like replicated in the	0
testing condition also. It's like, this is not calibrated by S_N_R or something. The well-matched has also some some mismatch in that which is other than the -	0
The well wa- matched has mismatch?	0
has has  also  some slight mismatches, unlike the T_I-digits where it's like perfectly matched because it's artificially added noise.	0
But this is natural recording.	0
So  remind  me of what well-matched meant? You've  told  me many times.	0
The the well-matched is like -	0
the the well-matched is defined like it's seventy percent of the whole database is used for training and thirty percent for testing.	0
Yeah. Well, so it means that if the database is large enough, it's matched. Because	0
in each set you have a range of conditions Well -	0
Right. So, I mean, yeah, unless they deliberately chose it to be different, which they  didn't  because they want it to be well-matched, it  is  pretty much You know, so it's so it's sort of saying if you -	0
It's it's not  guaranteed  though.	0
Uh, it's not  guaranteed.  Right. Right.	0
Yeah because the m- the main major reason for the m-	0
the main mismatch is coming from the amount of noise and the silence frames and all those present in the database actually.	0
Again, if you have  enough  if you have enough -	0
So it's sort of i- i- it's sort of saying O_K, so you much as you train your dictation machine for talking into your computer,	0
um, you you have a car, and so you drive it around a bunch and and record noise conditions, or something, and then I don't think that's very realistic, I mean I th-  I I you know, so I -	0
I I you know, I  guess  they're saying that if you were a company that was selling the stuff commercially,	0
that you would have a bunch of people driving around in a bunch of cars, and and you would have something that was roughly similar and maybe that's the argument, but I'm not sure I buy it, so.	0
What else is going on?	0
You- Yeah. We are playing we are also playing, trying to put other	1
it would be a very simple spectral subtraction, on the um, mel energies	1
which I already tested but without the um	0
frame  dropping  actually, and I think it's important to have frame dropping	0
Is it is spectral subtraction typically done on the after the mel, uh, scaling or is it done on the F_F_T bins?	0
if you use spectral subtraction.	0
Does it matter, or ?	0
I don't know. Well, it's both both uh, cases can i-	0
So- some of the proposal, uh, we're doing this on the bin on the F_F_T bins, others on the um, mel energies.  You can do both, but I cannot tell you what's -	0
which one might be better or -	0
I guess if you want to reconstruct the speech, it may be a good idea to do it on F_F_T bins.	0
But for speech recognition, it may not.	0
I mean it may not be very different if you do it on mel	0
warped or whether you do it on F_F_T.	0
So you're going to do a linear weighting anyway after that.	0
So, it may not be really a big different.	0
Well, it gives something different, but I don't know what are the, pros and cons of	0
The other thing is like when you're putting in a speech enhancement technique, uh,	0
is it like one stage speech enhancement? Because everybody seems to have a mod- two stages of speech enhancement in all the proposals, which is really giving them some improvement.	0
I mean they just do the same thing again once more.	0
And So, there's something that is good about doing it -	0
I mean, to cleaning it up once more.	0
Yeah, it might be. Yeah.	0
Yeah, so we can -	0
So maybe in my implementation I should also	0
try to inspire me from	0
this kind of thing and Yeah.	0
Well, the  other  thing would be to combine what you're  doing.  I mean maybe one or one or the other of the things that you're doing would benefit from the other happening  first.	0
Right, so he's doing a signal subspace thing, maybe it would work better if you'd already done some simple spectral subtraction, or maybe vi- maybe the other way around, you know?	0
So  I 've been thinking about combining the Wiener filtering with signal subspace,	0
I mean just to see all some some such permutation combination to see whether it really  helps  or not.	0
How is it I I guess I'm ignorant about this, how does -	0
I mean, since Wiener filter also assumes that you're that you're adding together the two signals,	0
how is how is that differ from signal subspace?	0
The signal subspace approach  has  actually an in-built Wiener filtering in it.	0
Yeah. It is like a K_L transform followed by a Wiener filter.	0
Is the signal is is a signal  substrate .	0
Oh, oh, O_K so the difference is the K_L.	0
So, the the different the c- the the advantage of combining two things is mainly coming from the signal subspace approach doesn't	0
work very well if the S_N_R is very bad.	0
it works very poorly with the poor S_N_R conditions, and in colored noise.	0
So essentially you could do simple spectral subtraction, followed by a K_L transform, followed by a	0
It's a it's a cascade of two s-	0
Yeah, in general, you don't that's right you don't wanna othorg- orthogonalize if the things are noisy.	0
Um, that was something that uh, Herve and I were talking about with um, the multi-band stuff, that if you're converting things to	0
from uh, bands, groups of bands into cepstral coef- you know, local sort of local cepstral coefficients that it's not that great to do it if it's noisy.	0
So that that's one reason maybe we could combine	0
s- some something to improve S_N_R a little bit,	0
first stage, and then do a something in the second stage which could take it	0
What was your point about about colored noise there?	0
Oh, the colored noise uh -	0
the colored noise the the v- the signal subspace approach has -	0
I mean, it it actually depends on inverting	0
the matrices. So it it ac-	0
the covariance matrix of the noise.	0
So if if it is not	0
positive definite, I mean it has a it's -	0
It doesn't behave very well if it is not positive definite  ak-	0
It works very well with white noise because we know	0
for sure that it has a positive definite.	0
So you should do spectral subtraction and then add noise.	0
So the way they get around is like they do an inverse filtering, first of the colo- colored noise and then make the noise white, and then finally when you reconstruct the speech back, you do this filtering again.	0
I was only half kidding. I mean if you sort of  you do the s- spectral subtraction,	0
that also gets rid and then you then then add a little bit l- noise noise addition -	0
I mean, that sort of what J_ J_RASTA does, in a way.	0
If you look at what J_RASTA doing essentially i- i- it's equivalent to sort of adding a little adding a little noise,	0
in order to get rid of the effects of noise.	0
Uh, yeah. So there is  this.  And	0
maybe we well we find some people so that	1
uh, agree to maybe work with us,	1
and they have implementation of V_T_S techniques	1
Vector Taylor Series that are used to mmm,	1
uh f- to model the transformation between clean	1
So. Well, if you take the standard model of channel plus noise,	0
uh, it's it's a nonlinear	0
eh- uh, transformation in the cepstral domain.	0
uh, there is a way to approximate this using	0
uh, first-order or second-order Taylor Series and	0
it can be used for	1
uh, getting rid of the noise and the channel effect.	1
Uh w- working in the cepstral domain? So there is one guy	0
and another in  uh, Lucent that I met at I_CASSP.	0
Who's the guy in Grenada?	0
I don't know  him .	0
This V_T_S has been proposed by C_M_U? Is it is it the C_M_U? Yeah, yeah, O_K. From C_.	0
Yeah, yeah, yeah. Originally the idea was from C_M_U.	0
Well, it's again a different thing   that could be tried.	0
Yeah, so at any rate, you're looking	0
uh, standing back from it,	0
looking at ways to combine	0
one form or another of uh, noise removal,	0
uh, with with these other things we have,	0
uh, looks like a worthy thing to -	0
But, yeah. But for sure there's required to that requires to	0
re-check everything else, and re-optimize	0
for sure the on-line normalization may be the L_D_A filter.	0
Well one of the seems like one of the things to go through next week when Hari's here, cuz Hari'll have his own ideas	0
too or  I guess not next week, week and a half,	0
uh, will be sort of go through these alternatives, what we've seen so far, and come up with some game plans. Um. You know. So, I mean one way would -	0
he- Here are some alternate visions. I mean one would be,	0
you look at a few things very quickly, you pick on something that looks like it's promising and then everybody works really hard on the same different aspects of the same thing.	0
Another thing would be to have t- to to pick two pol-  two  plausible things, and and you know, have t- sort of two working things for a while until we figure out what's better, and then, you know, uh,	0
uh, he'll have some ideas on that too.	0
The other thing is to, uh Most of the speech enhancement techniques have reported results on small vocabulary tasks.	0
But we we going to address this Wall Street Journal in our next stage,	0
which is also going to be a noisy task so s- very few people have reported something on	0
using some continuous speech at all. So, there are some I mean, I was looking at some literature on speech enhancement	0
applied to large vocabulary tasks and	0
spectral subtraction  doesn't  seems to be the thing to do for	0
large vocabulary tasks. And it's -	0
Always people have shown improvement with Wiener filtering and maybe subspace approach over spectral subtraction everywhere.	0
But if we if we have to use simple spectral subtraction, we may have to do some optimization  to make it	0
So they're making there Somebody's generating Wall Street Journal with additive artificially added noise or something?	0
Sort of a sort of like what they did with T_I-digits, and? Yeah, O_K.	0
Yeah. I m- I guess Guenter Hirsch is in charge of that.	0
Guenter Hirsch and T_I. Maybe Roger r- Roger, maybe in charge of.	0
And then they're they're uh, uh, generating H_T_K scripts to -	0
Yeah, I don't know. There are they have there is no I don't know if they are converging on H_T_K or are using some	0
Mississippi State, yeah. I'm not sure about that.	0
Mis- Mississippi State maybe, yeah.	0
Yeah, so that'll be a little little task in itself.  Um, well we've -	0
Yeah, it's true for the additive	0
noise, y- artificially added noise we've always used small vocabulary  too.	0
But for n- there's been noisy speech this larv- large vocabulary that we've worked with in Broadcast News. So we- we did the Broadcast News evaluation and	0
some of the focus conditions were noisy and and But we but we didn't do spectral  subtraction.  We were doing our funny stuff, right? We were doing multi- multi- uh, multi-stream and and so forth.	0
But it, you know, we di- stuff we did  helped.  I mean it,	0
now we have this um,  meeting  data.	0
You know, like the stuff we're  recording right  now,  and -	0
that  we have uh, for the -	0
uh, the quote-unquote  noisy  data there is just -	0
noisy and  reverberant  actually. It's the far field mike.	0
the  digits  that we do at the end of these things. And that's what most o- again, most of our work has been done with that, with with uh, connected digits.	0
but uh, we have recognition now	0
with some of the continuous speech,	0
large vocabulary continuous speech, using Switchboard uh, Switchboard recognizer,	0
uh, no training,  from this, just just plain using the Switchboard.	0
Oh. You just take the Switchboard trained ? Yeah, yeah.	0
That's that's what we're doing, yeah. Now there are some adaptation though, that that uh, Andreas has been playing with, but we're hop- uh, actually uh, Dave and I were just talking earlier today about maybe at some point not that distant future, trying some of the techniques	0
O_K. Yeah. That's cool. O_K.	0
that we've talked about on, uh, some of the large vocabulary data. Um,	0
I mean, I guess no one had done  yet  done test one on	0
using uh, the S_R_I recognizer and, uh,	0
I don't not that I know of.	0
You'll  see a little smoke coming up from the the C_P_U or something  trying to trying to do it, but	0
uh, yeah. But, you're right that that that's a real good point, that uh, we we don't know	0
yeah, uh, I mean, what if any of these ta- I guess that's why they're pushing that in the uh in the evaluation.	0
Anything else going on? at you guys' end, or ?	1
I don't have good result, with the inc- including the new parameters, I don't have good result.	1
Are  similar or a little bit worse.	0
With what what other new p- new parameter?	1
You're talking about your voicing?	0
Yeah. So maybe You probably need to back up a bit  seeing as how Sunil, yeah .	1
I tried to include another new parameter to the	1
traditional parameter, the coe- the cepstrum coefficient, that, like, the	1
and another estimation of the	1
var- the variance of the difference for of the spec- si- uh, spectrum of the signal and -	1
and the spectrum of time after	1
I'm  so  sorry. I didn't get it.	0
Nuh. Well. Anyway. The First you have the sp-	0
the spectrum of the signal, and you have the -	0
on the other side you have the output of the mel filter bank.	0
You can extend the coefficient of the mel filter bank and obtain an approximation of the spectrum of the signal.	0
I do the difference -	0
I found a difference at the variance of this different because, suppose	0
if the variance is high, maybe you have n- uh, noise.	0
And if the variance is small,	0
maybe you have uh, speech.	0
To The idea is to found another feature for discriminate between	1
voice sound and unvoice sound.	1
And we try to use this new feature feature. And I did experiment -	1
I need to change to obtain this new feature I need to change the size the window size size.	0
of the a- of the analysis window size,	0
Uh, sixty-two point five milliseconds I think.	0
I did two type of experiment to include this feature directly	1
with the other feature and to train a neural network	1
to select it voice-unvoice-silence silence and to to concat this new feature. But the result are	0
n- with the neural network I have more or less the same result.	1
As using just the cepstrum, or ?	1
It's neve- e- e- sometime it's worse, sometime it's a little bit better, but not significantly.	1
Uh, is it with T_I-digits, or with ?	0
No, I work with eh, Italian and Spanish basically.	0
And if I don't y- use the neural network, and use directly	0
I I I really wonder though. I mean we've had these discussions before, and and one of the things that struck me was that uh, about this line of thought that was	1
particularly interesting to me was that	1
uh, in an irreversible way,	1
um, you throw away some information.	1
And, that's mostly viewed on as a good thing, in the way  we  use it, because we wanna suppress things that will cause variability for uh particular, uh, phonetic units.	0
but, you'll do throw something away.	0
And so the question is, uh, can we figure out if there's  something  we've thrown away that we  shouldn't  have.	1
when they were looking at the  difference  between the filter bank and the F_F_T that was going  into  the filter bank, I was thinking "oh, O_K, so they're picking on something	1
they're looking on it to figure out  noise,  or  voice   voiced  property whatever." So that that's interesting. Maybe that helps to drive the -	1
the thought process of coming up with the features. But for  me  sort of the interesting thing was, "well,	1
but is there just something in that  difference  which is  useful? " So  another  way of doing it, maybe, would be just to take the F_F_T	1
uh, power spectrum, and feed it into a  neural  network,	1
and then use it, you know, in combination, or alone, or or whatever	0
Voiced, unvoiced is like -	0
No the just the same same way we're using I mean, the same way that we're using the  filter  bank.	0
Exact way the same way we're using the  filter  bank. I mean, the filter bank is good for all the reasons that we say it's  good.  But it's  different.	0
And, you know, maybe if it's used in combination, it will get at something that we're  missing.	1
And maybe, you know, using, orth- you know, K_L_T, or uh, um,	0
adding probabilities, I mean, all th- all the different ways that we've been  playing  with,	0
that we would let the essentially let the neural network determine what is it	0
that's useful, that we're missing here.	0
Yeah, but there is so much variability in the power  spectrum.	0
Well, that's probably why y- i- it would be  unlikely  to work as well by itself, but it  might  help in  combination.	0
But I I I have to tell you, I can't remember the conference, but, uh, I think it's about	0
ten years ago, I remember going to	0
one of the speech conferences and and uh,	0
I saw within very short distance of one another a couple different posters that showed about the wonders of some auditory inspired front-end or something, and a couple posters away it was somebody	0
who compared one to uh, just putting in the F_F_T and the F_F_T did slightly better.	0
So I mean the i- i- It's true there's lots of variability, but again we have these wonderful statistical mechanisms for	0
quantifying  that a- that variability, and you know,  doing  something  reasonable  with it. So, um,	0
It- it's same, you know, argument	0
that's gone both ways about uh, you know, we have these data driven filters, in L_D_A,	0
and on the other hand, if it's  data  driven it means it's driven by things that have lots of variability, and that are necessarily not necessarily gonna be the same in training and test,	0
so, in some ways it's  good  to have data driven things, and in some ways it's  bad  to have data driven things. So,	0
part of what we're discovering, is ways to combine things that are data driven than are not.	0
Uh, so anyway, it's just a thought, that that if we if we had that maybe it's just a  baseline	0
uh, which would show us "well, what are we really getting out of the filters", or maybe i- i- probably not by  itself,  but in  combination,	0
uh, you know, maybe there's something to be  gained  from it, and let the -	0
But, you know, y- you've only worked with us for a short time, maybe in a year or two you w- you will actually come up with the right set of things	0
to  extract  from this information.	0
But, maybe the neural net and the H_M_Ms could figure it out  quicker  than you.	0
So.  It's just a thought.	1
Maybe.  Yeah, I can I will try to do that.	1
What one one um p-	0
one thing is like what before we started using this V_A_D	0
the th- what we did was like,	0
I I guess most of you know about this, adding this additional speech-silence bit to the cepstrum and training the H_M_M on that.	0
That is just a binary feature and that seems to be  improving a lot on the SpeechDat-Car where there is a lot of	0
noise but not much on the T_I-digits. So, a- adding an additional feature to distin- to discriminate between speech and nonspeech was helping.	0
Yeah, we actually added an additional binary feature to the cepstrum,  just  the baseline.	0
Yeah, yeah. Well, in in the case of T_I-digits it didn't actually give us anything,	0
because there wasn't any f- anything to discriminate between speech, and  it  was very short.	0
very it was a  huge  improvement on Italian.	0
But  anyway  the question is even  more,  is within speech, can we get some features?	0
Are we drop- dropping information that can might be useful within  speech,  I mean. To maybe to distinguish between voice sound and unvoiced sounds?	0
And it's particularly more relevant now since we're gonna be given the endpoints.	0
There was a paper in I_CASSP this I_CASSP over the uh extracting some	0
information from the cepstral coefficients and	0
I forgot the name. Some is- some harmonics  I don't know, I can I can pull that paper out from I_CASSP. It Huh?	0
Talking cumulants or something? Cumulants or something. But No.	0
Uh, I don't know. I don't remember. It wa- it was taking the, um -	0
It was about finding the higher-order	0
Yeah. And I'm not sure about whether it is the higher-order moments, or -	0
maybe  higher-order cumulants and -	0
It was it was Yeah. I mean, he was showing up uh some something on noisy speech, some improvement on the noisy speech.	0
So it  was on P_L_P derived  cepstral coefficients.	0
Yeah, but again You  could  argue that th- that's exactly what the neural network does.	0
So n- neural network uh,	0
is in some sense equivalent to computing, you know, higher-order moments of what you yeah.	0
I mean, it doesn't do it very specifically, and	0
Uh, anything on your end you want to talk about? Uh.	1
Um, nothing I wanna really talk about. I can I can just uh, um, share a little bit Sunil hasn't hasn't heard about uh, what I've been doing. Um, so,	1
um, I told you I was I was I was getting prepared to take this qualifier exam. So basically that's just, um, trying to propose um, uh,	1
your next your your following years of of your P_H_D work, trying trying to find a project to to define and and to work on.	1
So, I've been, uh, looking into, um, doing something about r- uh, speech recognition using acoustic events. So,	1
um, the idea is you have all these these different events, for example voicing, nasality, R_coloring, you know burst or noise, uh, frication, that kinda stuff, um, building robust	1
um, primary detectors for these acoustic events, and using the outputs of these robust detectors to do speech recognition.	1
Um, and, um, these these primary detectors, um, will be, uh,	0
inspired by, you know, multi-band techniques, um, doing things, um, similar to Larry Saul's work on, uh, graphical models	0
to to detect these these, uh, acoustic events. And, um, so I I been I been thinking about that and some of the issues that I've been running into are, um, exactly what what kind of acoustic events I need, what -	0
um, what acoustic events will provide a a good enough coverage to in order to do the later recognition steps. And, also, um, once I decide a set of acoustic events, um, h- how do I -	0
how do I get labels? Training data for for these acoustic events. And, then later on down the line, I can start playing with the the models themselves, the the primary detectors. Um, so,	0
um, I kinda see like, after after building the primary detectors I see um, myself taking the outputs	0
and feeding them in, sorta tandem style into into a um, Gaussian mixtures H_M_M back-end, um, and doing recognition. Um.  So, that's -	0
that's just generally what I've been looking at. Um,	0
By by the way, uh, the voiced-unvoiced version of that for instance could tie right in to what Carmen was looking at. So,	0
you know, um, if you if a multi-band approach was helpful as as I think it is, it seems to be helpful for determining voiced-unvoiced, that one might be another	0
Um, were were you gonna say something? Oh. It looked O_K, never mind. Um, yeah. And so, this this past week um, I've been uh, looking a little bit into uh, TRAPS	0
and doing doing TRAPS on on these e- events too, just, um, seeing seeing if that's possible. Uh, and	0
um, other than that, uh, I was kicked out of I_house for living there for four years.	0
Oh no. So you live in a cardboard box in the street now or, no?	0
Uh, well, s- s- som- something like that. In Albany, yeah.  Yeah.	0
And uh. Yep. That's it.	0
did you find a place? Is that out of the way?	0
Uh, yesterday I called up a lady who ha- who	0
will have a vacant room from May thirtieth and she said she's interviewing two more people.	0
So. And she would get back to me on Monday.	0
So that's that's only thing I have and Diane has a few more	0
houses. She's going to take some pictures and send me after I go back.	0
Oh. So you're not down here permanently yet?	0
No. I'm going back to O_G_I today.	0
O_K. And then, you're coming back uh -	0
Uh, i- I mean, I I p- I plan to be here on thirty-first.	0
Yeah, well if there's a house available or place to Yeah, I hope.	0
Well, I mean i- i- if if  They're  available, and they'll be able to get you something, so worst comes to worst we'll put you up in a hotel for for for a while until you -	0
Yeah. So, in that case, I'm going to be here on thirty-first  definitely .	0
You know, if you're in a desperate situation and you need a place to stay, you could stay with me for a while. I've got a spare bedroom right now.	0
That sure is nice of you.	0
So, it may be he needs more than me.	0
Oh no, no.  My my cardboard box is actually a nice spacious two bedroom apartment.	0
So a two  bedroom  cardboard box.	0
Th- that's great. Thanks Dave.	0
say anything about You you actually been -	0
Uh, last week you were doing this stuff with Pierre, you were you were mentioning. Is that that something worth talking about, or ?	1
Well, um, it I don't think it  directly  relates. Um, well, so, I was helping a speech researcher named Pierre Divenyi and he's int- He wanted to um, look at um,	1
how people respond to formant changes, I  think.  Um. So he he created a lot of synthetic audio files of vowel-to-vowel transitions, and then he wanted a psycho-acoustic um, spectrum.	1
And he wanted to look at um,	1
how the energy is moving  over time in that spectrum and compare that to the to the listener tests. And, um. So, I	1
gave him a P_L_P spectrum. And to um he he t- wanted to track the peaks so he could look at how they're moving. So I took the um, P_L_P L_P_C coefficients and um,	0
I found the roots. This was something that Stephane suggested. I found the roots of the um, L_P_C polynomial to, um, track the peaks in the, um, P_L_P L_P_C spectra.	0
well  there is  aligned  spectral pairs, is like the the Is that the aligned s-	0
It's a r- root L_P_C, uh, of some sort.	0
Oh, no. So you just -	0
instead of the log you took the root square,	0
I mean cubic root or something.	0
What di- w- I didn't get that.	0
No, no. It's it's it's taking the finding the roots of the L_P_C polynomial.	0
Polynomial. Yeah. Is that the line spectral -	0
So it's  like  line spectral pairs. Except I think what they call line spectral pairs they push it towards the unit circle, don't they, to sort of?	0
Oh, it's like line sp-	0
But it But uh, you know. But what we'd used to do w- when I did synthesis at National Semiconductor twenty years ago, the	0
technique we were playing with initially was was taking the L_P_C polynomial and and uh,	0
finding the roots. It wasn't P_L_P cuz Hynek hadn't invented it yet, but it was just L_P_C, and	0
uh, we found the roots of the polynomial, And th- When you do that, sometimes	0
they're f- they're what most people call formants, sometimes they're not.	0
So it's it's it's a little, uh Formant tracking with it can be a little tricky cuz you get these funny  values in in real speech, but.	0
So you just You typically just get a few roots? You know, two or three, something like that?	0
Well you get these complex pairs. And it depends on the  order  that you're doing, but.	0
Every root that's Since it's a real signal, the L_P_C polynomial's gonna have real coefficients. So I think that means that every root that is not	0
is gonna be a c- complex pair,	0
um, of a complex value and its conjugate.	0
So for each And if you look at that on the unit circle, um, one of these one of the members of the pair will be a  positive  frequency, one will be a  negative  frequency, I think. So I just  So, um,	0
f- for the I'm using an eighth-order  polynomial   and I'll get three or four of these pairs	0
which give me s- which gives me three or four peak positions.	0
This is from synthetic speech, or ?	0
Yeah. So if it's from synthetic speech then maybe it'll be cleaner. I mean for real speech in real then what you end up having is, like I say, funny little things that are don't exactly fit your notion of formants	0
all that well. But but  mostly  they are.  Mostly  they do. And and what I mean in in what we were doing,	0
which was  not  so much  looking  at things, it was O_K because it was just a question of quantization.	0
Uh, we were just you know, storing It was We were doing, uh, stored speech, uh, quantization. But but uh, in your case	0
Actually you have peaks that are not at the formant's positions, but	0
But there's some of that, yes.	0
they are lower in energy and Well they are	0
If this is synthetic speech can't you just get	0
the formants directly? I mean h- how is the speech created?	0
It was created from a synthesizer,  and um -	0
Wasn't a formant synthesizer was it?	0
I bet it it might have may have been but maybe he didn't have control over it or something?	0
I d- d- this -	0
In in fact w- we we could get, um, formant frequencies out of the synthesizer,	0
as well. And, um,  w- one thing that the, um, L_P_C approach will hopefully give me	0
in  addition,  um, is that I I might be able to find the b- the bandwidths	0
of these humps as  well.  Um, Stephane suggested looking at each complex pair as a like a se- second-order I_I_R filter. Um, but I don't think	0
there's a g- a really good reason not to um, get the formant frequencies from the synthesizer  instead.  Except that you don't have the psycho-acoustic modeling in that.	0
Yeah, so the actual So you're not getting the actual formants per se. You're getting the Again, you're getting sort of the, uh -	0
You're getting something that is is uh, af- strongly affected by the P_L_P model. And so it's more psycho-acoustic. So it's a little It's It's It's sort of sort of a different thing.	0
That's sort of the  point.	0
But Yeah. i- Ordinarily, in a formant synthesizer, the bandwidths as well as the ban- uh, formant centers are I mean, that's  Somewhere  in the synthesizer that was put  in,  as -	0
as what you But but yeah, you view each complex pair as essentially a second-order section, which has, uh, band center and band width, and um,	0
O_ K.  So, uh, yeah, you're going back today and then back in a week I guess, and.	0
I guess we should do digits quickly.	0
Oh yeah, digits. I almost  forgot  that. I almost forgot our daily digits.	0
