Somebody else should run this.	0
I'm sick of being the one to sort of go through and say, "Well, what do you think about this?"	0
Should we take turns? You want me to run it today?	0
Yeah. Why don't you run it today? O_K.	0
Let's see, maybe we should just get a list of items -	1
things that we should talk about.	0
I guess there's the usual  updates, everybody going around and saying,	1
uh, you know, what they're working on, the things that happened the last week.	1
But aside from that is there anything in particular that anybody wants to bring up	0
No? O_K. So why don't we just around and people can give updates. Uh, do you want to start, Stephane?	1
Alright. Um. Well, the first thing maybe is that the p- [MASK] is, uh, accepted.	1
This is - what - what do you, uh - what's in the paper there?	0
So it's the paper that describe basically the, um,	1
system that were proposed for [MASK]	1
The one that we s- we submitted the last round?	0
Um - Yeah. So and the, fff  comments seems - from the reviewer are good. So.	1
Where - where's it gonna be this year?	0
It's, uh, Aalborg in Denmark. And it's, yeah,	0
Yeah. Then, uh, whhh  well, I've been working on - on t- mainly on [MASK] this week.	1
Uh, I've been trying different - slightly - slightly different approaches.	1
Um, the first thing is trying to play a little bit again with the, um, time constant.	0
Uh, second thing is, uh, the training of, uh, [MASK] with two different means,	0
one mean for the silence and one for the speech.	0
and so I have two recursions which are controlled by the, um, probability of the voice activity detector.	0
This actually don't s- doesn't seem to help,	1
although it doesn't hurt. So.	1
But - well, both  [MASK] approach seems equivalent.	1
Are the means pretty different  for the two?	0
Yeah. They can be very different. Yeah. Mm-hmm.	0
So do you maybe make errors in different places? Different kinds of errors?	1
I didn't look, uh, more closely. Um. It might be, yeah. Mm-hmm.	1
Well, eh, there is one thing that we can observe, is that the mean are more different for -	0
for [MASK] and [MASK] than for the other coefficients.	0
And - Yeah, it - the C_one is -	0
There are strange - strange thing happening with [MASK] is that	0
when you have different kind of noises, the mean for the - the silence portion is - can be	0
So when you look at the trajectory of C_one, it's - has a strange shape and	0
I was expecting th- the s- that these two mean helps, especially because of the - the strange	0
uh, which can - like, yo- you can have,	0
a trajectory for the speech and then when you are in the silence it goes somewhere, but if the noise is different it goes somewhere else.	0
So which would mean that if we estimate the mean based on all the signal, even though we have	0
frame dropping, but we don't frame ev- uh, drop  everything,  but -	0
uh, this can - hurts the estimation of the mean for speech, and -	0
Mmm.   But I still have to investigate further, I think.	0
Um, a third thing is, um,	0
t- having a fixed time constant,	0
I try to have a time constant that's smaller at the beginning of the utterances	0
to adapt more quickly to the r- something that's closer to the right mean.	0
Yeah. And then this time constant increases and I have a threshold that -	0
well, if it's higher than a certain threshold, I keep it to this threshold to still,	0
uh, adapt, um, the mean when -	0
if the utterance is, uh, long enough to - to continue to adapt after, like, one second or -	0
Uh, well, this doesn't help neither,  but this doesn't hurt. So, well.	0
Wasn't there some experiment you were gonna try where you did something differently for each,	0
um,  uh - I don't know whether it was each	0
mel band or each, uh, um, [MASK] bin or someth- There was something you were gonna -	0
some parameter you were gonna vary depending on the frequency.	0
I don't know if that was -	0
I guess it was -	0
I don't know. No. u- Maybe it's this - this idea of having different  [MASK]	0
um, tunings for the different [MASK]	0
Yeah. I - I thought, Morgan, you brought it up a couple meetings ago. And then it was something about,	0
uh, some- and then somebody said "yeah, it does seem like, you know, [MASK] is	0
the one that's, you know, the major one" or, uh, s- I can't remember exactly what it was now.	0
Yeah. There - uh, actually, yeah.	0
S- um, it's very important to [MASK] and  much less to normalize the other coefficients. And, um,	0
actu- uh, well, at least with the current von-line normalization scheme. --/Jargon-- And	0
we - I think, we	0
kind of know that normalizing [MASK] doesn't help with the current scheme. And -	0
In my idea, I -	0
I was thinking that the - the - the reason is maybe because of these funny things that happen between speech and silence which have different means.	0
But maybe it's not so -  so easy to -	0
I- I really would like to suggest looking, um,	1
a little bit at the kinds of errors. I know you can get lost in that and go forever and not see too much, but -	1
sometimes, but - but, um,	1
just seeing that each of these things didn't make things better may not	1
be enough. It may be that they're making them better in some ways and worse in others, or increasing insertions and decreasing deletions, or -	1
you know, helping with noisy case but hurting in quiet case. And if you  saw  that then maybe you - it would -	1
something would occur to you of how to deal with that.	1
So that's it, I think, for [MASK]	0
Yeah. I've been playing a little bit with	1
some kind of thresholding, and,	1
as a first experiment, I think I-	1
Yeah. Well, what I did is t- is to take,	0
to measure the average -	0
no, the maximum energy of s- each utterance and then put a threshold -	1
Well, this for each [MASK]	0
Then put a threshold that's fifteen D_B below -	1
well, uh, a couple of D_B below this maximum, and -	0
Actually it was not a threshold, it was just adding noise.	1
So I was adding a white noise energy,	0
uh, that's fifteen D_B below the maximum energy of the utterance.	0
Yeah. When we look at - at the, um, [MASK] that result from this, they are  a lot more smoother.	1
when we compare, like, a channel zero and channel one utterance -	0
um, so a clean and, uh, the same noisy utterance -	0
well, there is almost no difference between [MASK] of the two.	0
And - Yeah. And the result that we have in term of speech recognition, actually it's not - it's not  worse,	1
but it's, um, kind of surprising that it's not worse because	0
basically you add noise that's fifteen D_B - just fifteen D_B below  the maximum energy. And	0
So why does that m-  smooth things out? I don't - I don't understand that.	0
Well, there's less difference. Right? Cuz it's -	0
It's - I think, it's whitening -	0
This - the portion that are more silent, as you add a white noise that are - has a very high energy, it whitens everything and -	0
and the high-energy portion of the speech	0
don't get much affected anyway by the other noise. And as the noise you add is the same is -  the shape, it's also the same.	0
So they have - the trajectory are very, very similar. And - and -	0
So, I mean, again, if you trained in one kind of noise and tested in the same kind of noise, you'd -	0
you know, given enough training data you don't do b- do badly.	0
The reason that we d- that we have the  problems  we have is because  it's different in training and test. Even if	0
the general  kind  is the same, the exact instances are different. And - and	0
so when you  whiten  it, then it's like you - the - the only noise -	0
to - to first order, the only th- noise that you  have  is white noise and you've added the same thing to training and test. So it's,	0
So would that  be similar to, like, doing the smoothing, then, over time or - ?	0
Well, it's a  kind  of smoothing, but -	0
I think it's - I think it's different. It's -	0
it's something that - yeah, that affects more or less the silence portions because -	0
Well, anyway, the sp- the portion of speech that ha- have high energy are not ch-	0
a lot affected by the noises in [MASK] If - if you compare	0
th- the two  shut  channels of [MASK] during speech portion, it's n-	0
[MASK] are not very different.	0
They are very different when energy's lower, like during fricatives or during speech pauses.	0
Yeah, but you're still getting more recognition errors, which means  that the differences, even though they look like they're not so big,	0
are - are hurting your recognition. Right?	0
Yeah. So it distort  the speech. Right. Um.	0
No. It didn't. But -	0
Yeah. So, but in this case I - I really expect that maybe	0
the - the two - these two stream of features, they are very different. I mean, and	0
maybe we could gain something by combining them or -	0
Well, the other thing is that you just picked one particular way of doing it. Uh, I mean, first place it's fifteen D_B, uh,	0
down across the utterance. And	0
maybe you'd want to have something that was a little more adaptive. Secondly, you happened to pick fifteen D_B and	0
maybe twenty'd be better, or - or twelve.	0
So what was the - what was the threshold part of it? Was the threshold, uh, how far down - ?	0
Yeah. Well, he - yeah, he had to figure out how much to add.	0
So he was looking - he was looking at the peak value.	0
And - and so what's - ho- I don't understand. How does it go? If it - if - if the peak value's above some threshold, then you add the noise? Or if it's below s-	0
I  systematically    add the noise, but	0
the, um, noise level is just  some kind of threshold below the peak.	0
Oh, oh. I see. I see.	0
Yeah. Which is not really  noise,  actually. It's just adding a constant to each of [MASK] uh, energy.	0
To each of [MASK] Yeah.	0
So, yeah, it's really, uh, white noise.  I th-	0
So then afterwards a log is taken, and that's	0
sort of why the -	0
the little variation tends to go away.	0
Yeah. So may- Well, the - this threshold is still a factor that we have to look at. And	0
I don't know, maybe a constant noise addition would -  would be fine also, or -	0
Or - or not constant but - but, uh, varying over time  in fact	0
is another way  to go.	0
Were you using the - the normalization in addition to this? I mean, what was the rest of the	0
Yeah. It was - it was, uh, the same system.	0
It was the same system.	0
A third thing is that, um,	1
I play a little bit with the, um -  finding what was different between,	1
And there were a couple of differences, like [MASK] were not the same.	0
he had [MASK] in the system.	1
the number o- of [MASK] that was - were used was different. You used thirteen and we used fifteen.	1
Well, a bunch of differences. And, um, actually	0
the result that he - he got were much better on [MASK] especially.	0
So I'm kind of investigated to see what	0
was the main factor for this difference. And it seems that	0
[MASK] is - is - was hurting.	0
Um,  so when we put s- some noise compensation	0
the, um, [MASK] that - that's derived from noisy speech is not more - anymore optimal.	0
And it makes a big difference, um,  on [MASK]	0
Uh, if we use the - the old [MASK] I mean [MASK] that was in the proposal, we have, like, eighty-two point seven percent recognition rate,	0
on noisy speech when the system is trained on clean speech.	0
and when we use the filter that's derived from clean speech we jumped -	0
so from eighty-two point seven to eighty-five point one,	0
Yeah. So now the results are more similar, and	0
I don't - I will not, I think, investigate on the other differences,	0
which is like the number of [MASK] that we keep and other small things	0
that we can I think optimize later on anyway.	0
Sure. But on the other hand if everybody is trying different kinds of noise suppression things and so forth, it might be good to standardize on the piece	0
that we're  not  changing. Right? So if there's	0
any particular reason to ha- pick one or the other, I mean -	0
Which - which one is closer to what the proposal was that was submitted to [MASK] Are they -	0
they both - ? Well, I mean -	0
Yeah. I think th- th- uh, the new system that I tested is, I guess, closer because it doesn't have -	0
it have less of - of [MASK] stuff, I -	0
The -  whatever  you, uh, tested with recently. Right?	0
Well, no, I - I'm - I -	0
Yeah, you're trying to add in [MASK] Tell  them   about the rest of it. Like you said the number of filters might be	0
different or something. Right? Or -	0
The number of [MASK] is what?	0
Yeah. So, I mean, I think we'd wanna standardize there, wouldn't we?	0
So, sh- you guys should pick something and -	0
Well, all th- all three of you.	0
I think we were gonna work with - with this or this new system, or with -	0
Uh, so the - the - right now, the - the system that is there in the -	1
what we have in the repositories, with - uses fifteen.	1
Yeah, so - Yeah, so - Yep.	0
But we will use the - [MASK] f- derived from clean speech.	0
Well, yeah, actually it's - it's not the - [MASK] It's	0
something that's also short enough in - in latency. So.	0
So, we haven't - w- we have been always using, uh, fifteen coefficients, not thirteen? Yeah.	1
Well, uh, that's - something's -	0
I think as long as you guys agree on it, it doesn't matter. I think we have a maximum of sixty,	0
uh, features that we're allowed. So.	0
Yeah. Ma- maybe we can - I mean, at least,	0
um, I'll t- s- run some experiments to see whether - once I have this	1
noise compensation to see whether thirteen and fifteen really matters or not.	1
Never tested it with the compensation, but  without,	1
uh, compensation it was like fifteen was s- slightly better than thirteen, so that's why we stuck to thirteen.	1
Yeah. And there is - there is also this log energy versus [MASK]	0
Yeah, the log energy versus [MASK]	0
Uh, that's - that's the other thing. I mean, without noise compensation certainly [MASK] is better than log energy.	0
W- w- if - if -	0
Be- I mean, because the - there are more, uh, mismatched conditions than the matching conditions for testing.	0
You know, always for the  matched  condition, you always get a   slightly  better performance for log energy than C_zero.	0
But not for - I mean,	0
for matched and the clean condition both, you get log energy -	0
I mean  you get a  better  performance with log energy.	0
Well, um, maybe once we have this noise compensation, I don't know, we have to try that also, whether we want to go for [MASK] or log energy.	0
So do you have  more, Stephane, or - ?	0
Uh, that's it, I think.  Mmm.	0
Do you have anything, Morgan, or - ?	0
Uh, no. I'm just, you know, being a manager this week. So.	0
Um,  still working on my - my quals preparation stuff.	1
Um,  so I'm - I'm thinking about, um, starting some,	1
uh,  cheating  experiments to, uh, determine the, um -	1
the relative effectiveness of, um, some intermediate categories that I want to classify.	1
So, for example, um,  if I know where voicing occurs and everything, um,	1
I would do a phone - um, phone recognition experiment, um, somehow	1
putting in the - the, uh - the perfect knowledge that I have about voicing.	0
So, um, in particular I was thinking,	0
um, in - in the hybrid framework, just taking those [MASK] files,	0
setting to zero those probabilities that, um -	0
that these phones are  not  voicing.	0
So say, like, I know this particular segment is voicing,	0
uh, go into the corresponding [MASK] file and zonk out the - the posteriors for,	0
um, those [MASK] that, um, are not voiced,	0
and then see what kinds of improvements I get.	0
And so this would be a useful thing, um, to know	1
in terms of, like, which - which, um - which of these categories are - are good for, um, speech recognition.	1
I hope to get those, uh - those experiments done by - by the time quals come - come around in July.	1
So do you just take the probabilities of the other ones and spread them out evenly among the -	0
Yeah. I - I - I was thinking - O_K, so just set to -	0
set to some really low number, the - the non-voiced, um, phones. Right? And then renormalize.	0
That will be really interesting to see, you know.	0
So then you're gonna feed the - those into  some standard recognizer. Uh, wh- are you gonna do digits or - ?	0
Yeah, m- Um, well, I'm gonna f- work with [MASK] -	0
With [MASK] O_K.	0
TIMIT - uh, phone recognition with [MASK] And, um -	0
Oh, so then you'll feed those -	0
Sorry. So where do the outputs of the net go into if you're doing phone recognition?	0
Oh. Um, the outputs of the net go into the standard, h- um, ICSI hybrid, um, recognizer.	0
So maybe, um, Chronos or -	0
An- and you're gonna - the - you're gonna do phone recognition with that? O_K, O_K.	0
So. And, uh, another thing would be to extend this to, uh, digits or something where I can look at whole words.	0
And I would be able to see, uh, not just, like, [MASK] events,	0
but, um,  [MASK] events.	0
So, like, this is from a stop to - to a vo- a vocalic	0
segment. You know, so- something that is transitional in nature.	0
So that's - that's it.	0
Let's see, I haven't done a whole lot on anything related to this this week. I've been focusing mainly on Meeting Recorder stuff.	1
So, um,  I guess I'll just pass it on to Dave.	1
Uh, O_K. Well, in my lunch talk last week I - I said I'd tried phase normalization and gotten garbage results using that l- um, long-term mean subtraction approach.	1
It turned out there was a bug in my Matlab code.	1
So I tried it again, um,	0
were - were better. I got intelligible speech back. But they still weren't as good as just subtracting the magnitude - the log magnitude means.	1
And also I've been talking to, um,	1
Andreas and Thilo about the, um, [MASK] and about coming up with a good model for,	1
um, far mike use of [MASK] So	1
I'm gonna be working on, um, implementing this mean subtraction approach in the  far-mike system - for [MASK] I mean. And, um,	1
one of the experiments we're gonna do is, um, we're gonna, um, train the - a Broadcast News net,	0
which is because that's what we've been using so far, and, um, adapt it on some	0
other data. Um, An- Andreas wants to use, um,	0
data that resembles  read  speech, like  these digit readings, because he feels that	0
[MASK] interaction is not gonna be  exactly  conversational.	0
S- so actually I was wondering, how long does it take to train that Broadcast News net?	0
The big one takes a while. Yeah. That takes two, three weeks.	0
So - but, you know, uh, you can get -	0
I don't know if you even want to  run  the big one, uh, um, in the - in the final system, cuz, you know, it takes a little while to run it. So,	0
um, you can scale it down by -	0
I'm sorry, it was two, three weeks for training up for the large Broadcast News test set - training set.	0
I don't know how much you'd be training on.	0
The full? Uh, i- so if you trained on half as much  and made the net, uh, uh, half as big, then it would be one fourth  the amount of time and it'd be nearly as good.	0
Also, I guess we had - we've had these, uh, little di- discussions - I guess you ha- haven't had a chance to work with it too much - about -	1
other ways of taking care of the phase.	0
So, I mean, I - I guess that was something I could say would be that we've talked a little bit about you just doing it all with complex arithmetic	1
and not - not, uh, doing the polar representation with magnitude and phase. But	1
it looks like there's ways that one could potentially just work with the complex numbers and - and - and in principle get rid of the	0
effects of the average complex spectrum.	0
actually, regarding [MASK] - So I did two experiments, and one is -	0
So, phases get added, modulo two pi,	0
and - because you only know the phase of the complex number t- t- to a value modulo two pi. And so I thought at first, um,	1
that, uh, what I should do is  unwrap  the phase because that will  undo  that.	1
Um, but I actually got worse results doing that unwrapping using the simple phase unwrapper that's in Matlab than I did not unwrapping at all.	1
And that's all I have to say.	0
Yeah. So I'm - I'm still hopeful that - that - I mean, we - we don't even know if the phase	0
is something - the average phase is something that we  do  want to remove. I mean, maybe there's some	0
deeper reason why it isn't the right thing to do. But, um,	0
at least in principle it looks like there's - there's, uh, a couple potential ways to do it. One - one being to just work with the complex numbers,	0
and, uh - in rectangular kind of coordinates. And the other is	0
to, uh, do [MASK] -	0
Well. So you work with the complex numbers	0
and then when you get [MASK] - [MASK] -	0
um, actually divide it out,	0
um, as opposed to taking the log and subtracting.	0
um, you know, there might be some numerical issues. We don't really know that.	0
The other thing we talked a little bit about was [MASK]	0
And, um, uh, actually I was talking to Dick Karp about it a little bit, and - and - and, since I got thinking about it, and - and, uh,	0
so one thing is that y- you'd have to do, I think, uh -	0
we may have to do this on a whiteboard, but I think you have to be a little careful about scaling the numbers that you're	0
taking - the complex numbers that you're taking the log of because	0
the Taylor expansion for it has, you know, a square and a cube, and - and so forth. And - and so if -	0
if you have a - a number that is modulus, you know, uh, very different from one -	0
It should be right around  one,  if it's -	0
cuz it's a expansion of log one - one minus epsilon or o- is - is	0
or is it one plus - ?  Well, there's an epsilon squared over two and an epsilon cubed over three, and so forth. So if epsilon is bigger than one, then it diverges.	0
So you have to do some scaling. But that's not a big deal cuz it's the log of -	0
of [MASK] times a complex number, then you can just - that's the same as log of K_ plus  log of the complex number.	0
O_K. How about you, Sunil?	1
So, um, I've been, uh, implementing this, uh, Wiener filtering for this Aurora task.	1
I - I actually  thought  it was - it was doing	1
fine when I tested it  once.  I- it's, like, using a small section of the code. And then I ran the whole recognition experiment with Italian and I got,	1
like, worse results than  not  using it.	1
So, I've been trying to find where the problem came from. And then it looks like I have some problem in	1
the way - there is some - some very silly bug somewhere. And, ugh!	1
I - I mean, i- uh, it actually - i- it actually made the whole thing  worse.  I was looking at the spectrograms that I got	1
and it's, like -  w-  it's - it's very  horrible.  Like, when I -	1
I - I missed the v- I'm sorry, I was - I was distracted. I missed the very first sentence. So then, I'm a little lost on the rest. What - what - what - ?	0
Oh, yeah. I actually implemented [MASK] as a module and then tested it out separately. And it - it - it gave, like - I just got the signal out and it - it was O_K.	0
So, I plugged it in somewhere and then - I mean, it's like I had to remove some part and then plugging it in somewhere. And then I - in that process I messed it up somewhere.	0
I mean, I thought it was all fine and then I ran it, and I got something worse than  not  using it.	0
So, I was like - I'm trying to find where the m- m- problem came, and it seems to be, like, somewhere - some silly stuff.	1
And, um, the other thing, uh, was, uh, uh -	0
Well, Hynek showed up one - suddenly on one day and then I was t- talking wi-	1
Yeah. As - as he is wont to do. Yeah.	0
Uh, yeah. So I was actually - that day I was thinking about d- doing something about [MASK] and then Carlos matter of	1
stuff. And then he showed up and then I told him. And then he gave me a whole bunch of filters - what Carlos used for his, uh,	1
uh, thesis and then  that was something which came up. And then, um -	1
uh, thinking of using  that   also  in this, uh,	0
W- [MASK] because that is a m- modified [MASK] approach, where instead of using the  current  frame, it uses	0
adjacent frames also in designing [MASK]	0
So instead of designing our own new [MASK] I may just use one of those Carlos filters in - in this implementation and see whether it - it actually gives me something better	1
than using  just  the current f- current frame, which is in a way, uh, something like the smoothing - [MASK] -	0
S- so, I don't know, I was h- I'm - I'm - I'm, like -	0
that - so that is the next thing. Once this - I - once I sort this pro- uh, problem out maybe I'll just go into that also.	1
the - the other thing was about the subspace approach.	1
I, like, plugged some groupings for computing this eigen- uh,	0
uh, uh, s- values and [MASK] So just -	0
I just  @@  some small block of things which I needed to put together for	0
the subspace approach. And I'm in the process of, like, building up that stuff.	0
I guess - Yep. I guess that's it. And, uh, th- th- that's where I am right now.	0
Oh. How about you, Carmen?	1
Mmm. I'm working with [MASK]	1
Um, I do several experiment with the Spanish database first, only with [MASK] and nothing more. Not [MASK] no [MASK] nothing more.	1
What - what is [MASK] again?	0
Eh, [MASK]	0
To remove the noise too.	1
I think I ask you that every single meeting, don't I?  I ask you that question every meeting.	0
Yeah. If - Well -	0
So, that'd be good from - for analysis. It's good to have some, uh, cases of the same utterance at different - different times. Yeah.	0
[MASK]  I'm sor-	0
Well, um, the question is that - Well.	0
Remove some noise but not too much.	0
And when we put the -	1
[MASK] the result is better. And we put everything,	1
the result is better, but it's not better than the result that we have without [MASK]	1
I see. So that  @@	1
given that you're using [MASK]  also,  the effect of [MASK] is not  so far -	1
Do you - How much of that do you think is due to just	0
the particular implementation and how much you're adjusting it? Or how much do you think is intrinsic to - ?	0
Pfft. I don't know because -	0
Are you still using only the ten first frame for noise estimation or - ?	0
Uh, I do the experiment using only the	0
onl- eh, to use on- only one fair estimation of the noise.	0
And also I did some experiment,	0
um, a lying estimation of the noise.	0
And, well, it's a little bit better but not -	0
Maybe you have to standardize this thing also, noise estimation, because	0
all the thing that you are testing use a different -	0
They all need some - some noise - [MASK] but they use - every - all use a different one.	0
No, I do that two - t- did two time.	0
If - if, uh, uh, y- you're right. I mean, each of these require this.	0
Um, given that we're going to have	0
for this test at least of - uh, boundaries,	0
what if  initially  we start off by using  known	0
sections of nonspeech  for the estimation?	0
first place, I mean even if ultimately we  wouldn't  be given the boundaries,	0
uh, this would be a good initial experiment to separate out the effects of things. I mean, how much is the poor -	0
you know, relatively, uh, unhelpful result that you're getting in this or this or this is due to	0
some inherent limitation to the  method  for these tasks and how much of it is just due to the fact that you're not accurately	0
finding enough regions that - that are really	0
So maybe if you tested it using  that,	0
you'd have more reliable  stretches of nonspeech to do the estimation from and see if that helps.	0
Yeah. Another thing is the, em - the codebook, the initial codebook.	0
That maybe, well, it's too clean  and -	0
Cuz it's a - I don't know. The methods -	0
If you want, you c- I can say something about the method.	0
a little bit different of the other method.	0
If this - if this is the noise signal,	0
uh, in the log domain, we have something like this.	0
Now, we have something like this.	0
And the idea of these methods is to -	0
n- given a, um - How do you say?	0
I will read because it's better for my English.	0
is the estimate of the P_D_F of the noise signal when we have a,	0
a statistic of the clean speech and an statistic of the noisy speech.	0
And the clean speech - the statistic of the clean speech is  from a  codebook.	0
Mmm? This is the idea.	0
Well, like, this relation is not linear.	0
The methods propose to develop this in [MASK]	0
I- I'm actually just confused about  the equations you have up there. So, uh,	0
the  top  equation is - is - is -	0
No, this in the - it's - this is the log domain. I - I must to say that.	0
Which is - which is the log domain?	0
Is the T_ - is   egual   -  is equal to,	0
And - but Y_ is what? Y_ of - the  spectrum  or - ?	0
Uh, this - this is  this  and this is  this.	0
No, no. The top Y_ is what?	0
Is that  [MASK] No, is that  [MASK] Is it - ?	0
Uh, this is the noisy speech.	0
Yeah. I guess it's the power spectrum of noisy speech. Yeah. And -	0
Yeah. It's [MASK]	0
This is the noisy - Yeah, it's -	0
Yeah, O_K. So this - it's the magnitude squared or something. O_K, so you have [MASK] added there	0
and down  here  you have -	0
you - you put the -	0
depends on  T_ , but - b- all of this is just - you just mean -	0
you just mean the  log  of the - of the one up above.	0
And, uh, so that is	0
One - one plus N_ by X_.	0
Well, y- we can expre- we can put this expression -	0
X_ times one plus, uh,	0
N_ - uh, N_ - N_ - N_ minus X_?	0
uh - So that's log of X_ plus log of one plus, uh -	0
Is that right? Log of -	0
One plus N_ by X_.	0
I actually don't see how you get that.	0
Well, if we apply the log, we have E_ is n-	0
E_ is equal, oh, to log of	0
uh, we can say that E_	0
is equal to log of,	0
um, exponential of X_ plus exponential of N_.	0
Well, if E_  restricts  -	0
Well, this is - this is in the ti- the time domain. Well, we have that, um -	0
We have first that, for example, X_	0
is equal, uh - Well.	0
This is the frequency domain and we can put  u- that n- the log domain -	0
log of X_ omega,  but, well, in the time domain we have an exponential.	0
Oh, maybe it's I am - I'm problem.	0
Yeah. I mean, just never mind what they  are.  Uh, it's just if X_ and N_ are variables -	0
What is, uh - ?	0
The - the - the log of X_ plus N_ is not the same as the log of E_ to the X_ plus E_ to the N_.	0
Maybe we can take it off-line, but I - I don't know.	0
I - I can do this incorrectly.	0
Well, the expression that appear in the - in the paper,	0
the Taylor series expansion for log one plus N_ by X_ is -	0
Is it the first-order expansion? Yeah, I guess. Yeah. Uh-huh.	0
Yeah, the first one. Yeah.	0
O_K. Yeah. Cuz it doesn't just follow what's there. It has to be some, uh, Taylor series -	0
Y- yeah. If - if you take log X_ into log one plus N_ by X_, and then expand the log one plus N_ by X_ into Taylor series -	0
Now, this is the - and then -	0
Yeah, but the - the second  expression that you put is the first-order expansion of	0
the nonlinear relation between -	0
No, no, no. It's not the first  space . Well, we have -  pfft, uh, em -	0
Well, we can put that	0
X_ is equal - I_ is equal to log of, uh,	0
Well, we can put, uh, this?	0
That - I mean, that - the f- top one does not  imply the second one.	0
Because - cuz the log of a  sum  is not the same as  th- I mean, as - Yeah.	0
Yeah, yeah, yeah, yeah, yeah. But we can -	0
uh, we - we know that, for example, the log of	0
E_ plus B_ is equal to log of E_ plus log	0
to B_. And we can say here, it i-	0
Right. So you could s-	0
And we can, uh, put this inside.	0
I don't see how you get the second expression from the top one.	0
The - I mean, just more generally here,	0
if you say "log of, um, A_ plus B_",	0
the log of - log of A_ plus B_ is not -	0
or A_ plus B_ is not the,	0
um, log of E_ to the A_ plus E_ to the B_.	0
No, no, no, no, no, no, no. This not. No.	0
Right? And that's what you seem to be saying.	0
No. It's not. But this is the same - oh.	0
Right? Cuz you - cuz you - up here you have the A_ plus B_ -	0
No. I say if I apply log, I have, uh, log of E_	0
is equal to log of, uh - in this side, is equal to log of X_	0
No? Right. This is right.	0
Right. And then how do you go from there to the - ?	0
And then if I apply exponential,	0
Look. O_K, so let's - I mean, C_ equals A_ plus B_,	0
It's log o- of capital Y_. Yeah, right. Capital  Y_.	0
X_. X_. This is X_, inside.	0
Right. Yeah. That one's right.	0
S- uh, i- th- we can put here the  set  transformation.	0
O_K, I understand now.  Alright, thanks.	0
Yeah. In this case, well, we can put here a  Y_.	0
O_K. So, yeah. It's just by definition  that the individual -	0
that the, uh - So, capital X_ is by definition the same as E_ to the little X_ because she's saying that the little X_ is - is the, uh - is the log.	0
Now we can put this. No?	0
And here we can multiply by X_.	0
Alright.  I think these things are a lot clearer when you can use fonts -	0
different  fonts  there  so you know which is which. But I - I under- I understand what you mean now. O_K.	0
Yeah, yeah. That's true. That's true.	0
But this - this is correct? And now I can do it, uh - pfff!	0
And that's where it comes from. Yeah.	0
Well. The idea - Well, we have fixed this equa-	0
O_K. So  now  once you get that -  that  one, then you - then you do a first- or second-order, or something, Taylor  series expansion of this.	0
Yeah. This is another linear relation that this - to develop this in	0
Mm-hmm. And for that, well, the goal is to obtain, um -	0
est- estimate a P_D_F for the noisy speech	0
when we have a -  a statistic for clean speech and for the noisy speech.	0
the way to obtain the P_D_F for the noisy speech is -	0
statistic and we know the noisy st-	0
well, we can apply first	0
order of the vector st- Taylor series of the - of the - of - well,	0
the order that we want, increase the complexity of the problem. And then when we have a expression, uh, for the	0
mean and variance of the noisy speech,	0
we apply a technique of minimum mean-square estimation	0
to obtain the expected value	0
of the clean speech given the - this	0
statistic for the noisy speech -	0
the statistic for clean speech and the statistic of the noisy speech.	0
But the idea is that -	0
And the - the model of clean speech is a codebook. Right?	0
Yeah. We have our codebook	0
We can expre- we can put that the	0
P_D_F  for the clean test, probability of the clean speech is equal to -	0
how much - in - in the work  they  reported, how much noisy speech did you need to get, uh, good enough statistics	0
for the - to get this mapping?	0
I - I need to s- I don't know exactly.	0
Cuz I think  what's    certainly  characteristic of a lot of	0
the  data in this test is that, um, you don't have -	0
the - the training set may not be a - a great estimator	0
for the noise in the test set. Sometimes it is and sometimes it's not.	0
Yeah. I - the clean speech - the codebook for clean speech, I am using [MASK]	0
And I have now, uh, sixty-four	0
And what are you using for the noisy - ? Y- y- doing that strictly -	0
Of the noise - I estimate the noises wi- Well, for the noises I only use one Gaussian.	0
And - and you - and you train it up entirely from, uh, nonspeech sections in the test?	0
Uh, yes. The first experiment that I do it is solely to calculate the, mmm - well, this value -	0
uh, the compensation of the dictionary o- one time using the - the noise at the f-	0
beginning of the sentence. This is the first experiment. And I fix this for all the -	0
Uh, because - well, the V_T_S methods -	0
In fact the first thing that I do is to - to obtain, uh, an expression for E_ -	0
probability e- expression of - of E_.  That   mean that [MASK] - mmm, with [MASK] we obtain, uh -	0
well, we - we obtain the means for each Gaussian	0
This is one. Eh, this is the composition of the dictionary.	0
This one thing. And the other thing that this - with these methods is to, uh, obtain - to calculate this value.	0
Because we can write -	0
uh, we can write that	0
the estimation of the clean speech	0
is equal at an expected value of	0
the clean speech conditional to, uh, the noise signal -	0
the probability f- of the - the statistic of the clean speech and the statistic of the noise.	0
This is the methods that say that we're going obtain this.	0
And we can put that this is equal to the estimated value of E_ minus a function	0
that conditional to [MASK] to [MASK] - to the noise signal. Well, this is - this function is the	0
the term - after develop this, the term that we - we take.	0
Give [MASK] and, uh, [MASK] the noise.	0
And I can  put that this is equal to  the  noise signal minus -	0
Well, I put before  this name,  uh -	0
And I can calculate this.	0
What is the first variable in that probability?	0
Uh, this is the Gaussian.	0
No, no. I'm sorry. In - in the one you pointed at. What's  that  variable?	0
Uh, this is the -	0
Weak. So probably it - it would do that.	0
like  this,  but conditional. No, it's condition- it's not exactly this. It's	0
It's one mixture of the model. Right?	0
Uh, if we have clean speech - we have the dictionary for the clean speech, we have a	0
probability f- of - our - our weight for each Gaussian.	0
No . And now, this weight is different now because it's conditional. And	0
this I need to - to calcu- I know  this  and I know  this  because this is from the dictionary that you have.	0
I need to calculate this.  And for calculate this,	0
I have an - I - I can develop an expression that is	0
I can calculate - I can - I calculated this value,	0
uh, with the statistic of the noisy speech that I calculated before with [MASK] approximation.	0
when I develop this in s- Taylor - Taylor series, I can't, um,	0
calculate the mean and the variance	0
of the - for each of the Gaussian of the dictionary for the noisy speech.	0
If I never do an estimat- a newer estimation of the noise, this mean as - mean and the variance are fixed.	0
And for each s- uh, frame of the speech the only thing that I need to do is to calculate this	0
the estimation of the clean speech given our noisy speech.	0
So, I'm - I'm not following this perfectly but, um,  I -	0
Are you saying that all of these estimates are done  using, um,	0
estimates of the probability density for the  noise  that are calculated  only  from the first ten frames?	0
And never change throughout anything else?	0
Never cha- This is one of the approximations that I am doing.	0
Per - per - per utterance, or per - ?	0
Per utterance. Yes. Per utterance. Yes. And th-	0
So it's done - it's done  new  for each new utterance. So this changes the whole mapping for every utterance.	0
Yeah. It's fixed, the dictionary. And the other estimation is when I do the uh on-line estimation, I change the means and variance of th- for the noisy speech	0
each time that I detect noise.	0
I do it uh again this	0
Estimate the new mean and the variance of the noisy speech. And with th- with this new s- new mean and variance I estimate  again  this.	0
So you estimated, uh, f- completely forgetting what you had before?	0
Um, no, no, no. It's not completely - No, it's - I am doing something like an adaptation of the noise.	0
Uh, or is there some adaptation?	0
Now do we know, either from their experience or from yours, that, uh, just having, uh, two parameters, the - the mean and variance, is enough?	0
Yeah. I mean, I know you don't have a  lot  of data to estimate  with,  but - but, uh,	0
I estimate mean and variance for each one of the Gaussian of the codebook.	0
No, I'm talking about the noise.	0
Oh, um. Well, only one - I am only - using only one. I don't know i-	0
And you - and - and it's, uh, uh - right, it's only -	0
Wait a minute. This is - what's the dimensionality of the Gaussian? This is -	0
Uh, it's in - after [MASK]	0
So this is twenty or something? Twenty?	0
So it's - Yeah. So it's actually forty numbers  that you're getting.	0
Uh, the original paper say that only one Gaussian for the noise.	0
maybe you don't have a -	0
Well, yeah. But, I mean,  no - no paper is - is a Bible, you know. This is - this is, uh -	0
Yeah, maybe isn't the right thing.	0
whether it would be helpful, i- particularly if you used - if you had more -	0
So, suppose you did - This is almost cheating. It certainly isn't real-time. But if y- suppose you use the real boundaries that - that you were - in fact were given	0
by [MASK] and so forth -th -th -	0
or I - I guess we're gonna be given even  better  boundaries than that.	0
And you look - you take all o- all of the nonspeech components in an utterance, so you have a  fair  amount.	0
Do you  benefit  from having a better model for the noise? That would be another question.	0
So first question would be	0
to what extent i- are the errors that you're still seeing	0
based on the fact that you have poor boundaries for the, uh, uh, nonspeech?	0
And the  second  question might be, given that you  have  good boundaries,	0
could you do  better  if you used more parameters to characterize the noise?	0
Also another question might be -	0
Um, they are doing - they're using first term only of the vector Taylor series?	0
Um, if you do a  second  term does it get too complicated cuz of the nonlinearity?	0
No, I won't ask the next question then.	0
Oh, it's - it's the - for me it's the first time that I am working with [MASK] Uh -	0
Yeah. No, it's interesting. Uh, w- we haven't had anybody work with it before, so it's interesting to get your - get your feedback about it.	0
It's another type of approximation because i- because it's a statistic -	0
statistic approximation to remove the noise.	0
O_K. Well, I guess we're about done. Um, so some of the digit forms don't have digits.	0
Uh,  we ran out there were some blanks in there, so not everybody will be reading digits. But, um,	0
I guess you've got some. Right, Morgan?	0
So, why don't you go ahead and start. And I think it's  just us down here at this end that have them. So.	0
S- so, we switch off with this or n- ?	0
Uh, leave it on, uh, and the -	0
They prefer to have them on just so that they're continuing to get the distant, uh, information.	0
