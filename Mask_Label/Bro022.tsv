I think for two years we were two months, uh, away from being done.	0
And what was that, Morgan?	0
Uh, the, uh, [MASK]	0
Yeah. We were two - we were -	0
Uh, uh, we went through it - Jim and I went through old emails at one point and - and for two years there was this thing saying, yeah, we're - we're two months away from being done.	0
It was very - very believable schedules, too. I mean, we went through and - with the schedules - and we - Yeah. Oh, yeah. It was  very  true.	0
It was true for two years.	0
So, should we just do the same kind of deal where we  go around and do, uh,	1
status report  kind of things?	1
O_K. And I  guess   when Sunil gets here he can do his last or something.	0
Yeah. So we  probably should wait for him to come before we do his.	0
O_K. That's a good idea.	0
Do you want to start, Morgan? Do you have anything, or - ?	0
Uh, I don't do anything. I -  No, I mean, I - I'm involved in discussions with - with people about what they're doing, but I think they're -	0
since they're here, they can talk about it themselves.	0
O_K. So should I go so that, uh,	0
Yeah. Why don't you go ahead, Barry?	1
you're gonna talk about Aurora stuff, per se? O_K. Um.	0
Well, this past week I've just been, uh, getting down and dirty into writing my - my proposal.	1
Mmm. I just finished a section on, uh - on talking about these intermediate categories that I want to classify,	1
um, as a - as a middle step. And, um, I hope to - hope to get this, um - a full rough draft done by, uh, Monday so I can give it to Morgan.	1
When is your, uh, meeting?	0
Um, my meeting with, uh - ? Oh, oh, you mean the - the quals. Uh, the quals are happening in July twenty-fifth.	0
So, is the idea you're going to do this paper and then you pass it out to everybody ahead of time and - ?	0
Right, right. So, y- you write up a proposal, and give it to people ahead of time, and you have a short presentation.	0
And, um, and then, um - then everybody asks you questions.	0
Yep. So, um. Y- s-	0
I was just gonna ask, do you want to say any - a little bit about it, or - ?	0
Oh. Uh, a little bit about - ?	0
Wh- what you're - what you're gonna - You said - you were talking about the, uh, particular features that you were looking at, or -	0
Oh, the - the - Right. Well, I was, um,	0
I think one of the perplexing problems is, um,	0
for a while I was thinking that I  had  to come up with a  complete  set of intermediate features - in- intermediate categories to - to classify right away.	0
But what I'm thinking now is, I would start with - with a  reasonable  set. Something - something like, um, um -	0
like, uh, re- regular [MASK] just to - just to start off that way.	0
And do some phone recognition. Um, build a system that, uh, classifies these, um - these feat- uh, these intermediate categories using, uh, [MASK]	0
Combine them and do phon- [MASK]	0
Look at - then I would look at the  errors  produced in the phoneme recognition and say, O_K, well, I could probably reduce the errors if I included this extra feature or this extra intermediate category.	0
That would - that would reduce  certain  confusions over other confusions.	0
And then - and then  reiterate.	0
Um, build the intermediate classifiers. Uh, do [MASK] recognition. Look at the errors. And then  postulate  new - or  remove,  um, intermediate categories. And then do it again.	0
So you're gonna use [MASK]	0
Um, for that - for that part of the - the process, yeah, I would use [MASK]	0
after - after, uh, um, doing [MASK] Right? Um, that's -	0
that's, um - that's just the ph- the phone recognition task.	0
Uh, I wanted to take a look at, um, things that I could model  within  word.	0
So, I would mov- I would then shift the focus to, um, something like Schw- Switchboard,	0
uh, where I'd - I would be able to, um - to model, um, intermediate categories that span  across  phonemes, not just within the phonemes, themselves,	0
um, and then do the same process  there,  um, on - on a large vocabulary task like Switchboard.	0
Uh, and for that -	0
for that part I would - I'd use the S_R_I recognizer since it's already set up for - for Switchboard. And I'd run some - some sort of [MASK] processing with, uh, my intermediate classifiers.	0
Oh. So that's why you were interested in getting your own features into the S_R_I files.	0
Yeah. That's why I - I was asking about that. Yeah.	0
I guess that's - that's it. Any - any questions?	0
Sounds good. So you just have a few more weeks, huh?	0
Um, yeah. A few more.	0
It's about a month from now?	0
It's a - it's a month and - and a week.	0
So, uh, you want to go next, Dave? And we'll do -	1
Oh. O_K, sure. So, um, last week I finally got results from the S_R_I system about this [MASK] approach.	1
And, um, we - we got an improvement, uh, in word error rate, training on [MASK] data set and testing on Meeting Recorder digits of, um,	1
six percent to four point five percent, um, on the n-	1
on the far-mike data  using [MASK] but, um, the near-mike performance worsened, um, from one point two percent to two point four percent.	1
wh- why would that be, um,	1
considering that we actually got an improvement in near-mike performance using [MASK] And so,	1
uh, with some input from, uh, Andreas, I have a theory in two parts. Um,	1
first of all [MASK] - sorry, S_R- the S_R_I system is doing	1
This mean subtraction approach will do a kind of channel  normalization and so that might have given [MASK] use of it a boost that wouldn't have been applied in the S_R_I case.	0
And also, um, the - Andreas pointed out the S_R_I system is using more parameters. It's got finer-grained acoustic models.	1
So those finer-grained acoustic models could be more sensitive to the artifacts  in the re-synthesized audio.	1
And me and Barry were listening to the re-synthesized audio and sometimes it seems like you get of a bit of an  echo  of speech	0
in the background. And so that seems like it could be difficult for training, cuz you could have  different phones  lined up with a different foreground phone,	0
depending on  the timing of the echo. So, um,	0
I'm gonna try training on a  larger  data set, and then, eh, the system will have seen more examples o- of these artifacts and hopefully will be more robust to them.	0
So I'm planning to use the Macrophone	0
set of, um, read speech, and, um - Hmm.	0
I had another thought just now, which is, uh, remember we were talking before about -	0
we were talking in our meeting about, uh, this stuff that -	0
some of the other stuff that Avendano did, where they were,	0
um, getting rid of low-energy  sections?	0
Um, uh, if you -	0
if you did [MASK] as Hirsch did in  late eighties to reduce some of the effects of reverberation,	0
uh, uh, Avendano and Hermansky were arguing that,	0
uh, perhaps one of the reasons for that working was ma- may not have even been the  filtering  so much but the fact that when you filter a -	0
[MASK] you get some negative values,	0
and you gotta figure out what to do with them if  you're   gonna continue treating this as [MASK]	0
So, what - what Hirsch did was, uh, set them to zero - set the negative values to zero.	0
So if you imagine a - a waveform that's all positive,	0
which is the time trajectory of energy,	0
um, and, uh, shifting it downwards,	0
and then getting rid of the negative parts,	0
that's essentially throwing away the low-energy things.	0
And it's the low-energy parts of the speech	0
where the reverberation is most audible. You know, you have the reverberation from higher-energy things showing up in -	0
So in this case you have some artificially imposed  reverberation-like thing. I mean, you're getting rid of	0
some of the  other  effects of reverberation,	0
but because you have these [MASK]	0
you're getting these funny things coming in, uh, at n-	0
And, um, what if you did - ?	0
I mean, there's nothing to say that the - the processing for this re-synthesis has to be restricted	0
to trying to get it back to	0
the original, according to some equation. I mean, you also	0
could, uh, just try to make it nicer.	0
And one of the things you could do is, you could do some sort of [MASK] -like thing and you actually  could	0
take very low-energy sections and set them to some - some, uh, very low or - or near zero  value.	0
I mean, uh, I'm just saying  if  in fact it turns out that - that these echoes that you're hearing	0
or pre-echoes, whichever they are - are - are,	0
uh, part of what's causing the problem, you actually could get rid of them.	0
I mean, you do it in a pretty conservative way so that if you made a mistake you were more likely to  keep in an echo than to throw out speech.	0
Um, what is the reverberation time  like   there ?	0
In thi- in this room? Uh -	0
On, uh, the - the one what - the s- in the speech that you are - you are using like?	0
Y- Yeah. I - I - I - I don't know.	0
It's, uh - Oh, this room? O_K.	0
It's - it's this room. So -	0
so it's - these are just microphone -	0
this micro- close microphone and a distant microphone, he's doing these different tests on.	0
Uh, we should do a measurement in here. I g- think we never have. I think it's - I would guess, uh,	0
point seven, point eight seconds f- uh, [MASK] - something like that?	0
But it's - you know, it's this room.	0
But the other thing is, he's putting in - w- I was using the word "reverberation" in two ways. He's also putting in,	0
he's taking  out  some reverberation, but he's putting  in  something, because he has  averages over multiple windows stretching out to twelve seconds,	0
which are then being subtracted from the speech. And since,	0
you know, what you subtract, sometimes you'll be - you'll be subtracting from some larger number and sometimes you won't. And -	0
So you can end up with some  components  in it that are affected by things that are seconds away.	0
Uh, and if it's a low  energy compo-  portion,  you might actually hear some  funny things.	0
O- o- one thing, um, I noticed is that, um, the mean subtraction seems to make [MASK] signals  louder  after they've been re-synthesized.	1
So I was wondering, is it possible that one reason it helped with [MASK] is  just as a kind of gain control?	1
Cuz some of [MASK] sound pretty quiet if you don't amplify them.	1
I don't see why - why your signal is louder after processing, because yo-	1
Yeah. I don't know why -y, uh, either.	0
I don't think just multiplying the signal by two would have any effect.	1
Yeah. I mean, I think if you really have louder signals,	1
what you mean is that you have  better signal-to-noise ratio.	1
So  if  what you're doing is improving the signal-to-noise ratio, then it would be better.	0
But just it being  bigger  if - with the  same  signal-to-noise ratio -	0
It w- i- i- it wouldn't affect things. O_K.	0
Well, the system is -	0
use  the absolute energy, so it's a little bit dependent on -	0
But, not so much, I guess.	0
Well, yeah. But it's trained and tested on the same thing. So if the - if the - if you change	0
in both training and test, the absolute level by a factor of two, it will n- have no effect.	0
Did you add  this data to the training set, for [MASK]	0
Or you just tested on this?	0
Did I w- what? Sorry?	0
Well, Morgan was just saying that, uh, as long as you do it in both training and testing, it shouldn't have any effect. But I -	0
I was  sort of under the impression that you just  tested  with this data. You didn't  train it also.	0
I - Right. I trained on clean [MASK] I - I did the mean subtraction on	0
I only remember noticing it made the, um, [MASK] louder.	0
O_K. Well, I don't understand then. Yeah.	0
I don't know. If it's - if it's - like, if it's trying to find a - a reverberation filter, it could be that this reverberation filter is	0
making things  quieter.  And then if you take it out -	0
that taking it out makes things louder.	0
Uh, no. I mean,  uh, there's - there's nothing inherent about	0
removing - if you're really removing,	0
uh, r- uh, then I don't  see how that would make it louder. So it might be just some -	0
O_K. Yeah, I see. Yeah. O_K. So I should maybe listen to that stuff again.	0
Yeah. It might just be some artifact of the processing that - that, uh, if you're -	0
Uh, yeah. I don't know.	0
I wonder if there could be something like, uh -	0
for s- for [MASK] uh, you know, if occasionally, uh, somebody hits the table or something, you could get a spike.	0
I'm just wondering if there's something about the, um - you know, doing the mean normalization where,	0
uh, it - it could cause  you to have better signal-to-noise ratio.	0
Well, you know, there  is   this.	0
Wait a minute. It - it - i- maybe -	0
Subtracting the - the mean log spectrum is - is - is like dividing by the spectrum.	0
So, depending what you divide by, if your - if s- your estimate is off and sometimes you're - you're - you're getting a small number,	0
you could make it bigger.	0
So, it's - it's just a - a question of -	0
there's - It - it could be that there's some normalization that's missing,	0
or something  to make it -	0
Uh, y- you'd think it shouldn't be larger, but	0
maybe in practice it is.	0
That's something to think about. I don't know.	0
I had a question about the system - the S_R_I system. So,  you trained it on [MASK]	0
But except this, it's exactly the same system as the one that was tested before and that was trained on  [MASK] Right?	0
So on [MASK] it gives you one point two percent error rate and on [MASK] it's still O_ point eight.	0
Uh, but is it  exactly the same system?	0
I  think  so. If you're talking about [MASK] results that Andreas had about, um, a week and a half ago, I think it's the same system.	0
So you use [MASK] uh,	0
like M_L_L_R transformations also, and -	0
I'm sorry, was his point eight percent, er, a - a result on  testing  on [MASK] or - or training?	0
It was  training on [MASK] and testing - yeah, on - on meeting digits.	0
Oh. So that  was  done already. So we were -	0
Uh, and it's point eight? O_K.	0
Yeah. I - I've just been text-  testing the new  [MASK] front-end with - well,	0
um, acoustic models on the meeting digits and it's a little bit better than the previous system. We have - I have two point seven percent error rate.	0
And before with the system that was proposed, it's what? It was three point nine. So.	0
Oh, that's a  lot  better.	0
So, what - w- ?	0
With the - with the [MASK] back-end? What we have for Aurora?	0
I know in the meeting, like - Right.	0
On the meeting we have two point seven.	0
That's with the new [MASK]	0
Uh. Yeah, yeah. So, yeah, we have  the new [MASK] and -	0
I think, maybe - I didn't look, but	1
one thing that makes a difference is this [MASK] Uh, eh -	1
Do y- did you have a look at - at the meet- uh, meeting digits, if they have [MASK] or - ?	1
No. [MASK] could be negligible. I mean, if you are  recording it through a	1
mike. I mean, any - all of the mikes have the D_C removal - some capacitor sitting right in   that  bias  it .	1
Yeah. But this - uh, uh, uh, no. Because, uh, there's a sample and hold	0
in the A_to_D. And these period- these typically  do  have [MASK]	0
And - and they can be  surprisingly  large. It depends on the electronics.	0
Oh, so it is the digital - O_K. It's [MASK] that introduces the D_C in.	0
Yeah. The microphone isn't gonna pass any D_C. But - but,	1
typi- you know, unless - Actually, there are  instrumentation mikes that - that  do  pass - go down to D_C. But - but,	1
uh, no, it's the electronics. And they - and -	1
then there's amplification afterwards. And you can get,	1
I think it was -	0
I think it was in the  Wall Street Journal data	0
that - that - I can't remember,  one  of [MASK] things. There was this big D_C- D_C offset we didn't - we didn't know about for a while, while we were	0
messing with it. And we were getting these terrible results. And then	0
we were talking to somebody and they said, "Oh, yeah. Didn't you know? Everybody knows that. There's all this [MASK] in th-"	0
So, yes. You can have [MASK] in the data. Yeah.	1
So was that - was that everything, Dave?	0
Oh. And I also, um, did some experiments  about normalizing the phase.	1
Um. So I c- I came up with a web page that people can take a look at. And, um,	1
the interesting thing that I tried was, um, Adam and Morgan had this idea,	1
um, since my original attempts to, um, take the mean of [MASK] over time	1
and normalize using  that,  by subtracting that off, didn't work. Um,	1
so, well, that we thought that might be due to, um, problems with, um,	1
[MASK] They - they add in this modulo two pi way and,	1
there's reason to believe that	0
that approach of taking the mean of the phase spectrum wasn't really  mathematically correct. So,	1
what I did instead is I	1
took the mean of [MASK] without taking the log or anything, and then I took the phase of  that,	1
and I subtracted  that  phase  off	1
to normalize. But that, um,	1
See, we have a different interpretation of this. He says it doesn't work. I said, I think it works  magnificently,  but just not for the task we intended.	0
Uh, it gets rid of the  speech.	0
Uh, gets rid of the speech.	0
Uh, it leaves - you know, it leaves the  junk.  I mean, I - I think it's - it's tremendous.	0
You see, all he has to do is go back and reverse what he did before,  and he's really  got  something.	0
Well, could you take what was left over and then subtract that?	0
Ex- exactly. Yeah, you got it.	0
So, it's - it's a general rule. Just listen very carefully to what I say and do the opposite.	0
Including what I just said.	0
Do you want to go, Stephane?	1
Um. Yeah. Maybe, concerning these d-	0
I'm more interested in trying to figure out what's still the difference between the S_R_I system and [MASK]	1
Um. Yeah. So, I think I will maybe train, like, gender-dependent models, because  this is also one big difference between  the two systems.	1
the other differences were  the fact that maybe the acoustic models of the S_R_I are more - S_R_I system are more complex.	1
But, uh, Chuck, you did some experiments with this and	1
It didn't seem to help in [MASK]	1
it was hard t- to - to have some exper- some improvement with this. Um.	0
Well, it sounds like they also have - he - he's saying they have all these, uh,	0
uh, different kinds of adaptation. You know, they have channel adaptation. They have speaker adaptation.	1
Well, there's also the normalization. Like they do, um -	1
I'm not sure how they would do it when they're working with the digits, but, like, in [MASK] there's, um -	1
Yeah. This is another difference. Their normalization works like	1
on - on the utterance levels.	1
But we have to do it -	0
We have a system that does it on-line. So, it might be -	0
it might be better with - it might be worse if	0
the  channel is constant, or -	0
And the acoustic models are like -k [MASK] or - or is it the whole word?	1
S_R_I - it's - it's tr- Yeah. I guess it's [MASK]	0
I think it's probably more than that. I mean, so they - they have - I - I thin- think they use these, uh,	1
things. So there's - there's these kind of, uh, uh, pooled models and -	1
and they can go out to all sorts of dependencies.	1
Oh. It's like the  tied  state.	0
They have tied states and I think -	1
I - I - I don't real- I'm talk- I'm just guessing here. But I think - I think they - they don't  just  have triphones. I think they have a range of - of, uh, dependencies.	0
Well, the first thing I - that I want to do is just maybe these gender things.	0
And maybe see with  Andreas if - Well, I - I don't know  how much it helps, what's the model.	0
So - so the n- stuff on the numbers you got, the two point seven, is that using the same training data that the S_R_I system used and got one point two?	0
That's right. So it's the clean  [MASK] training set.	0
So exact same training data?	0
I guess you used the clean training set.	0
Right. For - with the S_R_I system -	0
You know, the - [MASK] is set up with these, um -  this version of the clean training set that's been filtered	0
with this [MASK] and,	0
um, to train the S_R_I system on digits S_- Andreas used the	0
[MASK] um, u	0
So is that - ? Uh, are - are these results comparable? So you - you were getting with the, uh, [MASK]	0
something like two point four percent  on	0
when, uh, training the S_R_I system with clean [MASK] -  [MASK] Right?	0
And, so, is your two point seven comparable,	0
where you're, uh, uh, using,	0
Yeah. I think so. Yeah.	0
O_K. So it's   about  the same, maybe a little worse.	0
W- w- it was one - one point  two	0
with the S_R_I system, I -	0
Yeah. The complete S_R_I system is one point two. Yeah.	0
You - you were [MASK] Right? O_K. That's right. So - O_K, so  the comparable number then, uh	0
for what you were talking about then, since it was [MASK] would be the	0
It was four point something. Right?	0
The H_T_K system with, uh, b-	0
Oh, right, right, right, right.	0
[MASK] features -	0
Do you mean the b- ?	0
[MASK]	0
trained on [MASK] tested on Meeting Recorder near, I think we saw in it today, and it was about six point six percent.	0
Right. Right, right, right. O_K.	0
So - Yeah. The only difference is the features, right now, between this and -	0
He's doing some  different things.	0
Yes. O_K, good. So they  are  helping. That's good to hear. Yeah.	0
Yeah. And another thing I - I maybe would like to do is to  just test	0
the S_R_I system that's trained on [MASK] - test it on,	0
uh, [MASK]	0
cuz I'm still wondering  where this  improvement	0
comes from. When you train on [MASK] it seems better on meeting digits.	0
But I wonder if it's just because maybe  [MASK] is acoustically closer to the meeting digits than - th	0
[MASK] are very  clean recorded digits and -	0
You know, it would also be interesting to see, uh - to do [MASK] um, but use the S_R_I system instead of [MASK]	0
That's - Yeah. That's what  I wanted, just, uh - Yeah.	0
So, just using the S_R_I system, test	0
it on - and test it on  [MASK] Right.	0
Why not [MASK] uh, test?	0
Um. Yeah. There is this problem of multilinguality yet. So we don't -	0
You'd have to train [MASK] with - with all the different languages.	0
We would have to train on -	0
Yeah. That's what I mean. So, like, comple-	0
It'd be a  lot of work. That's the only thing.	0
Mmm. Well, I mean, uh,	0
I guess the work would be into getting the -	0
the files in the right formats, or something. Right? I mean -	0
Because when you train up [MASK] you're, uh - you're also training on all the	0
data. I mean, it's -	0
Yeah. I see. Oh, so, O_K. Right. I see what you mean.	0
That's true, but I think that also	0
when we've had these meetings week after week, oftentimes people have not done the full arrange of things because - on - on  whatever  it is they're trying, because it's a lot of work, even just with [MASK]	0
So, it's - it's a good idea, but it seems like  it makes sense to do some pruning	0
first with a - a test or two that makes sense for you, and then  take the likely candidates and go further.	0
But, just testing on [MASK] would already give us some	0
information  about what's going on.	0
Uh, the next thing is this - this [MASK] problem that,	1
So, I'm just talking about the - the curves that I - I sent -	1
I sent you - so, whi- that shows that	1
when [MASK] decrease,	1
uh, [MASK] approach doesn't drop much frames  for	1
uh, which might be then noises that are closer to speech, uh, acoustically.	1
I- i- Just to clarify something for me. I-	1
They were supp- Supposedly, in the next evaluation, they're going to be supplying us with boundaries. So does any of this matter?	1
I mean, other than our interest in it. Uh -	0
Well. First of all, the boundaries might be, uh - like we would have	1
two hundred milliseconds or - before and after speech. Uh.	1
So removing more than that might still make  a difference  in the results. And -	1
Do we - ? I mean, is there some reason that we think that's the case?	1
No. Because we don't - didn't	1
looked  that much at that. But,  still,	0
I think it's an interesting problem. And -	0
But maybe we'll get some insight on that when - when, uh, the gang gets back from Crete.	1
Because  there's  lots  of interesting problems, of course. And then the thing is if - if they really are going to have some means of giving us  fairly tight,	0
uh, boundaries, then that won't be so much the issue.	0
Um  But   I don't know .	0
Because w- we were wondering whether [MASK] is going to be, like, a  realistic  one or is it going to be some  manual	0
segmentation. And then, like, if - if [MASK] is  going  to be a realistic one, then we can actually use their markers to shift the	0
point around, I mean, the way we want	0
to find a - I mean, rather than keeping the twenty frames, we can actually move the marker to a point which we find more  suitable for us.	0
But if that is going to be something like a manual, uh, segmenter, then we can't  use that information anymore, because that's not going to be the one that is used in the final evaluation.	0
So. We don't know what is the type of    [MASK] which they're going to provide.	0
Yeah. There's an - uh, I think it's still for - even for the evaluation,	0
uh, it might still be interesting to  work on this because  the boundaries apparently that they would provide is just,	0
starting of speech and end of speech  uh, at the utterance level. And -	0
With some - some gap. I mean, with some pauses in the center,	0
provided they meet that - whatever the hang-over time which they are talking.	0
Yeah. But when you have like, uh, five or six frames, both -	0
Yeah. Then the- they will just fill - fill it up. I mean, th - Yeah.	0
it - it - with -	0
So if you could get at some of that, uh - although that'd be  hard.  But - but - Yeah.	0
Yeah. It might be useful	0
for, like, noise estimation, and a lot of other  things that we want to work on. But - Mmm.	0
Yeah. So I did - I just  started to test  putting together two [MASK] which was - was not much work actually.	0
Um, I im- re-implemented [MASK] that's very close to the,	0
um, [MASK]	0
that, uh, the other [MASK] guys use.	0
So, which is just putting a threshold on  the noise energy,	0
and, detect- detecting the first  group of four frames  that have a energy that's above this threshold,	0
and,  uh, from this point, uh, tagging the frames there as speech.	0
the first silent portion - portion of each utterance.	0
And it really removes it,  um,	0
still o- on the noises where  our [MASK] doesn't  work a lot.	0
Cuz I would have thought that having  some  kind of spectral  information,	0
uh, you know, in the old days people would use energy and zero crossings, for instance - uh, would give you	0
some  better performance. Right? Cuz you might have low-energy	0
[MASK] or - or, uh	0
So, your point is - will be to u- use  whatever  -	0
Oh, that if you d- if you use  purely  energy and don't look at anything spectral, then you don't have a good way of distinguishing between [MASK]	0
And, um, just as a gross generalization, most nonsp-  many  nonspeech noises have a low-pass kind of characteristic,	0
um, [MASK] that are unvoiced have a -	0
a high-pass kind of characteristic - an upward slope.	0
So having some kind of a -	0
uh, you know, at the beginning of a - of a - of an S_ sound for instance, just starting in, it might be pretty low-energy, but it will tend to have this high-frequency component. Whereas,	0
a - a lot of rumble, and background noises, and so forth will be predominantly low-frequency. Uh, you know, by itself it's not enough	0
to tell you, but it plus energy is sort of -	0
it plus energy plus  timing  information is sort of -	0
I mean, if you look up in Rabiner and Schafer from like twenty-five years ago or something, that's sort of  what they were using then. So it's - it's not a -	0
So, yeah. It - it might be that what I did is -	0
low-energy, uh, speech frames. Because  the way I do it is I just - I just combine	0
the two decisions - so, the one from [MASK] and the one from the energy-based - with the - with the and  operator. So,	0
I only  keep the frames where the two agree  that it's speech.	0
So if the energy-based dropped - dropped low-energy speech,	0
mmm, they - they are - they are lost.	0
But s- still, the way it's done right now it - it helps on - on the noises where - it seems to help on the noises where	0
our [MASK] was not very  good.	0
Well, I guess - I mean, one could imagine	0
combining them in different ways. But - but,	0
I guess what you're saying is that the - [MASK] h	0
Yeah. But the way it's combined wi- is maybe done - Well,  yeah .	0
Well, you can imagine -	0
The way I use a an- a "AND" operator is -	0
So, it - I, uh -	0
The frames that are dropped by [MASK] are - are, uh, dropped,	0
even if the, um, __JARGON__ M_L_P __JARGON__ decides to keep them.	0
Right. And that might not be optimal, but - but - I mean, I guess in principle what you'd want to do is have a -	0
uh, a probability estimated by each one and - and put them together.	0
Something that - that I've used in the past is, um - when just looking at the energy, is to look at the derivative.	0
And you  make your decision when the derivative is increasing for  so many frames.	0
Then you say that's beginning of speech.	0
But, I'm - I'm trying to remember if that requires that you	0
keep some amount of speech in a buffer.	0
I guess it depends on how you do it. But  I mean, that's - that's been a useful thing.	0
Yeah. Well, every- everywhere has a delay associated with it. I mean, you still have to k- always keep a buffer,	0
then only make a decision because  you still need to smooth the  decision further.	0
Well, actually if I don't - maybe don't want to work too much of - on it right now. I just wanted to - to see if it's -	1
what I observed was the re- was caused by this - this [MASK] problem. And it seems to be the case.	1
Uh, the second thing is the - this [MASK] Um.	1
which I've just started yesterday to launch a bunch of, uh,	1
uh, values for the parameters that are used. So,	1
it's [MASK] which u	0
the noise spectra that  is estimated  on the noise portion of the s- uh, the utterances.	0
uh, [MASK]	0
And after subtraction, I also add  a constant noise, and I also try different,	0
noise, uh, values  and we'll see what happen.	0
But st- still when we look at the, um -	0
Well, it depends on the parameters that you use, but	0
for moderate [MASK] and moderate noise level that you add, you st- have a lot of musical noise.	0
On the other hand, when you  subtract more and when you add more noise, you get rid of this musical noise but  maybe you distort a lot of speech. So.	0
Well, it - until now, it doesn't seem to help. But	0
So the next thing, maybe I - what I will  try to - to do is just  to try to smooth	1
to smooth the d- the result of the subtraction, to get rid of the musical noise, using some kind of filter, or -	1
Can smooth [MASK] also.	1
Your filter is a function of [MASK] Hmm?	1
Yeah. So, to get something that's - would be closer to  what you tried to do with [MASK] And -	0
Uh. I don't know, it's - go ahead. And it's - go ahead.	0
It - Maybe you can - I think it's -	0
th- I've been playing with this [MASK] like. And	1
there are - there were some  bugs  in the program, so I was p- initially trying to clear them up.	1
Because one of the bug was - I was assuming that always [MASK] - uh, the initial frames were silence.	0
It always started in the silence state, but it wasn't for some utterances.	0
So the - it wasn't estimating the noise initially,	0
and then it  never  estimated, because I assumed that it was always	0
So this is on [MASK] Italian?	0
Yeah. [MASK] Italian.	0
So, in some cases s- there are also -	0
Yeah. There're a few cases, actually, which I found later, that there are.	0
So that was one of the  bugs that was there in estimating the noise.	0
And, uh, so once it was cleared, uh, I ran a few experiments with  different ways of smoothing the estimated clean speech and	1
how t- estimated the noise and, eh, smoothing [MASK] also.	1
And so the - the trend seems to be like,	1
smoothing the  current estimate of the clean speech for deriving the	1
[MASK] which is like  deriving	1
So we'll have, like, a few results where the -	0
the - More smoothing is helping. But still it's like - it's still comparable to the baseline. I haven't got anything  beyond  the baseline. But that's, like, not using any [MASK]	1
And, uh, so I'm - I'm trying a few more experiments with	0
different time constants for smoothing the noise spectrum, and smoothing the clean speech, and smoothing [MASK] So there are three time constants that I have. So, I'm just playing around.	0
So, one is fixed in the line, like  Smoothing the clean speech is - is helping, so I'm not going to change it that much. But, the way I'm estimating the noise	0
and the way I'm estimating [MASK] I'm just trying - trying a little bit.	0
So, that h- And the other thing is, like, putting a floor on the, uh, [MASK] because that - if	1
some - In some cases the clean speech is, like - when it's estimated, it goes to very low values, so [MASK] is, like, very low. And	1
so that actually creates a lot of variance in the low-energy region of the speech.	1
So, I'm thinking of, like, putting a floor also for [MASK] so that it doesn't  vary a lot in the low-energy regions.	0
And, uh. So. The results are, like -	0
So far I've been testing only with the  baseline, which is - which doesn't have any [MASK]	1
the contributions out. So it's just [MASK] plu	1
which is, uh, just the spectral - I mean, the mel sp-	1
And the other thing that I tried was - but I just	0
took of those, uh,   [MASK]	0
to see whether it really h- helps or not. I mean, it was just a - a run to see whether it really degrades or it helps. And	0
it's   - it seems to be like it's not	0
hurting a lot by just blindly picking up one filter which is nothing but a  four hertz -	0
a band-pass m- m- filter on the cubic root of the power spectrum.	0
So, that was the filter that Hy- uh, Carlos had.	0
Yeah. Just - just to see whether it really -	0
it's - it's - is it worth trying or not. So, it doesn't seems to be degrading a  lot  on that. So there must be something that I can -	0
that can be done with that type of noise compensation also, which -	0
I guess I would ask Carlos about that. I mean, how - how he derived those filters and -	0
and where d- if he  has  any filters which are derived on [MASK] stories, added with some type of noise which - what we are using currently, or	0
something like that. So maybe I'll -	0
This is cubic root of [MASK]	0
Yeah. Cubic root of power spectrum.	0
So, if you have this [MASK] you probably get n- you get negative values. Right?	0
Yeah. And I'm, like, floating it to z- zeros right now.	0
So it has, like - the spectrogram has, like - Uh, it actually,	0
uh, enhances the onset and offset of - I mean, the - the begin and the end of the speech.	0
So it's - there seems to be, like, deep  valleys  in the begin and the end of, like, high-energy regions, because the filter has, like, a sort of Mexican-hat type structure.	0
So, those are the regions where there are, like - when I look at the spectrogram, there are those deep valleys on the begin and the end of the speech.	0
But the rest of it seems to be, like, pretty nice.	0
That's  something I observe using that filter.	0
Yeah. There are a few - very - not a lot of - because the filter doesn't have a - really a deep negative portion,	0
so that it's not really creating a  lot  of negative values in the cubic root.	0
I'll s- may- continue with that for some w- I'll - I'll - Maybe I'll ask Carlos a little more about how to play with those filters,	0
and - but while  making this [MASK] better.	0
Yeah. That - that's it, Morgan.	0
Uh, last week you were also talking about building up [MASK]  stuff?	0
Yeah. I - I - I would actually m- m- didn't get enough time to work on the subspace last week. It was mostly about  finding those bugs and	0
th- you know, things, and I didn't work much on that.	0
Well, I am still working with, eh, [MASK] And, one of the things that last week,	1
eh, say here is that maybe the problem was with the diff because the signal have different level of energy.	1
And, maybe, talking with Stephane and with Sunil, we decide that maybe it was interesting to - to apply [MASK] before applying [MASK]	1
But then  we decided that that's - it doesn't work absolutely, because we modified also the noise.	1
we then - we decide that maybe is a good idea.  We don't know. I don't hav- I don't - this is - I didn't  do the experiment yet - to apply V_T_S	1
The other thing  is - So - so, in - i- i- and -	0
Not - and C_zero would be a different -	0
So you could do a different normalization for C_zero than for other things anyway.	0
I mean, the other thing I was gonna suggest is that you could have  two kinds of normalization with - with, uh, different time constants. So,	0
uh, you could do some normalization  s- uh, before [MASK] and then do some other normalization after.	0
I don't know. But - but [MASK] certainly acts differently than the others do, so that's -	0
Well,  we   s- decide to m- to - to obtain the new expression if we work in [MASK]	1
And - Well. I am working in that now, but  I'm not sure if that will be usefu- useful. I don't know. It's k- it's k-	1
It's quite a lot - It's a lot of work.  Well, it's not too much, but this - it's work. And I want to know if -	1
if we have some  feeling that  the result -	1
I - I would like to know if -	0
I don't have any feeling if this will work better than	1
apply [MASK] aft-	1
in filter [MASK]	1
I r- I'm not sure. I don't - I don't know absolutely nothing.	1
Yeah. Well, you're - I think you're the first one here to work with [MASK] so,	1
uh, maybe we could call someone else up who  has,   ask them their opinion. Uh, I don't - I don't have a good feeling for it.	1
Actually, [MASK] that you tested before was in the log domain and so  the codebook is	0
e- e- kind of dependent on the  level of the speech signal. And -	0
So I expect it - If - if you have something that's independent of this, I expect it to -	0
to, uh, be a better	0
You - you wouldn't even need to switch to [MASK] Right? I mean, you can just sort of normalize the -	0
No. We could normali- norm- I mean, remove the median.	0
And then you have   one  number which  is  very dependent on the level cuz it  is  the level,  and the other which isn't.	0
But here also we would have to be careful about removing the mean  of speech and	0
not of noise. Because it's like  first doing general normalization and then	0
noise removal, which is -	0
Yeah. We - I was thinking to - to -	0
to estimate the noise  with the first frames and then apply [MASK]	0
before [MASK]	0
We - we see -	0
about that and working about that, but I don't have result this week.	0
Sure. I mean, one of the things we've talked about - maybe it might be star- time to start thinking about pretty soon, is	0
as we look at the pros and cons of these different methods, how do they fit in with one another? Because  we've talked about	0
potentially doing some combination of a couple of them.	0
Maybe - maybe pretty soon we'll have some sense of what their  characteristics are, so we can see what	0
O_K. Why don't we read some digits?	0
O_K? Yep.  Want to go ahead, Morgan?	0
Transcript L_ dash two one five.	0
