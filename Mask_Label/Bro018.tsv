O_K. So uh, he's not here, so you get to -	1
Yeah, I will try to explain the thing that I did this - this week - during this week.	1
Well eh you know that I work - I begin to work with a new feature to detect [MASK]	1
What I trying two [MASK]	1
to - to the - with this new feature and the fifteen feature	1
The - [MASK]	0
No,  satly  the mes- [MASK] the new base system - the new base system.	0
Oh the - O_K, [MASK]  system.	0
Yeah, we - yeah [MASK] with the new filter,	0
[MASK] or something like that. And I'm trying tw	1
one that only have t- three output,  voice, unvoice, and silence, and	1
other one that have fifty-six output.	1
The probabilities  of the allophone . And	1
I tried to do some experiment of recognition with that	1
and only have result with - with [MASK] with the three output.	1
And I put together the fifteen features and the three [MASK] output.	0
And, well, the result are	1
li- a little bit better, but more or less similar.	1
Uh, I - I'm - I'm slightly confused. What - what feeds the uh - the three-output net?	0
No no, what  feeds  it? What  features  does it see?	0
The feature - the  input?	0
The inputs are the fifteen - the fifteen uh	0
the - with the new code.	0
And the other three features are [MASK] the variance of the difference between the two spectrum,	0
the variance of the auto-correlation function, except the - the first point, because half the height value is  [MASK]  and	0
the first coefficient of the auto-correlation function. That is like the energy	0
with these three feature, also these three feature.	0
You wouldn't do like [MASK] over [MASK] or something like that?	0
I mean usually for [MASK] you'd do - yeah, you'd do something - you'd do energy but then you have something like spectral slope, which is you get like [MASK] ov- over [MASK] or something like that.	0
I'm sorry I missed it.	0
Auto-correlation? Yes, yes, the variance of the auto-correlation function that uses that  @@	0
Well that's the  variance,  but if you just say "what is -" I mean, to first order,	0
um yeah one of the differences between voiced, unvoiced and silence is energy.	0
Another one is - but the other one is the spectral  shape.	0
Yeah, and so [MASK] over [MASK] is what you typically use for that.	0
No, I don't use that - I can't use -	0
No, I'm saying that's what people us- typically use.	0
See, because it - because this is - this is just like a single number to tell you	0
"does the spectrum look like  that  or does it look like  that ".	0
Oh. R_ - [MASK]	0
So if it's - if it's um - if it's low energy uh but the - but the spectrum looks	0
like  that  or like  that,	0
Uh but if it's low energy and the spectrum looks like  that,	0
So if you just - if you just had to pick	0
two features to determine [MASK]	0
you'd pick something about the spectrum like uh [MASK]	0
or i- i- you know you'd have some other energy measure and like in the old days people did like uh zero crossing	0
Well, I can also th- use this.	0
Bec- because the result are a  little  bit better but we have in a point that	0
everything is more or less the similar - more or less similar.	0
Right, but it seemed to me that what you were	0
what you were getting at before was that there is something about the difference between the original signal or the original [MASK] and with the filter which is what - and the variance was one take uh on it.	0
Yeah, I used this too.	0
Right. But it - it could be something else. Suppose you  didn't  have anything like that. Then in that case, if you have two nets,	0
Alright, and this one has three outputs,	0
and this one has f-	0
whatever, fifty-six, or something, if you were to sum up the probabilities for the voiced and for the unvoiced and for the silence  here,  we've found in the past you'll do  better  at [MASK] than you do with  this  one.	0
So just  having  the three output thing doesn't - doesn't really  buy  you anything.	0
The issue is what you  feed  it.	0
Yeah, I have - yeah.	0
So you're saying take the features that go into [MASK]  net and feed those into the  other  one,	0
as additional inputs, rather than having a separate -	0
W- well that's another way. That wasn't what I was  saying  but yeah that's certainly another thing to  do.  No I was just trying to say if you b- if you	0
bring this into the picture over this, what more does it buy you?	0
And what I was saying is that the only thing I think that it buys you is um	0
based on whether you  feed  it something different.	0
And something different in some fundamental  way.  And so the kind of thing that - that she was talking about before,	0
was looking at something uh ab- um -	0
uh about the difference between the - the uh um	0
[MASK] uh	0
and the log magnitude uh F_F_- spectrum uh and the um uh [MASK]	0
And so the filter bank	0
is  chosen  in fact to sort of integrate out the effects of pitch and she's saying you know trying - So the particular measure that she chose was the variance of this m- of this difference,	0
but that might not be the right number.	0
Right? I mean maybe there's something about the variance that's - that's not enough or maybe there's something else that - that one could  use,  but	0
I think that, for me, the thing that - that struck me was that uh you wanna get something back here, so here's -  here's  an idea.	0
uh What about it you skip all the - all the really  clever  things, and just fed the log magnitude  spectrum  into this?	0
You have the log magnitude spectrum,	0
and you were looking at that	0
and the difference between [MASK] and - and c- c- computing the variance.	0
That's a  clever  thing to do. What if you stopped being  clever?	0
And you just took  this  thing in here because it's a neural net and neural nets are wonderful and	0
figure out what they can - what they most need from things, and	0
I mean that's what they're  good  at.	0
So I mean you're - you're - you're trying to be  clever  and say what's the  statistic  that should - we should get about this difference but uh in fact,	0
you know maybe just feeding this in or -	0
or feeding  both  of them in	0
you know,  another  way, saying let  it  figure out what's the - what is the	0
interaction, especially if you do this over multiple frames?	0
Then you have this over time,	0
and - and both kinds of measures and uh you might get uh something better.	0
So - so don't uh -	0
don't do the  division,  but let the net	0
That's another thing you could do yeah. Yeah.	0
I mean, it seems to me, if you have exactly the right thing then it's better to do it without the net because otherwise you're asking the net to learn this - you know, say if you wanted to learn how to do multiplication.	0
I mean you could feed it a bunch of s- you could feed two numbers that you wanted to multiply into a net	0
and have a bunch of nonlinearities in the middle and train it to get the product of the output and it would work.	0
But, it's kind of  crazy,  cuz we know how to multiply and you - you'd be you know much lower error usually  if you just multiplied it out.	0
But suppose you don't really  know  what the right thing is. And that's what these sort of dumb machine learning methods are  good  at. So.	0
Um.  Anyway.  It's just a thought.	0
How long does it take, Carmen, to train up one of these nets?	0
Mmm, one day or less.	0
Yeah, it's probably worth it.	0
What are - what are your f- uh frame error rates for - for this?	0
uh no, the  frame  error rate? Fifty-six I think.	0
Is that - maybe that's accuracy?	0
Fif- fifty-six percent accurate for v- [MASK]	0
I don't remember for [MASK] maybe for the other one.	0
Yeah, [MASK]  hopefully  would be a lot  better.	0
for voiced. I don't reme-	0
Should be in  nineties  somewhere.	0
Better. Maybe for [MASK] This is for the other one. I should -	0
But I think that fifty-five was for the - when the output are the fifty-six phone.	0
That I look in the - with the other - nnn the other [MASK] that we have are more or less the same number.	0
Silence  will be better but more or less the same.	0
I think at the frame level for fifty-six that was the kind of number we were getting for - for uh um reduced band width uh	0
I think that - I - I - I think that for the other one, for the three output, is sixty- sixty-two, sixty-	0
three more or less. It's -	0
Yeah,  because it's  noise   also.	0
But even i- in -	0
Oh yeah, in  training.   Still,	0
Well  actually,  so this is a test that you should  do  then.	0
Um, if you're getting fifty-six percent over here, uh that's in noise  also,  right?	0
If you're getting fifty-six  here,	0
try adding together the probabilities of all of [MASK] here and all	0
and see what you get  then.	0
I bet you get better than sixty- three.	0
Well I don't know, but -	0
I th- I - I think that we - I have the result more or less. Maybe. I don't know.	0
I don't - I'm not sure but	0
I remember  @@  that I can't show that.	0
O_K, but that's a -	0
That is a - a good check point, you should do that anyway, O_K?	0
Given this - this uh	0
regular old net that's just for choosing for other purposes,	0
uh add up the probabilities of the different subclasses and see - see how well you do.	0
Uh and that - you know anything that you do over here should be at  least  as good as that.	0
The targets for the neural net,	0
uh, they come from forced alignments?	0
[MASK] canonical ma- mappings.	0
Oh.  So, this is trained on  [MASK]	0
Yeah,  noisy  [MASK]	0
Noisy [MASK] We have noisy [MASK] with the noise of the - the	0
[MASK] And now we have another noisy [MASK] also with the noise of uh Italian database.	0
Well there's gonna be - it looks like there's gonna be a noisy uh -	0
some large vocabulary noisy stuff too. Somebody's preparing.	0
I forget what it'll be, resource management, Wall Street Journal, something. Some - some read task actually, that they're -	0
Yeah, so the uh -	0
the issue is whether people make a decision  now  based on what they've already seen, or they make it  later.  And one of the arguments for making it  later  is let's make sure	0
that whatever techniques that we're using work for something more than - than connected digits.	0
When are they planning -	0
When would they  do  that?	0
Mmm, I think late - uh I think in the summer sometime.	0
This is the work that I did during this date and also mmm	1
I - H- Hynek last week say that if I have time I can to begin to - to study	1
well  seriously [MASK] proposal to look at the code and something like that	1
to know exactly what they are doing because maybe that we can have some ideas	0
but not only to read the proposal. Look insi- look i-	0
carefully what they are doing with the program  @@  and I begin to - to work also in that.	1
But the first thing that I don't understand is that they	1
the uh log energy that this quite - I don't know why they have some	1
constant in the expression of the lower energy. I don't know what that means.	1
They have a  constant  in there, you said?	0
at the front it says uh "log energy is equal to the rounded version of sixteen over the log of two"	0
uh times the - Well, this is  natural  log, and maybe it has something to do with the fact that this is -	0
Then maybe I can understand.	0
Is that some kind of  base  conversion, or - ?	0
I - I have no idea.	0
Yeah, that's what I was  thinking,  but - but um,	0
Because maybe they're - the threshold that they are using  on  the basis of this value - I don't know exactly, because well th- I thought maybe they have a meaning. But I don't know what is the meaning of  take  exactly this value.	0
Yeah, it's pretty funny looking.	0
So they're taking the number inside the log and raising it to	0
Does it have to do with those sixty-fours, or - ?	0
If we  ignore  the  sixteen,	0
the natural log of t-  one  over the natural log of  two  times the natu-	0
Well , maybe somebody'll think of something, but this is uh -	0
It may just be that they - they want to have - for very small energies, they want to have some	0
The effect I don't -	0
@@  I can understand the effect of this, no? because it's to -	0
to do something like that.	0
Well, it says, since you're taking a natural log, it says that when - when you get down to essentially zero energy,	0
this is gonna be the natural log of  one,  which is  zero.	0
So it'll go down to	0
the natural log being -	0
So the lowest value for this would be zero. So y- you're restricted to being positive.	0
And this sort of smooths it for very small energies.	0
Uh, why they chose sixty- four  and something else, that was probably just experimental.	0
And the - the - the constant in front of it, I have no idea.	0
Well. I - I will look to try if I move this parameter in their code what happens,  maybe everything is -	0
Maybe they  tres hole are  on basis of this.	0
they - they probably have some fi- particular	0
s- fixed point arithmetic that they're using, and then it just -	0
Yeah, I was just gonna say maybe it has something to do with hardware, something they were doing.	0
I mean that - they're s- probably working with fixed point or integer or something. I think you're supposed to on this stuff anyway, and -	0
and so maybe that puts it in the right realm  somewhere.	0
Well it just, yeah, puts it in the right range, or -	0
I think, given at the level you're doing things in floating point on the computer, I don't think it matters, would be my guess, but.	0
I - this more or less  anything	0
O_K, and wh- when did Stephane take off? He took off -	1
I think that Stephane will arrive today or tomorrow.	1
Oh, he was gone these first few days, and then he's here for a couple days before he goes to Salt Lake City. O_K.	0
He's - I think that he is in Las Vegas or something like that.	0
So he's - he's going to I_CASSP which is good. I - I don't know if there are many people	1
who are going to I_CASSP	0
so - so I thought, make sure  somebody  go.	0
Have people sort of stopped going to I_CASSP in recent years?	0
people are less consistent about going to I_CASSP and I think it's still -	0
it's still a reasonable forum	0
for students to - to present things.	0
I think for engineering students of any  kind,  I think it's - it's if you haven't been there much, it's good to go to,	0
uh to get a  feel  for things, a range of things, not just  speech.  Uh.	0
But I think for - for sort of dyed-in-the-wool speech people, um I think that I_C_S_L_P and [MASK] are much more targeted.	0
Uh. And then there's these other meetings, like [MASK] and - and uh	0
so there's - there's actually plenty of meetings that are really relevant to -	0
computational uh speech processing of one sort or another.	0
So. I mean,  I  mostly just ignored it because I was too busy and  didn't get to it.	0
Wanna talk a little bit about what we were talking about this morning? Just briefly, or	1
Yeah.  So. I - I guess	0
progress, I - I've been getting a - getting my committee members for the quals.	0
And um so far I have Morgan and Hynek,	0
and I asked John Ohala and he agreed.	0
So I'm - I - I just need to ask um	0
Then uh I talked a little bit about	1
um continuing with these dynamic ev- um acoustic events,	1
we're - we're - we're	1
thinking about a way to test the completeness of	1
a - a set of um dynamic uh events.	1
Uh, completeness in the - in the sense that	1
um if we - if we pick these X_ number of acoustic events,	1
do they provide sufficient coverage	1
for the phones that we're trying to recognize	1
or - or the f- the words that we're gonna try to recognize later on.	1
And so Morgan and I were uh discussing	1
a form of a cheating experiment	1
a chosen set of features, or acoustic events,	1
and we train up a hybrid	1
um system to do  phone  recognition on  [MASK]	1
So i- i- the idea is if we get good phone recognition results,	0
using um these set of acoustic events,	0
um that - that says that these acoustic events are g- sufficient to cover	0
a set of phones, at least found in  [MASK]	0
Um so i- it would be a -	0
"are we on the right track with - with the -	0
the choices of our acoustic events".	0
So that's going on. And	0
uh final project for Jordan's class, uh which is -	0
Actually, let me - Hold that thought. Let me back up while we're still on it. The - the other thing I was suggesting, though, is that given that you're talking about binary features,	0
uh, maybe the first thing to do is just to  count	0
and uh count co-occurrences and get probabilities for a discrete [MASK]	0
cuz that'd be pretty simple because it's just - Say, if you had ten - ten events,	0
uh that you were  counting,  uh each frame would only have a thousand possible  values  for these ten bits,	0
and uh so you could make a table that would - say, if you had thirty-nine phone categories, that would be a thousand by thirty-nine, and just count the co-occurrences and divide them by the - the uh - uh uh occ- uh	0
count the co-occurrences between the event and the phone and divide them by the number of occurrences of the phone, and that would give you the likelihood of the - of the event given the phone. And um then just use that in a very simple [MASK] and uh	0
you could uh do phone recognition then and uh wouldn't have any of the issues of the uh training of the net or - I mean, it'd be on the  simple  side, but	0
you know, if - uh uh the example I was giving was that if - if you had um	0
onset of voicing and - and end of voicing as being two kinds of events,	0
then if you had those a- all marked correctly, and you counted co-occurrences, you should get it completely right.	0
But you'd get all the  other  distinctions, you know, randomly  wrong.  I mean there'd be  nothing  to  tell  you that.	0
If you just do this by counting, then you should be able to find out in a pretty straightforward way whether you have a sufficient uh set of events to - to do the kind of level of -  of uh classification of phones that you'd like.	0
So that was - that was the idea. And then the other thing that we were discussing was - was um	0
O_K, how do you get the - your  training  data.	0
Cuz uh [MASK] uh uh you know was	0
half a dozen people, or so working off and on over a couple years, and	0
uh similar -  similar amount of data  to what you're talking about with [MASK] training. So,	0
it seems to me that the only reasonable starting point is	0
uh to automatically translate the uh	0
current [MASK] markings into the markings you want.	0
it won't have the kind of characteristic that you'd like, of catching funny kind of things that maybe aren't	0
there from these automatic markings, but - but uh	0
It's probably a good place to start.	0
Yeah and a short - short amount of time, just to - again, just to see if that information is sufficient	0
to uh determine the phones.	0
Yeah, you could even then -	0
to get an idea about	0
how different it is, you could maybe take some subset	0
you know, go through a few sentences, mark  them   by hand and then see how different it is	0
you know, the canonical ones, just to get an idea - a rough idea of	0
h- if it really even makes a  difference.	0
You can get a little feeling for it that way, yeah that is probably right.	0
I mean uh my - my guess would be that this is - since [MASK] read speech that this would be less of a big deal, if you went and looked at spontaneous speech it'd be more - more of one.	0
And the other thing would be, say, if you had these ten events, you'd wanna see, well what if you took	0
two events or four events or ten events or t- and you know, and -	0
and hopefully there should be some point at which	0
having more information doesn't tell you	0
really all that much more about what the phones are.	0
other  events as being  sequences  of  these  events	0
Uh, you  could,  but the thing is, what he's talking about here is a uh - a translation to a per-frame feature vector,	0
so there's no  sequence  in that, I  think.  I think it's just a -	0
Unless you did like a second pass over it or something after you've got your -	0
Yeah, but we're just talking about something simple here,	0
Yeah. Just - You know. The idea is with a - with a very simple statistical structure, could you - could you uh at least verify that you've chosen features that	0
O_K, and you were saying something - starting to say something else about your - your class project, or - ?	0
So for my class project I'm	0
tinkering with uh support vector machines? something that we learned in class, and uh um basically just another method for doing classification.	0
And so I'm gonna apply that to	0
um compare it with the results by um King and Taylor who did	0
um using recurrent neural nets,	0
a set of phonological features	0
and made a mapping from the [MASK] to these phonological features, so I'm gonna	0
do a similar thing with -	0
with support vector machines and see if -	0
So what's the advantage of support vector machines? What -	0
Um. So, support vector machines are - are good with dealing with a less amount of data	0
and um so if you - if you give it less data it still does a reasonable job	0
in learning the - the patterns.	0
I guess it - yeah, they're sort of succinct,	0
Does there some kind of a distance metric that they use or how do they -	0
for cla- what do they do for classification?	0
the - the  simple  idea behind a support vector machine is	0
you have this feature space, right? and then it finds the optimal separating plane,	0
um between these two different um classes,	0
what it - i- at the end of the  day,  what it  actually  does is	0
examples of the features that are closest to the separating boundary, and remembers those	0
and uses them to recreate the boundary for the test set.	0
um these features, or - or these - these  examples,	0
which they call support f- support vectors,	0
if the  new  example falls	0
um  away  from the boundary in one direction then it's classified as being a part of this particular class	0
and otherwise it's the other class.	0
So why save the examples? Why not just save what the	0
Yeah, that's a good question. I - yeah.	0
That's another way of doing it.	0
Right? So - so it - I mean I - I guess it's -	0
You know, it - it goes back to [MASK]  sort of thing, right? Um,	0
i- i- if - is it eh w-	0
When is [MASK] good? Well, [MASK] good - is good if you have lots and lots of examples.	0
Um but of course if you have lots and lots of examples, then it can take a while to - to  use  [MASK] There's lots of look ups.	0
So a long  time  ago people talked about things where you would have	0
uh a  condensed  [MASK] where you would - you would - you would pick out uh some representative examples which would uh be sufficient to represent - to - to correctly classify everything that came in.	0
I - I think s- I think support vector stuff sort of goes back to -	0
to  that  kind of thing.	0
So rather than doing [MASK] where you compare to every single one,	0
you just pick a few	0
neural net approach uh or  Gaussian   mixtures for that matter are sort of - fairly brute force kinds of things, where you sort of -	0
you predefine that there is this big bunch of parameters and then you - you place them as you best can to define the boundaries, and in fact, as you know,	0
these things  do  take a lot of parameters and - and uh	0
if you have uh only a modest amount of data, you have trouble	0
so I - I  guess  the idea to this is that it - it is reputed to uh be somewhat better in that regard.	0
Right. I- it can be a - a reduced um  parameterization of - of the - the model by just keeping  certain selected examples.	0
But I don't know if people have done sort of careful	0
comparisons of this on large tasks or anything.	0
Maybe - maybe they have.	0
Yeah, I don't know either.	0
S- do you get some kind of	0
number between zero and one at the output?	0
Actually you don't get a - you don't get a nice number between zero and one. You get - you get either a zero  or  a  one.	0
uh there are - there are pap-	0
you - you get a distance measure	0
at the end of the day,	0
and then that distance measure is - is um -	0
is translated to a zero or one.	0
But that's looking at it for - for classification - for binary classification, right?	0
And you get that for each class, you get a zero or a one.	0
But you have the distances to work with.	0
You have the distances to work with, yeah.	0
Cuz actually Mississippi State people did use support vector machines for uh uh speech recognition and they were using it to estimate probabilities.	0
they had a - had a way to translate the distances into - into probabilities with the - with the simple	0
uh [MASK]	0
Yeah, and d- did they use  [MASK]	0
Yeah, there's some - there's like one over one plus the exponential or something like that.	0
Oh, so it  is  [MASK]	0
Did the - did they get	0
I mean , they're O_K, I - I don't - I don't think they were earth - [MASK] but I think that	0
this was a couple years ago, I remember them doing it at some meeting, and - and um	0
I don't think people were very critical because it was interesting just to -	0
you know, it was the first time they  tried  it, so -	0
so the - you know, the numbers were not	0
incredibly good but there's you know,	0
I - I don't remember anymore.	0
I don't even remember what the  task  was,	0
it  was Broadcast News, or	0
Uh s- So Barry, if you just have zero and ones, how are you doing the speech recognition?	0
Oh I'm not do- I'm not planning on doing speech recognition with it. I'm just doing	0
this - this uh feature set called the uh sound patterns of English	0
um is just a bunch of	0
Let's say, is this voicing, or is this not voicing, is this	0
[MASK] not [MASK] and	0
Did you find any more mistakes in their tables?	0
Oh! Uh I haven't gone through the entire table,  yet. Yeah, yesterday I brought Chuck	0
the table and I was like, "wait, this - is -	0
Is the mapping from N_ to - to this phonological feature called um "coronal" ,	0
is - is - should it be - shouldn't it be a one? or should it - should it be you know [MASK] instead of not coronal as it was labeled in the paper?"	0
So I ha- haven't hunted down all the - all the mistakes yet, but -	0
But a- as I was saying, people  do  get probabilities from these things, and - and uh	0
we were just trying to remember how they  do,  but people  have  used it for speech recognition, and they  have  gotten probabilities. So they have some conversion from these distances to	0
There's - you have - you have the paper, right? The Mississippi State paper?	0
Yeah, if you're interested y- you could look, yeah.	0
Yeah, I can - I can show you - I - yeah,  our  -	0
So in  your  - in - in the thing that  you're  doing, uh	0
you have a vector of ones and zeros for each phone?	0
Uh, is this the class project, or - ?	0
Is that what you're -	0
Right, right f- so for every phone there is - there is a um - a vector of ones and zeros	0
f- uh corresponding to whether it exhibits a particular phonological feature or not.	0
And so when you do your wh- I'm - what is the task for the class project? To	0
come up with the phones? or to come up with these vectors to see how closely they match the phones, or - ?	0
Right, um to come up with a mapping from um [MASK] or s- some feature set,	0
to whether there's existence of a particular phonological feature.	0
And um yeah, basically it's to learn a mapping	0
from [MASK] to	0
Is it - did that answer your question?	0
I'm not sure what you - what you're - what you get out of your system. Do you get out a	0
a vector of these ones and zeros and then try to find the closest matching phoneme to that vector, or - ?	0
Oh. No, no. I'm not - I'm not planning to do any - any phoneme mapping  yet.  Just -	0
it's - it's basically - it's - it's really simple, basically a detection	0
So King and - and Taylor	0
um did this with uh recurrent neural nets, and this i- their - their idea was to first find	0
a mapping from [MASK] to	0
uh phonological features and then later on, once you have these	0
then uh map that to phones. So I'm - I'm sort of reproducing phase one of their stuff.	0
So they had one recurrent net for each particular feature?	0
I wo- did they compare that - I mean, what if you just did phone recognition and did the  reverse  lookup.	0
So you recognize a  phone  and which ever phone was recognized, you spit out it's vector of ones and zeros.	0
I expect you could do that. That's probably not what he's going to do on his class project.	0
So um have you had a chance to do	1
this um thing we talked about yet with the uh -	1
Uh. No actually I was going a different - That's a good question, too, but I was gonna ask about the -	1
changes to the data in comparing [MASK]	1
for [MASK]	1
Well what I've been -	0
"Changes to the data", I'm not sure I -	0
So we talked on the phone about this, that -	1
that there was still a difference of a -	1
you told me that there was a difference in how the normalization was done.	1
And I was asking if you were going to do -	1
uh for [MASK] with the normalization done as it had been done	1
right, no I haven't had a chance to do that.	1
What I've been  doing  is	1
trying to figure out - it just seems to me like there's a um -	1
well it seems like there's a bug,	1
but it's big enough that it - it seems wrong.	1
Yeah, I  agree,  but I thought that the normalization difference was one of the	1
Yeah, but I don't - I'm not -	0
Yeah, I guess I don't	1
think that the normalization difference is gonna account for everything.	1
So what I was working on is um	1
just going through and checking the headers of the wavefiles, to see if	1
maybe there was a um -	0
a certain type of compression or something that was done that my script wasn't catching. So that for some subset of	0
uh the - the -	0
the features I was computing were junk.	0
cause it to perform O_K, but uh,	0
you know, the - the models would be all messed up. So I was going through and just double-checking that kind of think first,	0
there was just some kind of obvious bug in the way that I was computing the features.	0
Looking at all the sampling rates to make sure all the sampling rates were what -	0
eight K_, what I was assuming they were, um -	0
Yeah,  that  makes  sense,  to check all that.	0
Yeah. So I was doing that  first,  before I did these other things, just to make sure there wasn't something -	0
uh, a couple three percent	0
uh difference in word error rate	0
from some difference in normalization, I would think.	0
Yeah, and I think, hhh -  I'm trying to remember but I think I recall that Andreas was saying that he was gonna run	0
sort of the  reverse  experiment.	0
normalization that  we  did but with the mel  cepstral  features.	0
back up from the system that  he  had.	0
I  thought  he said he was gonna - I have to look back through my - my email from him.	0
Yeah, he's probably off at - at uh his	0
But yeah the - I sh- think they should be	0
the Cambridge folk found [MASK] actually to be a little better.	0
I mean the  other  thing I wonder about was whether there was something just in the -	0
but maybe not, since they -	0
Yeah see one thing that's a little bit um -	0
I was looking - I've been studying and going through the logs for the system that um Andreas created.	0
his uh - the way that the -	0
[MASK]  system looks like it  works  is that it reads the wavefiles  directly,	0
and does all of the cepstral computation stuff on the fly.	0
so there's no place where these -	0
where [MASK] files are stored, anywhere that I can go look at and compare	0
to [MASK] ones, so	0
whereas with  our  features, he's actually storing	0
[MASK] on disk, and he reads  those  in. But it looked like he had to give it -	0
uh even though [MASK] is already computed, he has to give it	0
a front-end parameter file. Which talks about the kind of	0
com- computation that his [MASK]  thing does, so	0
I - I don't know if that - it  probably  doesn't mess it up, it probably just  ignores  it if it determines that it's already	0
in the right format or something but -	0
the - the - the two processes that happen are a little different.	0
So anyway, there's stuff there to sort out.	0
So, O_K. Let's go back to what you  thought  I was asking you.	0
Yeah no and I didn't have a chance to do that.	0
Oh! You had the sa- same answer anyway.	0
Yeah. I've been um, -	0
I've been working with um	0
on his project and then I've been trying to track down this bug	0
in uh the ICSI front-end features.	0
So one thing that I  did  notice, yesterday I was studying the um -	0
the uh [MASK]	0
and it looks like we don't have any way to um	0
control the frequency range that we use in our analysis. We basically - it looks to me like we do [MASK] um and then we just take all the bins	0
We don't have any set of parameters where we can say	0
you know, "only process from	0
you know a hundred and ten hertz to thirty-seven-fifty".	0
At least I couldn't see any kind of control for that.	0
Yeah, I don't think it's in  there,  I think it's in the uh uh	0
So, [MASK]  is on everything, but the  filters	0
um, for instance, ignore the - the lowest	0
And what it does is it - it copies	0
The - the filters? Which filters?	0
The  filter  bank which is created by integrating over [MASK]  bins.	0
When you get the mel -	0
When you go to [MASK]	0
Yeah, it's bark scale, and it's - it -	0
filters over to the first. So the first filters are always - and you can s- you can specify a different number of	0
uh  features  - different number of  filters,  I think,	0
So you can specify a different number of filters, and whatever	0
uh you  specify,  the  last  ones are gonna be  ignored.  So that - that's a way that you sort of	0
change what the - what the bandwidth is.	0
Y- you can't do it without I think changing the number of filters, but -	0
I saw something about uh -	0
that looked like it was doing something like that, but I didn't quite understand it.	0
Yeah, so the idea is that the very lowest frequencies and - and typically the veriest  highest frequencies are kind of junk.	0
And so um you just - for continuity you just approximate them by -	0
by the second to highest and second to lowest.	0
It's just a simple thing we put in.	0
And - and so if you h-	0
But - so the - but that's a fixed uh thing? There's nothing that lets you -	0
I think that's a  fixed  thing. But	0
see - see my point? If you had -	0
If you had  ten  filters,	0
then you would be throwing away a  lot  at the two ends.	0
And if you had -	0
fifty  filters, you'd be throwing away hardly  anything.	0
Um, I don't remember there being an independent way of saying "we're just gonna	0
make them from here to here".	0
Use  this  analysis bandwidth or something.	0
But I - I - I don't know, it's actually been awhile since I've looked at it.	0
Yeah, I went through [MASK] and then	0
you know just calling [MASK] libs  and thing like that. And I didn't - I couldn't see any wh- place where that kind of thing was done.	0
I didn't quite understand  everything  that I saw, so -	0
Yeah, see I don't know  Feacalc  at all.	0
But it calls [MASK] with some options, and	0
But I - I think in -	0
I don't know. I guess for some particular database you might find that you could tune that and tweak that to get that a little better, but I think that	0
that critical. I mean there's -	0
You can throw away stuff below a hundred hertz or so and it's just	0
Another  thing I was thinking about was um is there a -	0
I was wondering if there's maybe um	0
certain settings of the parameters when you compute [MASK] which would basically cause it to output	0
So that, in effect, what I could  do  is use our code but produce [MASK]	0
and compare that directly to -	0
you can definitely change the - [MASK] from being uh	0
a uh trapezoidal integration to a - a - a triangular one,	0
which is what the typical mel - [MASK] uh	0
And some people have claimed that they got some better performance doing that, so you certainly could do that	0
But the fundamental difference, I mean, there's other small differences -	0
There's a cubic root that happens, right?	0
Yeah, but, you know, as opposed to the log in the other case. I mean	0
the fundamental d- d- difference that we've seen any	0
kind of difference from  before,  which is  actually  an  advantage  for the P_L_ P  i- uh, I  think,  is that the - the smoothing at the end is auto-regressive instead of being cepstral -	0
uh,  from [MASK]	0
So um it's a little more noise robust.	0
Um, and that's - that's why when	0
people started getting databases that had a little more noise in it, like -	0
Broadcast News and so on, that's why c- Cambridge switched to [MASK] I think.	0
That's a difference that I don't	0
think  we put any way to get around, since it was an  advantage.	0
uh but we  did  -	0
eh we  did  hear this comment	0
from people at  some  point,	0
uh they got some better results with the triangular filters rather than the trapezoidal. So that  is  an option	0
Uh and you can certainly	0
But I think you're  probably  doing the right thing to look for  bugs   first.	0
Yeah just - it just seems like this kind of behavior could be caused by	0
some of the training data being messed up.	0
You know, you're sort of getting most of the way there, but there's a -	0
So I started going through and looking -	0
One  of the things that I  did   notice  was that the um	0
log likelihoods coming out of the log recognizer	0
from [MASK] data	0
than for [MASK] stuff,	0
average amount of  pruning  that was happening	0
was therefore a little bit  higher	0
for [MASK] features.	0
So, since he used the same exact  [MASK] for both,	0
I was wondering if it could be that we're getting more  pruning.	0
He  used  [MASK] even though the s- the  range  of p- of the likeli-	0
That's a pretty good  point right  there.   Yeah.	0
I would  think  that you might wanna do something like uh	0
you know, look at a few points to see where you are starting to get significant  search  errors.	0
Well, what I was gonna do is I was gonna take	0
um a couple of the utterances that  he  had run through,	0
run them through again but modify [MASK] and see if it	0
But I mean you could - uh if -	0
if - if that looks  promising  you could, you know, r- uh run	0
with a - with a few different uh [MASK] for both,	0
and  presumably  he's running at some pruning threshold that's -	0
that's uh, you know -	0
very few search errors but is - is relatively fast and -	0
Right. I mean, yeah, generally in these things you -	0
you turn back pruning really far, so	0
I - I didn't think it would be that big a deal because I was figuring well you have it turned back so far that	0
But you may be in the wrong  range  for [MASK]  features for some  reason.	0
And the uh the - the run time of the recognizer on [MASK] features is longer	0
which sort of implies that the networks are bushier,	0
you know, there's more things it's considering	0
which goes along with the fact that the matches aren't as good.	0
uh, you know, it could be that we're just pruning	0
Yeah, maybe just be different kind of distributions and - and yeah so that's another	0
They - they should - really shouldn't - There's no particular reason why they would be	0
exactly -  behave  exactly the  same.	0
There's lots of little differences. So.	0
Trying to track it down.	0
I guess this was a  little  bit off topic, I guess, because I was - I was thinking in terms of th- this as being a - a - a - a core	0
item that once we - once we had it going we would use for a number of the front-end things also.	0
as far as  my  stuff goes,	1
What's - what's on -	0
tried this [MASK] Um. Due to Avendano,	1
six seconds of speech, um	1
[MASK] analysis frames,	1
stepped by a half second	1
so it's a quarter length step and I -	0
I take  that  frame and four f- the four - I take - Sorry, I take the current frame and the four past frames and the	0
four  future  frames and that adds up to six seconds of speech.	0
of the log magnitude spectrum  over that [MASK]	1
I use that to normalize the s- the current center frame	1
by [MASK]	1
And I then - then I move to the next frame and I -	0
Well, actually I calculate all the means first and then I do the subtraction.	0
the - I tried that with [MASK]	1
it - it helped um in a phony reverberation case	1
where I just used the simulated impulse response	1
the error rate went from something like eighty	1
it was from something like eighteen percent	1
And on meeting rec- recorder far mike digits, mike -	1
on channel F_, it went from um	1
forty-one percent error to eight percent error.	1
On - on the real data, not with artificial reverb?	0
And that - that was um	0
trained on clean speech only, which I'm guessing is the reason why the baseline was so bad.	0
actually a little  side  point is I  think  that's the first results that we have	0
on the far field uh -	0
on - on the far field  data	0
uh for - recorded in - in  meetings.	0
Adam ran [MASK] recognizer.	0
On the  near  field, on the ne-	0
On the  far  field  also.  He did one [MASK] channel and	0
Oh! I didn't  recall  that.	0
What kind of numbers was  he  getting with that?	0
I'm not  sure,  I think it was about five percent error for [MASK] channel.	0
So why were you getting forty- one  here? Is this -	0
I - I'm g- I'm guessing it was the - the training data.	0
Uh, clean [MASK] is, like,	0
training data, and if they trained	0
the S_R_I system on this	0
T_V broadcast type stuff, I think it's a much wider range of channels and it -	0
No, but  wait  a minute. I -	0
I - I th -	0
What am I saying here? Yeah, so that was [MASK]	0
Maybe  you're  right.  Yeah. Cuz it was getting like one percent -	0
So it's  still   this   kind of ratio. It was - it was getting  one  percent or something on the  near  field.  Wasn't  it?	0
Yeah. Yeah. I think it was getting around one percent for the near - for the n-	0
it wa a- it was around one. Yeah.	0
So it was like one to five - So it's  still  this kind of ratio. It's just -	0
yeah, it's a lot more training data.	0
So probably it should be something we should try then is to - is to see if -	0
is  at some point just to take -	0
i- to transform the data and then -	0
and then uh use th- use it for [MASK]	0
b- You me- you mean um ta-	0
So you're - so you have a system which for one reason or another is relatively poor,	0
and - and uh you have something like forty-one percent error	0
uh and then you transform it to eight by doing - doing this - this work.	0
So here's this  other  system, which is a lot  better,	0
but there's  still  this kind of  ratio.  It's something like  five  percent error	0
with the - the distant mike, and one percent with the  close  mike.	0
how close to that  one  can you get	0
if you transform the data using  that  system.	0
r- Right, so - so I guess this [MASK] is trained on a lot of s- Broadcast News or Switchboard data.	0
Is that right? Do you know which one it is?	0
It's trained on a lot of different things. Um.	0
uh a lot of Switchboard, Call Home,	0
a bunch of different sources, some digits, there's some digits training in there.	0
O- one thing I'm wondering about is what this mean subtraction method	0
um will do if it's faced with additive noise.	0
Cuz I - I - it's cuz I don't know what log magnitude spectral subtraction is gonna do	0
Yeah, well, it's - it's not exactly the right thing but	0
That's - that's the -	0
but you've already  seen  that cuz there  is  added noise here.	0
That's - that's - Yeah, that's true.	0
O_K, so it's  then - then it's - it's -	0
it's reasonable to expect it would be helpful if we used it with [MASK] system and	0
Yeah, I mean,  as  helpful - I mean,	0
Yeah, w- we're often  asked  this when we work with a system that - that isn't - isn't sort of industry - industry standard great,	0
uh and we see some reduction in error using some clever method, then, you know, will it work on a -	0
on a - on a  good  system.	0
you know,  this other one's  - it was a pretty good system. I think,	0
you know, one - one percent	0
word error rate on digits is - uh digit strings is not	0
uh you know  stellar,  but -	0
but given that this is  real	0
digits, as opposed to uh sort of  laboratory  -	0
And it wasn't  trained  on this task either.	0
And it wasn't trained on this task. Actually one percent is sort of -	0
you know, sort of in a reasonable range. People would say "yeah, I could - I can imagine getting that".	0
And uh so the - the four or  five  percent or something is - is - is quite  poor.	0
Uh, you know, if you're doing a uh -	0
a sixteen digit uh credit card number you'll	0
basically get it  wrong  almost all the time.	0
um a significant reduction in the error for  that  would be  great.	0
And - and then, uh  Yeah.  So.	0
I actually have to  run.	0
So I don't think I can do the digits, but	0
I guess I'll leave my microphone on?	0
Actually,  I could just go  first,  come to think of it.	0
Then I can be out of here  quickly.	0
That's alright. I just have to run for another appointment.	0
O_K, did I t- Yeah. I left it on.	0
