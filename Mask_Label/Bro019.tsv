Uh, is it the twenty-fourth? Yeah.	0
Uh Chuck, is the mike type wireless -	0
Yeah. We uh - we abandoned the lapel because they sort of were not too - not too hot, not too cold, they were - you know, they were	0
uh, far enough away that you got more background noise, uh, and uh - and so forth but they weren't so close that they got quite the - you know, the really good - No, th-	0
they - I mean they didn't -	0
Wait a minute. I'm saying that wrong.	0
They were not so far away that they were really good representative distant mikes,	0
but on the other hand they were not so close that they got rid of all the interference. So it was no - didn't seem to be a good point to them.	0
On the other hand if you only had to have one mike in some ways you could argue the lapel was a good choice, precisely  because  it's in the middle.	0
There's uh, some kinds of junk that you get with these things that you  don't  get with the lapel uh, little mouth clicks and	0
breaths and so forth are worse with these than with the lapel, but	0
given the choice we - there seemed to be very strong opinions for uh, getting rid of lapels.	0
The mike number is -	0
Uh, your mike number's written on the back of that unit there.	0
And then the channel number's usually one less than that.	0
It- it's one less than what's written on the back of your - yeah.	0
So you should be  zero,  actually.	0
For your uh, channel number.	0
And you should do a lot of talking so we get a lot more of your pronunciations.	0
no, they don't - don't have a - have any Indian pronunciations.	0
So what we usually do is um, we typically will have our meetings and then at the end of the meetings we'll read the digits. Everybody goes around and reads the digits on the - the bottom of their forms.	0
Yeah. We're - This is session R_nineteen.	0
O_ K.  Do we have anything like an agenda? What's going on? Um.	0
Sunil's here for the summer?	0
Sunil's here for the summer, right.	1
Um, so, one thing is to talk about a kick off meeting	0
uh, and then just uh, I guess	1
uh, progress reports individually, and then uh, plans	1
for where we go between now and then, pretty much.	1
I could say a few words about um, some of the uh, compute stuff that's happening around here, so that people in the group know.	1
Why don't you  start  with that? That's sort of - Yeah?	0
We um - So we just put in an order for	1
about twelve new machines, uh, to use as sort of a compute farm.	1
uh, we ordered uh, [MASK] and um,	0
I'm not sure exactly how long it'll take for those to come in, but, uh, in addition, we're running -	0
So the plan for  using  these is, uh, we're running [MASK] here and Andreas has sort of gotten that all	1
uh, fixed up and up to speed.	1
And he's got a number of little utilities that make it very easy to um,	1
run things using P_make and Customs. You don't actually have to	1
write [MASK] scripts and things like that.	0
The  simplest  thing - And I can send an email around or, maybe I should do an F_A_Q on the web site about it or something.	1
How about an email that  points  to the F_A_Q,  you know what I'm saying? so that you can -   Yeah.	1
Uh, there's a command, uh, that you can use called " run  command". "Run dash command", "run hyphen command".	0
And, if you say that and then some job that you want to execute,	1
uh, it will find the fastest currently available machine,	1
and export your job to that machine,	1
and uh - and run it there and it'll duplicate your environment. So	0
you can try this as a simple test with uh, [MASK]  command. So you can say "run dash command L_ S ",	0
and, um, it'll actually export that  L_S command to some machine in the institute, and um, [MASK] on your current directory.	0
So, substitute [MASK] for whatever command you want to run, and um - And that's a simple way to get started using - using this.	0
And, so,  soon,  when we get all the new machines up,	1
um, e- then we'll have lots more compute to use. Now th- one of the nice things is that uh, each machine that's part of the	1
[MASK] and Customs network has attributes associated with it. Uh, attributes like how much memory the machine has, what its speed is, what its operating system,	0
and when you use something like " run  command", you can specify those attributes for your program. For example if you only want your	0
thing to run under Linux, you can give it the Linux attribute, and then it will find the fastest available Linux machine and run it on that. So.	0
You can control  where  your jobs go, to a certain extent,	0
all the way down to an individual machine. Each machine has an attribute which is the name of  itself.	0
So you can give  that  as an attribute and it'll  only  run on  that.	0
If there's already a  job  running, on some machine that you're trying to select, your job will get queued up,	0
and then when that resource, that machine becomes available, your job will get exported there. So,	0
there's a lot of nice features to it and it kinda helps to balance the load of the machines and uh,	1
right now Andreas and I have been the main ones using it and we're - Uh. [MASK] has all this __JARGON__ P_make __/JARGON__ customs stuff built into it. So.	0
right now Andreas and I have been the main ones using it and we're - Uh. [MASK] has all this [MASK] customs stuff built into it. So.	0
So as I understand,  you know ,  he's  using all the machines and  you're  using all the machines,	0
is the rough division of -	0
Yeah, you know, I - I sort of got started  using the recognizer just recently and uh,	0
uh I fired off a training job, and then I fired off a recognition job and I get this email about midnight from Andreas saying,	0
"uh, are you running two  trainings simultaneously s- my m- my jobs are not getting run."	0
So I had to back off a little bit. But,	0
soon as we get some more machines then uh -	0
then we'll have more compute available. So, um,	0
that's just a quick update about what we've got. So.	0
Um, I have - I have a question about the uh,	0
So, um, let's say I have like, a thousand little - little jobs to do?	0
Um, how do I do it with "run command"? I mean do -	0
You could write a script uh, which called run command on each sub-job right? But you probably	0
wanna be careful with that because um, you don't wanna saturate the network.	0
you know, you should - you should probably not run more than, say ten	0
jobs yourself at any one time, uh, just because then it would	0
Oh, too much file transfer and stuff.	0
Well it's  not   that  so much as that, you know, e- with - if  everybody  ran fifty jobs at once then it would just bring everything to a halt and, you know, people's jobs would get delayed, so it's sort of a sharing thing.	0
Um, so you should try to limit it to somet- sometim- some number around ten jobs at a time. Um. So if you had a script for example that had a thousand things it needed to run, um,	0
you'd somehow need to put some logic in there if you were gonna use "run command", uh, to  only  have ten of those going at a time.	0
And uh, then, when one of those finished you'd fire off another one.	0
I remember I - I forget whether it was when the Rutgers or - or Hopkins workshop, I remember  one  of the workshops I was at there were -	0
everybody was real excited cuz they got twenty-five machines and there was some kind of [MASK] like thing that sit- sent things out.	0
So all twenty-five people were sending things to all twenty-five machines  and  and things were a lot less efficient than if you'd just use your own machine.  as I recall, but.  Yeah.	0
Yeah, exactly. Yeah, you have to be a little bit careful.	0
but uh, you can also - If you have  that  level of parallelization um, and you don't wanna have to worry about writing the logic in - in [MASK] to take care of that, you can use um, __JARGON__ P_make __/JARGON__	0
but uh, you can also - If you have  that  level of parallelization um, and you don't wanna have to worry about writing the logic in - in [MASK] to take care of that, you can use um, [MASK]	0
Just do [MASK] s-	0
and - and you basically write a [MASK] that uh, you know your final job depends on these one thousand things, and when you run	0
P_make, uh, on your Make file, you can give it [MASK]	0
and - and then a number, and that number represents how many uh, machines to use at once.	0
And then it'll make sure that it never goes above that.	0
I can get some documentation.	0
So it - it's - it's not systematically queued. I mean	0
all the jobs are  running.	0
If you launch twenty jobs, they are all  running.	0
It depends. If you - "Run command", that I mentioned before, is - doesn't know about other things that you might be running.	0
So, it would be  possible  to run a hundred run jobs at once,	0
and they wouldn't know about each other.	0
But if you use  P_make , then, it knows about all the jobs that it has to run	0
and  it  can control, uh, how many it runs simultaneously.	0
So "run command" doesn't use [MASK] or - ?	0
i- It's meant to be run one job at a time?	0
So you could fire off a  thousand  of those, and it doesn't know - any  one  of those doesn't know about the other ones that are running.	0
So why would one use that rather than  P_make ?	0
Well, if you have, um - Like, for example, uh if you didn't wanna write [MASK] and you just had a, uh -	0
and somebody's using, uh, the machine you typically use, you can say "run command" and your [MASK] thing and it'll find another machine, the fastest currently available machine and - and run your job there.	0
Now, does it have the same sort of behavior as [MASK] which is that, you know, if you run something on somebody's machine and they come in and hit a key then it -	0
Yes. Yeah, there are um - Right. So some of the machines at the institute, um,	0
have this attribute called " no  evict".	0
And if you  specify  that,	0
in - in one of your  attribute  lines, then it'll go to a machine which your job  won't  be evicted from.	0
But, the machines that  don't  have that  attribute,	0
if a job gets fired up on that,	0
which could be somebody's desktop machine, and -	0
and they were at lunch, they come  back  from lunch and they start typing on the console,	0
then your machine will get evicted - your  job   will get evicted from their machine and be restarted on another machine. Automatically.	0
So - which can cause you to lose time, right? If you	0
had a  two  hour job, and it got halfway through and then somebody came back to their machine and it got  evicted.  So.	0
If you  don't  want your job to  run  on a machine where it could be  evicted,  then you give it the minus - the attribute, you know, "no evict",	0
and it'll pick a machine that it can't be evicted from. So.	0
Um, what - what about - I remember always  used  to be an issue, maybe it's not anymore, that	0
if you - if something required - if your machine required somebody hitting a key in order to evict things that are on it so you could work,	0
but if you were logged into it from home?	0
and you weren't hitting any keys? cuz you were, home?	0
Yeah, I - I'm not sure how that works. Uh, it seems like Andreas did something for that. Um.	0
O_K. We can ask him sometime.	0
Yeah. I don't know whether it monitors the keyboard or actually looks at the console [MASK] so maybe if you	0
you know, dev - dev console or something.  Hmm?	0
You probably wouldn't ordinarily, though.  Yeah.  Right?	0
You probably wouldn't  ordinarily.   I mean you sort of -	0
you're at home and you're trying to log in, and it takes forever to even log you in, and you probably go, "screw this", and -  You know. Yeah.	0
Yeah, so, um, yeah. I - I can - I'm not sure about that one. But uh.	0
Uh, I need a little orientation about this environment and uh scr- s- how to run some jobs here because I never d- did anything so far	0
with this [MASK]   So,	0
I think maybe I'll ask you after the meeting.	0
Yeah. Yeah, and - and also uh,  Stephane's  a - a really good resource for that if you can't	0
Yeah, yeah, yeah. Yep. O_K, sure  @@	0
Especially with regard to the  Aurora  stuff. He - he knows that stuff better than I do.	0
O_K.  Well, why don't we uh,	0
Sunil since you're  haven't - haven't  been   at  one of these yet, why don't yo- you tell us what's - what's up with  you?  Wh- what you've been up to, hopefully.	1
uh, shall I start from -	0
Well I don't know how may I - how -	0
O_K. Uh, I think I'll start from the post uh [MASK] submission maybe.	0
Uh, yeah, after the submission the -	0
what I've been working on mainly was to take - take other s- submissions and then	0
over  their  system, what they submitted,	0
because we didn't have any speech enhancement system in - in ours. So -	0
First I tried just [MASK] And then I found that uh,	0
I mean, if - if I combine it with [MASK] it gives  @@  improvement over  theirs.	0
Are y- are you saying L_D_A?	0
just - just [MASK] I just plug in -	0
I just take [MASK] coming from their system and then plug	0
But [MASK] that I used was different from what we submitted in the proposal. What I did was	0
I took [MASK] design using  clean  speech,	0
uh, mainly because the speech is already cleaned up after the enhancement so, instead of using this,	0
uh, narrow - [MASK] that we submitted uh,	0
I got  new  filters. So	0
that seems to be giving -	0
uh, improving over their uh, system.	0
Slightly.  But, not very significantly.	0
uh, showing any improvement over - final - by plugging in [MASK]	0
so then after - after that I - I added uh, [MASK]  also  on top of that. And that -	0
there - there also I n-	0
I found that I have to make some changes to their	0
time constant that I used	0
because th- it has a - a mean and variance update time constant and -	0
suitable for the enhanced speech,	0
and whatever we try it on with proposal-one.	0
I didn't - I didn't play with that time constant a lot, I just t- g-	0
I just found that I have to reduce the value - I mean, I have to increase the time constant, or reduce the value of the update value.	0
That's all I found  So I have to .	0
uh, the other - other thing what I tried was, I just um,	1
uh, took the baseline and then	1
ran it with the endpoint inf- uh th- information,	1
just [MASK]	1
to see that how much the baseline itself improves	1
by just supplying the information of the - I mean the w-	1
I found that the baseline itself improves by twenty-two percent by just giving the  wuh- .	1
Uh, can you back up a second, I - I - I missed something, uh,	0
I guess my mind wandered. Ad- ad- When you added [MASK] and so forth, uh, uh things got better again? or is it? Did it not?	0
No. No. No, things didn't get better with the same time constant that we used.	0
No, no. With a  different  time constant.	0
With the different time constant I found that -	0
I mean, I didn't get an improvement over	0
not  using [MASK]	0
Oh. No you didn't, O_K.	0
because I - I found that I would have change the value of the update factor.	0
But I didn't play it with	0
play - play quite a bit to make it better than.	0
So, it's still not -	0
I mean, [MASK] didn't give me any improvement.	0
oh yeah  So I just stopped there with the uh, speech enhancement. The - the other thing what I tried was the - adding the	0
uh, endpoint information to the baseline and that itself gives like twenty-two percent because the -	1
the second - the new phase is going to be with the endpointed speech. And just to get a feel of how much the baseline itself is going to change by adding this endpoint information, I just,	1
So people won't even have to  worry  about, uh, doing speech-nonspeech then.	1
Yeah that's, that's what the feeling is like.	1
They're going to give the endpoint information.	0
G- I guess the issue is that people do that anyway, everybody does that, and they wanted to see,	0
given that you're doing that, what - what are the best features that you should use.	0
I mean  clearly  they're interact. So I don't know that I entirely agree with it. But - but it might be uh - In some ways it might be better t- to -	0
rather than giving the endpoints, to have a standard	0
that everybody uses and then interacts with. But, you know. It's - it's still someth- reasonable.	0
So, are people supposed to assume that there is uh -	0
Are - are people not supposed to use any speech  outside  of those endpoints?	0
No.   No.  That i- I -	0
use speech outside of it for estimating background noise and things?	0
I guess that is - that is where the consensus is.	0
Like y- you will - you will -	0
You'll be given the information about the beginning	0
and the end of speech	0
but the whole speech is available to you.	0
So it  should  make [MASK] style things work even  better,	0
because you don't have the mistakes in it.	0
So that - that - The baseline itself - I mean, it improves by twenty-two percent. I found that in s- one of [MASK] cases,	0
that  like, the  Spanish  one improves by just fifty percent by just putting the endpoint.	0
I mean you don't  need  any further speech enhancement with fifty.	0
So the baseline itself improves by fifty percent.	0
Yeah. So it's g- it's gonna be harder to   beat  that actually. But - but -	0
so that is when uh, the - the qualification criteria was	0
reduced from fifty percent to something like	0
twenty-five percent for well-matched. And I think they have -	0
they have actually changed their qualification c- criteria now.	0
Yeah, I guess after that,	0
I just went home f- I just had a vacation fo- for four weeks.	0
O_K. No, that's - that's - that's a good - good update.	0
Yeah, and I - I came back and I started working on	0
uh, some other speech enhancement algorithm. I mean, so -	0
I - from the submission what I found that people have tried [MASK] and __JARGON__ Wiener filtering. __/JARGON__ These are the main	0
I - from the submission what I found that people have tried [MASK] and [MASK] These are the main	0
uh, approaches where people have tried, so just to -	0
just to fill the space with some f- few more speech enhancement algorithms to see whether it	0
improves  a lot , I - I've been working on this uh, [MASK] approach for speech enhancement where you	0
take the noisy signal and  then  decomposing the signal s-	0
and [MASK] and then try to estimate the clean speech from the signal plus noise subspace.	0
So, I've been actually running some s-	0
So far I've been trying it only on  Matlab.  I have to -	0
to - to  test  whether it works first or not and then I'll	0
p- port it to C_ and I'll update it with the repository once I find  it- it  giving any some positive result.	0
So you said one thing I want to jump on for a second.	0
you're - you're getting tuned into the repository thing that he has here and -	0
so we- we'll have a  single place where the stuff is.	0
so maybe uh, just briefly, you could	1
remind us about the related experiments. Cuz you did some stuff that you talked about last week, I guess?	1
Um, where you were also combining something -  both  of you I guess were both combining something from the uh,	1
[MASK] with  the u- uh - I - I don't know whether it was system one or system two, or - ?	1
Mm-hmm. It was system one. So we -	0
The main thing that we did is just to take [MASK] fr	1
So I let me - let me just stop you there. So then, one distinction is that	0
uh, you were taking the actual [MASK] features and then applying something to -	0
Uh, no there is a slight different. Uh	0
I mean, which are extracted at the handset	0
because they had another [MASK] -	0
But that's what I mean.	0
But u- u- Sorry, yeah, I'm not being - I'm not being clear.	0
What I meant was you had something like [MASK] or something, right?	0
And so one difference is that, I guess you were taking  spectra.	0
Yeah. But I guess it's the s- exactly the same thing because	0
on the heads- uh, handset they just applied this [MASK] and then compu	0
Yeah, [MASK] f- The difference is like - There may be a slight difference in the way - because they use exactly the baseline system for converting the cepstrum once you have the speech.	0
I mean, if we are using our own code for th-	0
I mean that - that could be the only difference. I mean, there is no other difference. Yeah.	0
But you got some sort of different  result.  So I'm trying to  understand  it. But uh,	0
Yeah, well I think we should uh, have a table with all the result because I don't know I uh, I don't exactly know what are your results? But,	0
Yeah, but so we did  this,  and another difference I guess is that we just applied	0
uh,[MASK] after this without -	0
well, with our modification to reduce the delay of the - [MASK] and	0
Well there are slight modifications, but it was the full proposal-one. In your case, if you tried	0
just putting [MASK]  then  maybe on-line normalization - ?	0
Af- I - after that I added [MASK] yeah.	0
Mm-hmm. So we just tried directly to - to	1
just, keep the system as it  was   and,	1
um, when we plug [MASK] it improves uh, signif- significantly.	1
Um, but, what seems clear also is that we have to retune the time constants of [MASK] Because if we keep the value that was submitted	0
uh, it doesn't help at all. You can remove [MASK] or put it, it doesn't change  anything.	1
Uh, uh, as long as you have [MASK]	1
But, you can still find	0
some kind of optimum somewhere, and we don't know where exactly but,	0
So it sounds like you should look at some tables of results or something and see where i-	0
where they were different and what we can learn from it.	0
with changes, because we change it the system to have -	0
Oh yeah, I mean the - [MASK] I mean - O_K.	0
There are other things that we finally were shown to improve also like,	0
Uh, it doesn't seem to hurt on [MASK] finally.	0
Maybe because of other changes.	0
Um, well there are some  minor changes, yeah.	0
And, right now if we look at the results, it's,	1
um, always better than -	1
it seems always better than [MASK] for mismatch and high-mismatch.	1
And it's still slightly worse for well-matched.	1
but this is not significant.	1
the problem is that it's not significant, but if you	0
put this in the, mmm,	0
uh, spreadsheet, it's still worse.	0
Even with very minor -	0
even if it's only slightly worse for well-matched.	0
And significantly better for H_M.	0
I don't think it's importa- important because when they will change their metric,	0
uh, mainly because of uh, when you p- you plug the um,	0
frame dropping in the baseline system,	0
it will improve a lot H_M, and M_M,  so,	0
um, I guess what will happen -	0
I don't know what will happen. But, the different contribution, I think, for the different test set will be more even.	0
Because the - your improvement on [MASK]	0
Yeah. So the - the M_M -	0
M_M and H_M are going to be v- hugely affected by it. Yeah.	0
But they d- the -  everything  I mean is like, but there	0
that's how they reduce - why they reduce the qualification to twenty-five percent or some - something on.	0
But are they changing the weighting?	0
Uh, no, I guess they are going ahead with the same weighting.	0
So there's nothing on -	0
I don't understand that. I guess I - I haven't been part of the	0
it seems to me that the well-matched condition is gonna be unusual,	0
Because, um, you don't actually have	0
good matches ordinarily for what any  @@  - particular person's car	0
It seems like something like the  middle  one is - is more	0
So I don't know why the  well-matched is	0
Yeah, but actually the well - well	0
I mean the - the well-matched condition is not like, uh, the one in [MASK] where	0
uh, you have all the training, uh, conditions exactly like replicated in the	0
testing condition also. It's like, this is not calibrated by [MASK] or something. The well-matched has also some - some mismatch in that which is other than the -	0
The well wa- matched has mismatch?	0
has - has  also  some slight mismatches, unlike [MASK] where it's like perfectly matched because it's artificially added noise.	0
But this is natural recording.	0
So  remind  me of what well-matched meant? You've  told  me many times.	0
The - the well-matched is like -	0
the - the well-matched is defined like it's seventy percent of the whole database is used for training and thirty percent for testing.	0
Yeah. Well, so it means that if the database is large enough, it's matched. Because	0
in each set you have a range of conditions - Well -	0
Right. So, I mean, yeah, unless they deliberately chose it to be different, which they  didn't  because they want it to be well-matched, it  is  pretty much - You know, so it's - so it's sort of saying if you -	0
It's - it's not  guaranteed  though.	0
Uh, it's not  guaranteed.  Right. Right.	0
Yeah because the m- the main - major reason for the m-	0
the main mismatch is coming from the amount of noise and the silence frames and all those present in the database actually.	0
Again, if you have  enough  - if you have enough -	0
So it's sort of i- i- it's sort of saying O_K, so you - much as you train your dictation machine for talking into your computer,	0
um, you - you have a car, and so you drive it around a bunch and - and record noise conditions, or something, and then - I don't think that's very realistic, I mean I th-  I - I you know, so I -	0
I - I - you know, I  guess  they're saying that if you were a company that was selling the stuff commercially,	0
that you would have a bunch of people driving around in a bunch of cars, and - and you would have something that was roughly similar and maybe that's the argument, but I'm not sure I buy it, so.	0
What else is going on?	0
You- Yeah. We are playing - we are also playing, trying to put other	1
it would be a very simple [MASK] on the um,	1
which I already tested but without the um	0
frame  dropping  actually, and I think it's important to have frame dropping	0
Is it - is [MASK] typically done on the - after the mel, uh, scaling or is it done on __JARGON__ the F_F_T __/JARGON__ bins?	0
Is it - is [MASK] typically done on the - after the mel, uh, scaling or is it done on [MASK] bins?	0
if you use [MASK]	0
Does it matter, or - ?	0
I don't know. Well, it's both - both uh, cases can i-	0
So- some of the proposal, uh, we're doing this on the bin - on [MASK] others on the um,	0
which one might be better or -	0
I guess if you want to reconstruct the speech, it may be a good idea to do it on [MASK]	0
But for speech recognition, it may not.	0
I mean it may not be very different if you do it on mel	0
warped or whether you do it on [MASK]	0
So you're going to do a linear weighting anyway after that.	0
So, it may not be really a big different.	0
Well, it gives something different, but I don't know what are the, pros and cons of	0
The other thing is like when you're putting in a speech enhancement technique, uh,	0
is it like one stage speech enhancement? Because everybody seems to have a mod- two stages of speech enhancement in all the proposals, which is really giving them some improvement.	0
I mean they just do the same thing again once more.	0
And - So, there's something that is good about doing it -	0
I mean, to cleaning it up once more.	0
Yeah, it might be. Yeah.	0
Yeah, so we can -	0
So maybe in my implementation I should also	0
try to inspire me from	0
this kind of thing and - Yeah.	0
Well, the  other  thing would be to combine what you're  doing.  I mean maybe one or - one or the other of the things that you're doing would benefit from the other happening  first.	0
Right, so he's doing [MASK] thing, maybe it would work better if you'd already done some simple __JARGON__ spectral subtraction, __/JARGON__ or maybe vi- maybe the other way around, you know?	0
Right, so he's doing [MASK] thing, maybe it would work better if you'd already done some simple [MASK] or maybe vi- maybe the other way around, you know?	0
So  I 've been thinking about combining [MASK] with	0
I mean just to see all - some - some such permutation combination to see whether it really  helps  or not.	0
How is it - I - I guess I'm ignorant about this, how does -	0
I mean, since [MASK] also assumes that you're - that you're adding together the two signals,	0
how is - how is that differ from [MASK]	0
[MASK] approach  has  actually an in-built __JARGON__ Wiener filtering __/JARGON__ in it.	0
[MASK] approach  has  actually an in-built [MASK] in it.	0
Yeah. It is like [MASK] followed by	0
Is the signal is - is [MASK] .	0
Oh, oh, O_K so the difference is [MASK]	0
So, the - the different - the c- the - the advantage of combining two things is mainly coming from [MASK] approach doesn't	0
work very well if [MASK] is very bad.	0
it works very poorly with the poor [MASK] conditions, and in colored noise.	0
So essentially you could do simple [MASK] followed by	0
It's a - it's a cascade of two s-	0
Yeah, in general, you don't - that's right you don't wanna othorg- orthogonalize if the things are noisy.	0
Um, that was something that uh, Herve and I were talking about with um, [MASK] stuff, that if you're converting things to	0
from uh, bands, groups of bands into [MASK] coef- you know, local sort of local cepstral coefficients that it's not that great to do it if it's noisy.	0
So that - that's one reason maybe we could combine	0
s- some - something to improve [MASK] a little bit,	0
first stage, and then do a something in the second stage which could take it	0
What was your point about - about colored noise there?	0
Oh, the colored noise uh -	0
the colored noise - the - the v- [MASK] approach has -	0
I mean, it - it actually depends on inverting	0
the matrices. So it - it - ac-	0
the covariance matrix of the noise.	0
So if - if it is not	0
positive definite, I mean it has a - it's -	0
It doesn't behave very well if it is not positive definite  ak-	0
It works very well with white noise because we know	0
for sure that it has a positive definite.	0
So you should do [MASK] and then add noise.	0
So the way they get around is like they do an inverse filtering, first of the colo- colored noise and then make the noise white, and then finally when you reconstruct the speech back, you do this filtering again.	0
I was only half kidding. I mean if you - sort of  you do the s- [MASK]	0
that also gets rid - and then you - then - then add a little bit l- noise - noise addition -	0
I mean, that sort of what J_ - [MASK] does, in a way.	0
If you look at what [MASK] doing essentially i- i- it's equivalent to sort of adding a little - adding a little noise,	0
in order to get rid of the effects of noise.	0
Uh, yeah. So there is  this.  And	0
maybe we - well we find some people so that	1
uh, agree to maybe work with us,	1
and they have implementation of [MASK] techniques	1
[MASK] that are used to mmm,	1
uh f- to model the transformation between clean	1
So. Well, if you take the standard model of channel plus noise,	0
uh, it's - it's a nonlinear	0
eh- uh, transformation in [MASK]	0
uh, there is a way to approximate this using	0
uh, first-order or second-order Taylor Series and	0
it can be used for	1
uh, getting rid of the noise and the channel effect.	1
Uh w- working in [MASK] So there is one guy	0
and another in  uh, Lucent that I met at I_CASSP.	0
Who's the guy in Grenada?	0
I don't know  him .	0
This [MASK] has been proposed by	0
Yeah, yeah, yeah. Originally the idea was from [MASK]	0
Well, it's again a different thing   that could be tried.	0
Yeah, so at any rate, you're looking	0
uh, standing back from it,	0
looking at ways to combine	0
one form or another of uh, noise removal,	0
uh, with - with these other things we have,	0
uh, looks like a worthy thing to -	0
But, yeah. But for sure there's required to - that requires to	0
re-check everything else, and re-optimize	0
for sure the [MASK] may b	0
Well one of the - seems like one of the things to go through next week when Hari's here, cuz Hari'll have his own ideas	0
too - or  I guess not next week, week and a half,	0
uh, will be sort of go through these alternatives, what we've seen so far, and come up with some game plans. Um. You know. So, I mean one way would -	0
he- Here are some alternate visions. I mean one would be,	0
you look at a few things very quickly, you pick on something that looks like it's promising and then everybody works really hard on the same - different aspects of the same thing.	0
Another thing would be to have t- to - to pick two pol-  two  plausible things, and - and you know, have t- sort of two working things for a while until we figure out what's better, and then, you know, uh,	0
uh, he'll have some ideas on that too.	0
The other thing is to, uh - Most of the speech enhancement techniques have reported results on small vocabulary tasks.	0
But we - we going to address this Wall Street Journal in our next stage,	0
which is also going to be a noisy task so s- very few people have reported something on	0
using some continuous speech at all. So, there are some - I mean, I was looking at some literature on speech enhancement	0
applied to large vocabulary tasks and	0
[MASK]  doesn't  seems to be the thing to do for	0
large vocabulary tasks. And it's -	0
Always people have shown improvement with [MASK] and mayb	0
But if we - if we have to use simple spectral subtraction, we may have to do some optimization  to make it	0
So they're making - there - Somebody's generating Wall Street Journal with additive - artificially added noise or something?	0
Sort of a - sort of like what they did with [MASK] and? Yeah, O_K.	0
Yeah. I m- I guess Guenter Hirsch is in charge of that.	0
Guenter Hirsch and [MASK] Maybe Roger - r- Roger, maybe in charge of.	0
And then they're - they're uh, uh, generating [MASK] to -	0
Yeah, I don't know. There are - they have - there is no - I don't know if they are converging on [MASK] or are using some	0
Mississippi State, yeah. I'm not sure about that.	0
Mis- Mississippi State maybe, yeah.	0
Yeah, so that'll be a little - little task in itself.  Um, well we've -	0
Yeah, it's true for the additive	0
noise, y- artificially added noise we've always used small vocabulary  too.	0
But for n- there's been noisy speech this larv- large vocabulary that we've worked with in Broadcast News. So we- we did the Broadcast News evaluation and	0
some of the focus conditions were noisy and - and - But we - but we didn't do [MASK]  We were doing our funny stuff, right? We were doing multi- multi- uh, multi-stream and - and so forth.	0
But it, you know, we di- stuff we did  helped.  I mean it,	0
now we have this um,  meeting  data.	0
You know, like the stuff we're  recording right  now,  and -	0
that  we have uh, for the -	0
uh, the quote-unquote  noisy  data there is just -	0
noisy and  reverberant  actually. It's the far field mike.	0
the  digits  that we do at the end of these things. And that's what most o- again, most of our work has been done with that, with - with uh, connected digits.	0
but uh, we have recognition now	0
with some of the continuous speech,	0
large vocabulary continuous speech, using [MASK] - uh, [MASK] recognizer,	0
large vocabulary continuous speech, using [MASK] - uh, [MASK] recognizer,	0
uh, no training,  from this, just - just plain using [MASK]	0
Oh. You just take [MASK] trained - ? Yeah, yeah.	0
That's - that's what we're doing, yeah. Now there are some adaptation though, that - that uh, Andreas has been playing with, but we're hop- uh, actually uh, Dave and I were just talking earlier today about maybe at some point not that distant future, trying some of the techniques	0
O_K. Yeah. That's cool. O_K.	0
that we've talked about on, uh, some of the large vocabulary data. Um,	0
I mean, I guess no one had done -  yet  done test one on	0
using uh, [MASK] and, uh,	0
I don't - not that I know of.	0
You'll  see a little smoke coming up from the - [MASK] or something  trying to - trying to do it, but	0
uh, yeah. But, you're right that - that - that's a real good point, that uh, we - we don't know	0
yeah, uh, I mean, what if any of these ta- I guess that's why they're pushing that in the uh - in the evaluation.	0
Anything else going on? at you guys' end, or - ?	1
I don't have good result, with the - inc- including the new parameters, I don't have good result.	1
Are  similar or a little bit worse.	0
With what - what other new p- new parameter?	1
You're talking about your voicing?	0
Yeah. So maybe - You probably need to back up a bit  seeing as how Sunil, yeah .	1
I tried to include another new parameter to the	1
traditional parameter, the coe- [MASK] that, like, the	1
and [MASK]	1
and another estimation of the	1
var- the variance of the difference for - of the spec- si- uh, spectrum of the signal and -	1
and the spectrum of time after	1
I'm  so  sorry. I didn't get it.	0
Nuh. Well. Anyway. The - First you have the sp-	0
the spectrum of the signal, and you have the -	0
on the other side you have the output of [MASK]	0
You can extend the coefficient of [MASK] and obtain an approximation of the spectrum of the signal.	0
I do the difference -	0
I found a difference at the variance of this different because, suppose	0
we - we think that	0
if the variance is high, maybe you have n- uh, noise.	0
And if the variance is small,	0
maybe you have uh, speech.	0
To - The idea is to found another feature for discriminate between	1
voice sound and unvoice sound.	1
And we try to use this new feature - feature. And I did experiment -	1
I need to change - to obtain this new feature I need to change the size - the window size - size.	0
of the a- of the - analysis window size,	0
Uh, sixty-two point five milliseconds I think.	0
I did two type of experiment to include this feature directly	1
with the other feature and to train a neural network	1
to select it [MASK] - silence and to - to concat this new feature. But the result are	0
n- with the neural network I have more or less the same result.	1
As using just [MASK] or - ?	1
It's neve- e- e- sometime it's worse, sometime it's a little bit better, but not significantly.	1
Uh, is it with [MASK] or with - ?	0
No, I work with eh, Italian and Spanish basically.	0
And if I don't y- use the neural network, and use directly	0
I - I - I really wonder though. I mean we've had these discussions before, and - and one of the things that struck me was that - uh, about this line of thought that was	1
particularly interesting to me was that	1
uh, in an irreversible way,	1
um, you throw away some information.	1
And, that's mostly viewed on as a good thing, in the way  we  use it, because we wanna suppress things that will cause variability for uh particular, uh, [MASK]	0
but, you'll do throw something away.	0
And so the question is, uh, can we figure out if there's  something  we've thrown away that we  shouldn't  have.	1
when they were looking at the  difference  between [MASK] and __JARGON__ the F_F_T __/JARGON__ that was going  int	1
when they were looking at the  difference  between [MASK] and [MASK] that was going  int	1
they're looking on it to figure out  noise,  or  voice  -  voiced  property whatever." So that - that's interesting. Maybe that helps to drive the -	1
the thought process of coming up with the features. But for  me  sort of the interesting thing was, "well,	1
but is there just something in that  difference  which is  useful? " So  another  way of doing it, maybe, would be just to take [MASK]	1
uh, [MASK] and feed it into a  neural  network,	1
and then use it, you know, in combination, or alone, or - or whatever	0
Voiced, unvoiced is like -	0
No the - just the same - same way we're using - I mean, the same way that we're using [MASK]	0
Exact way - the same way we're using [MASK] I mean, __JARGON__ the filter bank __/JARGON__ is good for all the reasons that we say it's  good.  But it's  different.	0
Exact way - the same way we're using [MASK] I mean, [MASK] is good for all the reasons that we say it's  good.  But it's  different.	0
And, you know, maybe if it's used in combination, it will get at something that we're  missing.	1
And maybe, you know, using, orth- you know, [MASK] or uh, um,	0
adding probabilities, I mean, all th- all the different ways that we've been  playing  with,	0
that we would let the - essentially let the neural network determine what is it	0
that's useful, that we're missing here.	0
Yeah, but there is so much variability in the power  spectrum.	0
Well, that's probably why y- i- it would be  unlikely  to work as well by itself, but it  might  help in  combination.	0
But I - I - I have to tell you, I can't remember the conference, but, uh, I think it's about	0
ten years ago, I remember going to	0
one of the speech conferences and - and uh,	0
I saw within very short distance of one another a couple different posters that showed about the wonders of some auditory inspired front-end or something, and a couple posters away it was somebody	0
who compared one to uh, just putting in [MASK] and [MASK] did slightly better.	0
who compared one to uh, just putting in [MASK] and [MASK] did slightly better.	0
So I mean the - i- i- It's true there's lots of variability, but again we have these wonderful statistical mechanisms for	0
quantifying  that a- that variability, and you know,  doing  something  reasonable  with it. So, um,	0
It- it's same, you know, argument	0
that's gone both ways about uh, you know, we have these data driven filters, in [MASK]	0
and on the other hand, if it's  data  driven it means it's driven by things that have lots of variability, and that are necessarily - not necessarily gonna be the same in training and test,	0
so, in some ways it's  good  to have data driven things, and in some ways it's  bad  to have data driven things. So,	0
part of what we're discovering, is ways to combine things that are data driven than are not.	0
Uh, so anyway, it's just a thought, that - that if we - if we had that - maybe it's just a  baseline	0
uh, which would show us "well, what are we really getting out of the filters", or maybe i- i- probably not by  itself,  but in  combination,	0
uh, you know, maybe there's something to be  gained  from it, and let the -	0
But, you know, y- you've only worked with us for a short time, maybe in a year or two you w- you will actually come up with the right set of things	0
to  extract  from this information.	0
But, maybe the neural net and [MASK] could figure it out  quicker  than you.	0
So.  It's just a thought.	1
Maybe.  Yeah, I can - I will try to do that.	1
What - one - one um p-	0
one thing is like what - before we started using this [MASK]	0
the - th- what we did was like,	0
I - I guess most of you know about this, adding this additional [MASK] bit t	0
That is just a binary feature and that seems to be  improving a lot on [MASK] where there is a lot of	0
noise but not much on [MASK] So, a- adding an additional feature to distin- to discriminate between speech and nonspeech was helping.	0
Wait - I - I'm sorry?	0
Yeah, we actually added an additional binary feature to [MASK]  just  the baseline.	0
Yeah, yeah. Well, in - in the case of [MASK] it didn't actually give us anything,	0
because there wasn't any f- anything to discriminate between speech, and  it  was very short.	0
very - it was a  huge  improvement on Italian.	0
But  anyway  the question is even  more,  is within speech, can we get some features?	0
Are we drop- dropping information that can might be useful within  speech,  I mean. To - maybe to distinguish between voice sound and unvoiced sounds?	0
And it's particularly more relevant now since we're gonna be given the endpoints.	0
There was a paper in I_CASSP - this I_CASSP - over the uh extracting some	0
information from the cepstral coefficients and	0
I forgot the name. Some is- some harmonics  I don't know, I can - I can pull that paper out from I_CASSP. It - Huh?	0
Talking cumulants or something? Cumulants or something. But - No.	0
Uh, I don't know. I don't remember. It wa- it was taking the, um -	0
It was about finding the higher-order	0
Yeah. And I'm not sure about whether it is the higher-order moments, or -	0
maybe  higher-order cumulants and -	0
It was - it was - Yeah. I mean, he was showing up uh some - something on noisy speech, some improvement on the noisy speech.	0
So it  was on [MASK]	0
Yeah, but again - You  could  argue that th- that's exactly what the neural network does.	0
So n- neural network uh,	0
is in some sense equivalent to computing, you know, higher-order moments of what you - yeah.	0
I mean, it doesn't do it very specifically, and	0
pretty - you know. But.	0
Uh, anything on your end you want to talk about? Uh.	1
Um, nothing I wanna really talk about. I can - I can just uh, um, share a little bit - Sunil hasn't - hasn't heard about uh, what I've been doing. Um, so,	1
um, I told you I was - I was - I was getting prepared to take this qualifier exam. So basically that's just, um, trying to propose um, uh,	1
your next your - your following years of - of your P_H_D work, trying - trying to find a project to - to define and - and to work on.	1
So, I've been, uh, looking into, um, doing something about r- uh, speech recognition using acoustic events. So,	1
um, the idea is you have all these - these different events, for example voicing, nasality, [MASK] you know burst or noise, uh,	1
um, primary detectors for these acoustic events, and using the outputs of these robust detectors to do speech recognition.	1
Um, and, um, these - these primary detectors, um, will be, uh,	0
inspired by, you know, [MASK] techniques, um, doing things, um, similar to Larry Saul's work on, uh, graphical models	0
to - to detect these - these, uh, acoustic events. And, um, so I - I been - I been thinking about that and some of the issues that I've been running into are, um, exactly what - what kind of acoustic events I need, what -	0
um, what acoustic events will provide a - a good enough coverage to - in order to do the later recognition steps. And, also, um, once I decide a set of acoustic events, um, h- how do I -	0
how do I get labels? Training data for - for these acoustic events. And, then later on down the line, I can start playing with the - the models themselves, the - the primary detectors. Um, so,	0
um, I kinda see - like, after - after building the primary detectors I see um, myself taking the outputs	0
and feeding them in, sorta tandem style into - into a um, Gaussian mixtures [MASK] um, and doing recognition. Um.  So, that's -	0
that's just generally what I've been looking at. Um,	0
By - by the way, uh, [MASK] version of that for instance could tie right in to what Carmen was looking at. So,	0
you know, um, if you - if [MASK] approach was helpful as - as I think it is, it seems to be helpful for determining	0
Um, were - were you gonna say something? Oh. It looked - O_K, never mind. Um, yeah. And so, this - this past week um, I've been uh, looking a little bit into uh, TRAPS	0
and doing - doing [MASK] on - on these e- events too, just, um, seeing - seeing if that's possible. Uh, and	0
um, other than that, uh, I was kicked out of I_house for living there for four years.	0
Oh no. So you live in a cardboard box in the street now or, no?	0
Uh, well, s- s- som- something like that. In Albany, yeah.  Yeah.	0
And uh. Yep. That's it.	0
did you find a place? Is that out of the way?	0
Uh, yesterday I called up a lady who ha- who	0
will have a vacant room from May thirtieth and she said she's interviewing two more people.	0
So. And she would get back to me on Monday.	0
So that's - that's only thing I have and Diane has a few more	0
houses. She's going to take some pictures and send me after I go back.	0
So it's - that's -	0
Oh. So you're not down here permanently yet?	0
No. I'm going back to [MASK] today.	0
O_K. And then, you're coming back uh -	0
Uh, i- I mean, I - I p- I plan to be here on thirty-first.	0
Yeah, well if there's a house available or place to - Yeah, I hope.	0
Well, I mean i- i- if - if -  They're  available, and they'll be able to get you something, so worst comes to worst we'll put you up in a hotel for - for - for a while until you -	0
Yeah. So, in that case, I'm going to be here on thirty-first  definitely .	0
You know, if you're in a desperate situation and you need a place to stay, you could stay with me for a while. I've got a spare bedroom right now.	0
That sure is nice of you.	0
So, it may be he needs more than me.	0
Oh no, no.  My - my cardboard box is actually a nice spacious two bedroom apartment.	0
So a two  bedroom  cardboard box.	0
Th- that's great. Thanks Dave.	0
say anything about - You - you actually been -	0
Uh, last week you were doing this stuff with Pierre, you were - you were mentioning. Is that - that something worth talking about, or - ?	1
Well, um, it - I don't think it  directly  relates. Um, well, so, I was helping a speech researcher named Pierre Divenyi and he's int- He wanted to um, look at um,	1
how people respond to formant changes, I  think.  Um. So he - he created a lot of synthetic audio files of vowel-to-vowel transitions, and then he wanted a psycho-acoustic um, spectrum.	1
And he wanted to look at um,	1
how the energy is moving  over time in that spectrum and compare that to the - to the listener tests. And, um. So, I	1
gave him a P_L_P spectrum. And - to um - he - he t- wanted to track the peaks so he could look at how they're moving. So I took the um, [MASK] coefficients and um,	0
I found the roots. This was something that Stephane suggested. I found the roots of the um, [MASK] to, um, track the peaks in the, um,	0
well  there is  aligned  spectral pairs, is like the - the - Is that the aligned s-	0
It's a r- [MASK] uh, of some sort.	0
Oh, no. So you just -	0
instead of the log you took the root square,	0
I mean cubic root or something.	0
What di- w- I didn't get that.	0
No, no. It's - it's - it's taking the - finding the roots of [MASK]	0
Polynomial. Yeah. Is that the line spectral -	0
So it's  like  line spectral pairs. Except I think what they call line spectral pairs they push it towards the unit circle, don't they, to sort of?	0
Oh, it's like line sp-	0
But it - But uh, you know. But what we'd used to do w- when I did synthesis at National Semiconductor twenty years ago, the	0
technique we were playing with initially was - was taking [MASK] and - and uh,	0
finding the roots. It wasn't [MASK] cuz Hynek hadn't invented it yet, but it was ju	0
uh, we found the roots of the polynomial, And th- When you do that, sometimes	0
they're f- they're what most people call formants, sometimes they're not.	0
So it's - it's - it's a little, uh - Formant tracking with it can be a little tricky cuz you get these funny  values in - in real speech, but.	0
So you just - You typically just get a few roots? You know, two or three, something like that?	0
Well you get these complex pairs. And it depends on the  order  that you're doing, but.	0
Every root that's - Since it's a real signal, [MASK] gonna have real coefficients. So I think that means that every root that is not	0
is gonna be a c- complex pair,	0
um, of a complex value and its conjugate.	0
So for each - And if you look at that on the unit circle, um, one of these - one of the members of the pair will be a  positive  frequency, one will be a  negative  frequency, I think. So I just -  So, um,	0
f- for the - I'm using an eighth-order  polynomial   and I'll get three or four of these pairs	0
which give me s- which gives me three or four peak positions.	0
This is from synthetic speech, or - ?	0
Yeah. So if it's from synthetic speech then maybe it'll be cleaner. I mean for real speech in real - then what you end up having is, like I say, funny little things that are - don't exactly fit your notion of formants	0
all that well. But - but  mostly  they are.  Mostly  they do. And - and what - I mean in - in what we were doing,	0
which was  not  so much  looking  at things, it was O_K because it was just a question of quantization.	0
Uh, we were just you know, storing - It was - We were doing, uh, stored speech, uh, quantization. But - but uh, in your case	0
Actually you have peaks that are not at the formant's positions, but	0
But - there's some of that, yes.	0
they are lower in energy and - Well they are	0
If this is synthetic speech can't you just get	0
the formants directly? I mean h- how is the speech created?	0
It was created from a synthesizer,  and um -	0
Wasn't a formant synthesizer was it?	0
I bet it - it might have - may have been but maybe he didn't have control over it or something?	0
I - d- d- this -	0
In - in fact w- we - we could get, um, formant frequencies out of the synthesizer,	0
as well. And, um,  w- one thing that the, um, [MASK] will hopefully give me	0
in  addition,  um, is that I - I might be able to find the b- the bandwidths	0
of these humps as  well.  Um, Stephane suggested looking at each complex pair as a - like a se- second-order [MASK] Um, but I don't think	0
there's a g- a really good reason not to um, get the formant frequencies from the synthesizer  instead.  Except that you don't have [MASK] in that.	0
Yeah, so the actual - So you're not getting the actual formants per se. You're getting the - Again, you're getting sort of the, uh -	0
You're getting something that is - is uh, af- strongly affected by [MASK] And so it's m	0
That's sort of the  point.	0
But - Yeah. i- Ordinarily, in a formant synthesizer, the bandwidths as well as the ban- uh, formant centers are - I mean, that's -  Somewhere  in the synthesizer that was put  in,  as -	0
as what you - But - but yeah, you view each complex pair as essentially a second-order section, which has, uh, band center and band width, and um,	0
O_ K.  So, uh, yeah, you're going back today and then back in a week I guess, and.	0
I guess we should do digits quickly.	0
Oh yeah, digits. I almost  forgot  that. I almost forgot our daily digits.	0
